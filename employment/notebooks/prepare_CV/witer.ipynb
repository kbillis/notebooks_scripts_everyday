{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf790caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dotenv\n",
      "  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
      "Requirement already satisfied: langchain in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (1.0.3)\n",
      "Collecting langchain-google\n",
      "  Downloading langchain_google-0.1.1-py3-none-any.whl.metadata (339 bytes)\n",
      "Requirement already satisfied: python-dotenv in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from dotenv) (1.2.1)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain) (1.0.2)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain) (1.0.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain) (2.12.3)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (0.4.39)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (4.15.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain) (3.0.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.0)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.2)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.9)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
      "Requirement already satisfied: ormsgpack>=1.10.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.11.0)\n",
      "Requirement already satisfied: httpx>=0.25.2 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (0.25.0)\n",
      "Requirement already satisfied: anyio in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (4.11.0)\n",
      "Requirement already satisfied: certifi in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (2.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.3.1)\n",
      "Downloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
      "Downloading langchain_google-0.1.1-py3-none-any.whl (1.1 kB)\n",
      "Installing collected packages: langchain-google, dotenv\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [dotenv]\n",
      "\u001b[1A\u001b[2KSuccessfully installed dotenv-0.9.9 langchain-google-0.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: langchain_community in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (0.4.1)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.1 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain_community) (1.0.2)\n",
      "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain_community) (1.0.0)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain_community) (2.0.44)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain_community) (2.32.5)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain_community) (6.0.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain_community) (3.13.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain_community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain_community) (2.11.0)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain_community) (0.4.39)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain_community) (0.4.3)\n",
      "Requirement already satisfied: numpy>=2.1.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain_community) (2.3.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.22.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain_community) (2.12.3)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.0.1->langchain_community) (1.33)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.0.1->langchain_community) (25.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.0.1->langchain_community) (4.15.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.1->langchain_community) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (0.25.0)\n",
      "Requirement already satisfied: anyio in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (4.11.0)\n",
      "Requirement already satisfied: certifi in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain_community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain_community) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain_community) (0.4.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (1.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from requests<3.0.0,>=2.32.5->langchain_community) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from requests<3.0.0,>=2.32.5->langchain_community) (2.5.0)\n",
      "Requirement already satisfied: greenlet>=1 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain_community) (3.2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tavily-python in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (0.7.12)\n",
      "Requirement already satisfied: requests in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from tavily-python) (2.32.5)\n",
      "Requirement already satisfied: tiktoken>=0.5.1 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from tavily-python) (0.12.0)\n",
      "Requirement already satisfied: httpx in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from tavily-python) (0.28.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from tiktoken>=0.5.1->tavily-python) (2025.10.23)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from requests->tavily-python) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from requests->tavily-python) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from requests->tavily-python) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from requests->tavily-python) (2025.10.5)\n",
      "Requirement already satisfied: anyio in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from httpx->tavily-python) (4.11.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from httpx->tavily-python) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from httpcore==1.*->httpx->tavily-python) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from anyio->httpx->tavily-python) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: langchain-community in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (0.4.1)\n",
      "Collecting beautifulsoup4\n",
      "  Using cached beautifulsoup4-4.14.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.1 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain-community) (1.0.2)\n",
      "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain-community) (1.0.0)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain-community) (2.0.44)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain-community) (2.32.5)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain-community) (6.0.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain-community) (3.13.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain-community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain-community) (2.11.0)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain-community) (0.4.39)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain-community) (0.4.3)\n",
      "Requirement already satisfied: numpy>=2.1.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain-community) (2.3.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.12.3)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (25.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (4.15.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.1->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
      "Requirement already satisfied: anyio in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.11.0)\n",
      "Requirement already satisfied: certifi in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (0.4.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
      "Requirement already satisfied: greenlet>=1 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
      "  Using cached soupsieve-2.8-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.3.1)\n",
      "Using cached beautifulsoup4-4.14.2-py3-none-any.whl (106 kB)\n",
      "Using cached soupsieve-2.8-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [beautifulsoup4]m [beautifulsoup4]\n",
      "\u001b[1A\u001b[2KSuccessfully installed beautifulsoup4-4.14.2 soupsieve-2.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting langchain_google_genai\n",
      "  Downloading langchain_google_genai-3.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain_google_genai) (1.0.2)\n",
      "Collecting google-ai-generativelanguage<1.0.0,>=0.7.0 (from langchain_google_genai)\n",
      "  Downloading google_ai_generativelanguage-0.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain_google_genai) (2.12.3)\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from langchain_google_genai)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain_google_genai)\n",
      "  Downloading google_api_core-2.28.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 (from google-ai-generativelanguage<1.0.0,>=0.7.0->langchain_google_genai)\n",
      "  Downloading google_auth-2.42.1-py2.py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting grpcio<2.0.0,>=1.33.2 (from google-ai-generativelanguage<1.0.0,>=0.7.0->langchain_google_genai)\n",
      "  Downloading grpcio-1.76.0-cp313-cp313-macosx_11_0_universal2.whl.metadata (3.7 kB)\n",
      "Collecting proto-plus<2.0.0,>=1.22.3 (from google-ai-generativelanguage<1.0.0,>=0.7.0->langchain_google_genai)\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 (from google-ai-generativelanguage<1.0.0,>=0.7.0->langchain_google_genai)\n",
      "  Downloading protobuf-6.33.0-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain_google_genai)\n",
      "  Downloading googleapis_common_protos-1.71.0-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain_google_genai) (2.32.5)\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain_google_genai)\n",
      "  Downloading grpcio_status-1.76.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting cachetools<7.0,>=2.0.0 (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain_google_genai)\n",
      "  Downloading cachetools-6.2.1-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain_google_genai)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain_google_genai)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.12 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from grpcio<2.0.0,>=1.33.2->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain_google_genai) (4.15.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain_google_genai) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain_google_genai) (0.4.39)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain_google_genai) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain_google_genai) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain_google_genai) (9.1.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain_google_genai) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain_google_genai) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain_google_genai) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain_google_genai) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain_google_genai) (0.25.0)\n",
      "Requirement already satisfied: anyio in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain_google_genai) (4.11.0)\n",
      "Requirement already satisfied: certifi in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain_google_genai) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain_google_genai) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain_google_genai) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain_google_genai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.0.0->langchain_google_genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.0.0->langchain_google_genai) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.0.0->langchain_google_genai) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain_google_genai) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain_google_genai) (2.5.0)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain_google_genai)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain_google_genai) (1.3.1)\n",
      "Downloading langchain_google_genai-3.0.0-py3-none-any.whl (57 kB)\n",
      "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading google_ai_generativelanguage-0.9.0-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_api_core-2.28.1-py3-none-any.whl (173 kB)\n",
      "Downloading google_auth-2.42.1-py2.py3-none-any.whl (222 kB)\n",
      "Downloading cachetools-6.2.1-py3-none-any.whl (11 kB)\n",
      "Downloading googleapis_common_protos-1.71.0-py3-none-any.whl (294 kB)\n",
      "Downloading grpcio-1.76.0-cp313-cp313-macosx_11_0_universal2.whl (11.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading grpcio_status-1.76.0-py3-none-any.whl (14 kB)\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading protobuf-6.33.0-cp39-abi3-macosx_10_9_universal2.whl (427 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Installing collected packages: filetype, pyasn1, protobuf, grpcio, cachetools, rsa, pyasn1-modules, proto-plus, googleapis-common-protos, grpcio-status, google-auth, google-api-core, google-ai-generativelanguage, langchain_google_genai\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/14\u001b[0m [langchain_google_genai]e-ai-generativelanguage]\n",
      "\u001b[1A\u001b[2KSuccessfully installed cachetools-6.2.1 filetype-1.2.0 google-ai-generativelanguage-0.9.0 google-api-core-2.28.1 google-auth-2.42.1 googleapis-common-protos-1.71.0 grpcio-1.76.0 grpcio-status-1.76.0 langchain_google_genai-3.0.0 proto-plus-1.26.1 protobuf-6.33.0 pyasn1-0.6.1 pyasn1-modules-0.4.2 rsa-4.9.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 22.11.1\n",
      "  latest version: 25.9.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c conda-forge conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=25.9.1\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/kbillis/miniconda3/envs/everyday_repo\n",
      "\n",
      "  added / updated specs:\n",
      "    - weasyprint\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    brotli-1.2.0               |       hb27157a_0          20 KB  conda-forge\n",
      "    brotli-bin-1.2.0           |       h5c1846c_0          18 KB  conda-forge\n",
      "    cairo-1.18.4               |       h950ec3b_0         872 KB  conda-forge\n",
      "    cffi-2.0.0                 |  py313hf57695f_1         284 KB  conda-forge\n",
      "    cssselect2-0.8.0           |     pyhd8ed1ab_0          20 KB  conda-forge\n",
      "    font-ttf-dejavu-sans-mono-2.37|       hab24e00_0         388 KB  conda-forge\n",
      "    font-ttf-inconsolata-3.000 |       h77eed37_0          94 KB  conda-forge\n",
      "    font-ttf-source-code-pro-2.038|       h77eed37_0         684 KB  conda-forge\n",
      "    font-ttf-ubuntu-0.83       |       h77eed37_3         1.5 MB  conda-forge\n",
      "    fontconfig-2.15.0          |       h37eeddb_1         227 KB  conda-forge\n",
      "    fonts-conda-ecosystem-1    |                0           4 KB  conda-forge\n",
      "    fonts-conda-forge-1        |                0           4 KB  conda-forge\n",
      "    fonttools-4.60.1           |  py313h0f4d31d_0         2.8 MB  conda-forge\n",
      "    freetype-2.14.1            |       h694c41f_0         170 KB  conda-forge\n",
      "    fribidi-1.0.16             |       h8616949_0          59 KB  conda-forge\n",
      "    graphite2-1.3.14           |       h21dd04a_2          83 KB  conda-forge\n",
      "    harfbuzz-12.1.0            |       hc5d3ef4_0         1.5 MB  conda-forge\n",
      "    icu-75.1                   |       h120a0e1_0        11.2 MB  conda-forge\n",
      "    libbrotlicommon-1.2.0      |       h105ed1c_0          77 KB  conda-forge\n",
      "    libbrotlidec-1.2.0         |       h660c9da_0          30 KB  conda-forge\n",
      "    libbrotlienc-1.2.0         |       h2338291_0         303 KB  conda-forge\n",
      "    libdeflate-1.25            |       h517ebb2_0          69 KB  conda-forge\n",
      "    libfreetype-2.14.1         |       h694c41f_0           8 KB  conda-forge\n",
      "    libfreetype6-2.14.1        |       h6912278_0         366 KB  conda-forge\n",
      "    libglib-2.86.1             |       h6ca3a76_1         3.5 MB  conda-forge\n",
      "    libiconv-1.18              |       h57a12c2_2         721 KB  conda-forge\n",
      "    libintl-0.25.1             |       h3184127_1          95 KB  conda-forge\n",
      "    libjpeg-turbo-3.1.2        |       h8616949_0         572 KB  conda-forge\n",
      "    libpng-1.6.50              |       h84aeda2_1         291 KB  conda-forge\n",
      "    libtiff-4.7.1              |       ha0a348c_1         395 KB  conda-forge\n",
      "    libwebp-base-1.6.0         |       hb807250_0         357 KB  conda-forge\n",
      "    munkres-1.1.4              |     pyhd8ed1ab_1          15 KB  conda-forge\n",
      "    openjpeg-2.5.4             |       h87e8dc5_0         327 KB  conda-forge\n",
      "    pango-1.56.4               |       h6ef8af8_0         423 KB  conda-forge\n",
      "    pcre2-10.46                |       ha3e7e28_0         1.0 MB  conda-forge\n",
      "    pillow-12.0.0              |  py313he918548_0         952 KB  conda-forge\n",
      "    pixman-0.46.4              |       ha059160_1         382 KB  conda-forge\n",
      "    pthread-stubs-0.4          |    h00291cd_1002           8 KB  conda-forge\n",
      "    pydyf-0.11.0               |     pyhd8ed1ab_1          14 KB  conda-forge\n",
      "    pyphen-0.17.2              |     pyhd8ed1ab_0         1.1 MB  conda-forge\n",
      "    tinycss2-1.4.0             |     pyhd8ed1ab_0          28 KB  conda-forge\n",
      "    tinyhtml5-2.0.0            |     pyhe01879c_0          41 KB  conda-forge\n",
      "    weasyprint-66.0            |     pyhcf101f3_0         246 KB  conda-forge\n",
      "    webencodings-0.5.1         |     pyhd8ed1ab_3          15 KB  conda-forge\n",
      "    zlib-ng-2.2.5              |       h55e386d_0         107 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        31.3 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  brotli             conda-forge/osx-64::brotli-1.2.0-hb27157a_0 \n",
      "  brotli-bin         conda-forge/osx-64::brotli-bin-1.2.0-h5c1846c_0 \n",
      "  cairo              conda-forge/osx-64::cairo-1.18.4-h950ec3b_0 \n",
      "  cffi               conda-forge/osx-64::cffi-2.0.0-py313hf57695f_1 \n",
      "  cssselect2         conda-forge/noarch::cssselect2-0.8.0-pyhd8ed1ab_0 \n",
      "  font-ttf-dejavu-s~ conda-forge/noarch::font-ttf-dejavu-sans-mono-2.37-hab24e00_0 \n",
      "  font-ttf-inconsol~ conda-forge/noarch::font-ttf-inconsolata-3.000-h77eed37_0 \n",
      "  font-ttf-source-c~ conda-forge/noarch::font-ttf-source-code-pro-2.038-h77eed37_0 \n",
      "  font-ttf-ubuntu    conda-forge/noarch::font-ttf-ubuntu-0.83-h77eed37_3 \n",
      "  fontconfig         conda-forge/osx-64::fontconfig-2.15.0-h37eeddb_1 \n",
      "  fonts-conda-ecosy~ conda-forge/noarch::fonts-conda-ecosystem-1-0 \n",
      "  fonts-conda-forge  conda-forge/noarch::fonts-conda-forge-1-0 \n",
      "  fonttools          conda-forge/osx-64::fonttools-4.60.1-py313h0f4d31d_0 \n",
      "  freetype           conda-forge/osx-64::freetype-2.14.1-h694c41f_0 \n",
      "  fribidi            conda-forge/osx-64::fribidi-1.0.16-h8616949_0 \n",
      "  graphite2          conda-forge/osx-64::graphite2-1.3.14-h21dd04a_2 \n",
      "  harfbuzz           conda-forge/osx-64::harfbuzz-12.1.0-hc5d3ef4_0 \n",
      "  icu                conda-forge/osx-64::icu-75.1-h120a0e1_0 \n",
      "  lcms2              conda-forge/osx-64::lcms2-2.17-h72f5680_0 \n",
      "  lerc               conda-forge/osx-64::lerc-4.0.0-hcca01a6_1 \n",
      "  libbrotlicommon    conda-forge/osx-64::libbrotlicommon-1.2.0-h105ed1c_0 \n",
      "  libbrotlidec       conda-forge/osx-64::libbrotlidec-1.2.0-h660c9da_0 \n",
      "  libbrotlienc       conda-forge/osx-64::libbrotlienc-1.2.0-h2338291_0 \n",
      "  libdeflate         conda-forge/osx-64::libdeflate-1.25-h517ebb2_0 \n",
      "  libfreetype        conda-forge/osx-64::libfreetype-2.14.1-h694c41f_0 \n",
      "  libfreetype6       conda-forge/osx-64::libfreetype6-2.14.1-h6912278_0 \n",
      "  libglib            conda-forge/osx-64::libglib-2.86.1-h6ca3a76_1 \n",
      "  libiconv           conda-forge/osx-64::libiconv-1.18-h57a12c2_2 \n",
      "  libintl            conda-forge/osx-64::libintl-0.25.1-h3184127_1 \n",
      "  libjpeg-turbo      conda-forge/osx-64::libjpeg-turbo-3.1.2-h8616949_0 \n",
      "  libpng             conda-forge/osx-64::libpng-1.6.50-h84aeda2_1 \n",
      "  libtiff            conda-forge/osx-64::libtiff-4.7.1-ha0a348c_1 \n",
      "  libwebp-base       conda-forge/osx-64::libwebp-base-1.6.0-hb807250_0 \n",
      "  libxcb             conda-forge/osx-64::libxcb-1.17.0-hf1f96e2_0 \n",
      "  munkres            conda-forge/noarch::munkres-1.1.4-pyhd8ed1ab_1 \n",
      "  openjpeg           conda-forge/osx-64::openjpeg-2.5.4-h87e8dc5_0 \n",
      "  pango              conda-forge/osx-64::pango-1.56.4-h6ef8af8_0 \n",
      "  pcre2              conda-forge/osx-64::pcre2-10.46-ha3e7e28_0 \n",
      "  pillow             conda-forge/osx-64::pillow-12.0.0-py313he918548_0 \n",
      "  pixman             conda-forge/osx-64::pixman-0.46.4-ha059160_1 \n",
      "  pthread-stubs      conda-forge/osx-64::pthread-stubs-0.4-h00291cd_1002 \n",
      "  pycparser          conda-forge/noarch::pycparser-2.22-pyh29332c3_1 \n",
      "  pydyf              conda-forge/noarch::pydyf-0.11.0-pyhd8ed1ab_1 \n",
      "  pyphen             conda-forge/noarch::pyphen-0.17.2-pyhd8ed1ab_0 \n",
      "  tinycss2           conda-forge/noarch::tinycss2-1.4.0-pyhd8ed1ab_0 \n",
      "  tinyhtml5          conda-forge/noarch::tinyhtml5-2.0.0-pyhe01879c_0 \n",
      "  weasyprint         conda-forge/noarch::weasyprint-66.0-pyhcf101f3_0 \n",
      "  webencodings       conda-forge/noarch::webencodings-0.5.1-pyhd8ed1ab_3 \n",
      "  xorg-libxau        conda-forge/osx-64::xorg-libxau-1.0.12-h6e16a3a_0 \n",
      "  xorg-libxdmcp      conda-forge/osx-64::xorg-libxdmcp-1.1.5-h00291cd_0 \n",
      "  zlib-ng            conda-forge/osx-64::zlib-ng-2.2.5-h55e386d_0 \n",
      "  zstd               conda-forge/osx-64::zstd-1.5.7-h8210216_2 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "libjpeg-turbo-3.1.2  | 572 KB    |                                       |   0% \n",
      "fonts-conda-ecosyste | 4 KB      |                                       |   0% \u001b[A\n",
      "\n",
      "brotli-1.2.0         | 20 KB     |                                       |   0% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "libbrotlienc-1.2.0   | 303 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libintl-0.25.1       | 95 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "zlib-ng-2.2.5        | 107 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "munkres-1.1.4        | 15 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libfreetype6-2.14.1  | 366 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "fonts-conda-forge-1  | 4 KB      |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "graphite2-1.3.14     | 83 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libglib-2.86.1       | 3.5 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "fonttools-4.60.1     | 2.8 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "fribidi-1.0.16       | 59 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "font-ttf-source-code | 684 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libdeflate-1.25      | 69 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libiconv-1.18        | 721 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tinycss2-1.4.0       | 28 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libpng-1.6.50        | 291 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "font-ttf-inconsolata | 94 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libtiff-4.7.1        | 395 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "pillow-12.0.0        | 952 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "font-ttf-ubuntu-0.83 | 1.5 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "pixman-0.46.4        | 382 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "libjpeg-turbo-3.1.2  | 572 KB    | #                                     |   3% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libintl-0.25.1       | 95 KB     | ######2                               |  17% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "brotli-1.2.0         | 20 KB     | ##############################        |  81% \u001b[A\u001b[A\n",
      "fonts-conda-ecosyste | 4 KB      | ##################################### | 100% \u001b[A\n",
      "\n",
      "\n",
      "libjpeg-turbo-3.1.2  | 572 KB    | ################5                     |  45% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "zlib-ng-2.2.5        | 107 KB    | #####5                                |  15% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "brotli-1.2.0         | 20 KB     | ##################################### | 100% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "munkres-1.1.4        | 15 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libfreetype6-2.14.1  | 366 KB    | #6                                    |   4% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "fonts-conda-forge-1  | 4 KB      | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libglib-2.86.1       | 3.5 MB    | 1                                     |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libintl-0.25.1       | 95 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "graphite2-1.3.14     | 83 KB     | #######                               |  19% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libintl-0.25.1       | 95 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "fribidi-1.0.16       | 59 KB     | #########9                            |  27% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "fonttools-4.60.1     | 2.8 MB    | 2                                     |   1% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "fonts-conda-ecosyste | 4 KB      | ##################################### | 100% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "font-ttf-source-code | 684 KB    | 8                                     |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libglib-2.86.1       | 3.5 MB    | ######                                |  16% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libdeflate-1.25      | 69 KB     | ########5                             |  23% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libiconv-1.18        | 721 KB    | 8                                     |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "fonttools-4.60.1     | 2.8 MB    | #########4                            |  25% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libglib-2.86.1       | 3.5 MB    | ###########1                          |  30% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "font-ttf-source-code | 684 KB    | ###############################1      |  84% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tinycss2-1.4.0       | 28 KB     | #####################4                |  58% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "libbrotlienc-1.2.0   | 303 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libiconv-1.18        | 721 KB    | #################################6    |  91% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libglib-2.86.1       | 3.5 MB    | ###############8                      |  43% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "fonttools-4.60.1     | 2.8 MB    | ###############8                      |  43% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "font-ttf-inconsolata | 94 KB     | ######2                               |  17% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libpng-1.6.50        | 291 KB    | ##                                    |   6% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libtiff-4.7.1        | 395 KB    | #4                                    |   4% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "zlib-ng-2.2.5        | 107 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "pillow-12.0.0        | 952 KB    | 6                                     |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "zlib-ng-2.2.5        | 107 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libglib-2.86.1       | 3.5 MB    | #######################5              |  64% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "fonttools-4.60.1     | 2.8 MB    | #########################7            |  70% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "font-ttf-ubuntu-0.83 | 1.5 MB    | 3                                     |   1% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "pixman-0.46.4        | 382 KB    | #5                                    |   4% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "pillow-12.0.0        | 952 KB    | #############6                        |  37% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libglib-2.86.1       | 3.5 MB    | #############################1        |  79% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "fonttools-4.60.1     | 2.8 MB    | ################################8     |  89% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "font-ttf-ubuntu-0.83 | 1.5 MB    | #################5                    |  48% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libjpeg-turbo-3.1.2  | 572 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "munkres-1.1.4        | 15 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "fonts-conda-forge-1  | 4 KB      | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libfreetype6-2.14.1  | 366 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libfreetype6-2.14.1  | 366 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "graphite2-1.3.14     | 83 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "graphite2-1.3.14     | 83 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "fribidi-1.0.16       | 59 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "fribidi-1.0.16       | 59 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libdeflate-1.25      | 69 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libdeflate-1.25      | 69 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tinycss2-1.4.0       | 28 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tinycss2-1.4.0       | 28 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "font-ttf-source-code | 684 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libiconv-1.18        | 721 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "font-ttf-inconsolata | 94 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "font-ttf-inconsolata | 94 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libpng-1.6.50        | 291 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libpng-1.6.50        | 291 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libtiff-4.7.1        | 395 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libtiff-4.7.1        | 395 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "pixman-0.46.4        | 382 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "pixman-0.46.4        | 382 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "pillow-12.0.0        | 952 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "pillow-12.0.0        | 952 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "fonttools-4.60.1     | 2.8 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libglib-2.86.1       | 3.5 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libglib-2.86.1       | 3.5 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "font-ttf-ubuntu-0.83 | 1.5 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "font-ttf-ubuntu-0.83 | 1.5 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \n",
      "                                                                                \u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting markdown\n",
      "  Using cached markdown-3.9-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: weasyprint in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (66.0)\n",
      "Requirement already satisfied: pydyf>=0.11.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from weasyprint) (0.11.0)\n",
      "Requirement already satisfied: cffi>=0.6 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from weasyprint) (2.0.0)\n",
      "Requirement already satisfied: tinyhtml5>=2.0.0b1 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from weasyprint) (2.0.0)\n",
      "Requirement already satisfied: tinycss2>=1.4.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from weasyprint) (1.4.0)\n",
      "Requirement already satisfied: cssselect2>=0.8.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from weasyprint) (0.8.0)\n",
      "Requirement already satisfied: Pyphen>=0.9.1 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from weasyprint) (0.17.2)\n",
      "Requirement already satisfied: Pillow>=9.1.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from weasyprint) (12.0.0)\n",
      "Requirement already satisfied: fonttools>=4.0.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from fonttools[woff]>=4.0.0->weasyprint) (4.60.1)\n",
      "Requirement already satisfied: pycparser in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from cffi>=0.6->weasyprint) (2.22)\n",
      "Requirement already satisfied: webencodings in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from cssselect2>=0.8.0->weasyprint) (0.5.1)\n",
      "Collecting brotli>=1.0.1 (from fonttools[woff]>=4.0.0->weasyprint)\n",
      "  Downloading Brotli-1.1.0-cp313-cp313-macosx_10_13_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting zopfli>=0.1.4 (from fonttools[woff]>=4.0.0->weasyprint)\n",
      "  Downloading zopfli-0.2.3.post1-cp313-cp313-macosx_10_13_x86_64.whl.metadata (2.9 kB)\n",
      "Using cached markdown-3.9-py3-none-any.whl (107 kB)\n",
      "Downloading Brotli-1.1.0-cp313-cp313-macosx_10_13_x86_64.whl (422 kB)\n",
      "Downloading zopfli-0.2.3.post1-cp313-cp313-macosx_10_13_x86_64.whl (163 kB)\n",
      "Installing collected packages: brotli, zopfli, markdown\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [markdown]2/3\u001b[0m [markdown]\n",
      "\u001b[1A\u001b[2KSuccessfully installed brotli-1.1.0 markdown-3.9 zopfli-0.2.3.post1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install dotenv langchain langchain-google\n",
    "# %pip install langchain_community \n",
    "# %pip install tavily-python\n",
    "# %pip install langchain-community beautifulsoup4\n",
    "# %pip install langchain_google_genai\n",
    "# %conda install weasyprint  # need to run this as well\n",
    "# %pip install markdown weasyprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf7c942",
   "metadata": {},
   "source": [
    "# Resume writer\n",
    "\n",
    "This notebook helps you create a tailored resume and cover letter for a specific job description using AI.\n",
    "It uses LangChain and Google GenAI to process your existing resume and the job description, extracting relevant information and formatting it appropriately.\n",
    "It expects a version of the resume in pdf or docx format.\n",
    "The resume should contain sections like \"Work Experience\", \"Skills\", \"Education\", and your personal information, etc.\n",
    "It also expects a job description as a URL (tested it with LinkedIn but should work with others too).\n",
    "\n",
    "It performs web search to get information about the hiring company to better tailor the resume and cover letter.\n",
    "\n",
    "What you need to provide:\n",
    "- Your resume in pdf or docx format\n",
    "- A job description URL \n",
    "- Add a `.env` file with your Google GenAI API key and the Tavily API key as follows\n",
    "- Modify the `Setup variables` cell to point to your resume and the job description URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3efbf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from langchain_community.tools import TavilySearchResults\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.agents import create_agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3666972c",
   "metadata": {},
   "source": [
    "## Setup variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff61513f",
   "metadata": {},
   "source": [
    "Setup the keys we need (for Tavily and GOOGLE gemini)\n",
    "\n",
    "We need to store the keys to a file named `.env` in the same directory as this notebook with the following content:\n",
    "\n",
    "```\n",
    "TAVILY_API_KEY=your_tavily_api_key\n",
    "GOOGLE_API_KEY=your_google_api_key\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27011a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "google_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    "tavily_key=os.getenv(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41946d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the variables\n",
    "# this will become some sort of an input\n",
    "\n",
    "company=None\n",
    "resume=\"resume.pdf\"\n",
    "jd=None\n",
    "title=None\n",
    "job_level=None # e.g director etc\n",
    "jd_url=\"https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/4314922582\"\n",
    "# jd_url=\"https://www.linkedin.com/jobs/view/4314922582/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27f42a91",
   "metadata": {},
   "outputs": [
    {
     "ename": "DefaultCredentialsError",
     "evalue": "Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDefaultCredentialsError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# create the model to use\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m model=\u001b[43minit_chat_model\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgoogle_genai:gemini-2.5-flash-lite\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/everyday_repo/lib/python3.13/site-packages/langchain/chat_models/base.py:311\u001b[39m, in \u001b[36minit_chat_model\u001b[39m\u001b[34m(model, model_provider, configurable_fields, config_prefix, **kwargs)\u001b[39m\n\u001b[32m    303\u001b[39m     warnings.warn(\n\u001b[32m    304\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_prefix\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m has been set but no fields are configurable. Set \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    305\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`configurable_fields=(...)` to specify the model params that are \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    306\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mconfigurable.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    307\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    308\u001b[39m     )\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m configurable_fields:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_init_chat_model_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model:\n\u001b[32m    317\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m] = model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/everyday_repo/lib/python3.13/site-packages/langchain/chat_models/base.py:368\u001b[39m, in \u001b[36m_init_chat_model_helper\u001b[39m\u001b[34m(model, model_provider, **kwargs)\u001b[39m\n\u001b[32m    365\u001b[39m     _check_pkg(\u001b[33m\"\u001b[39m\u001b[33mlangchain_google_genai\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    366\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_google_genai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatGoogleGenerativeAI\n\u001b[32m--> \u001b[39m\u001b[32m368\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mChatGoogleGenerativeAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_provider == \u001b[33m\"\u001b[39m\u001b[33mfireworks\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    370\u001b[39m     _check_pkg(\u001b[33m\"\u001b[39m\u001b[33mlangchain_fireworks\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/everyday_repo/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1854\u001b[39m, in \u001b[36mChatGoogleGenerativeAI.__init__\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m   1847\u001b[39m         suggestion = (\n\u001b[32m   1848\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m Did you mean: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggestions[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m?\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m suggestions \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1849\u001b[39m         )\n\u001b[32m   1850\u001b[39m         logger.warning(\n\u001b[32m   1851\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnexpected argument \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1852\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mprovided to ChatGoogleGenerativeAI.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1853\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1854\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/everyday_repo/lib/python3.13/site-packages/langchain_core/load/serializable.py:116\u001b[39m, in \u001b[36mSerializable.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    115\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419  # Intentional blank docstring\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/everyday_repo/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1915\u001b[39m, in \u001b[36mChatGoogleGenerativeAI.validate_environment\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1913\u001b[39m         google_api_key = \u001b[38;5;28mself\u001b[39m.google_api_key\n\u001b[32m   1914\u001b[39m transport: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28mself\u001b[39m.transport\n\u001b[32m-> \u001b[39m\u001b[32m1915\u001b[39m \u001b[38;5;28mself\u001b[39m.client = \u001b[43mgenaix\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild_generative_service\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgoogle_api_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1919\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1920\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransport\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1921\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1922\u001b[39m \u001b[38;5;28mself\u001b[39m.async_client_running = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1923\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/everyday_repo/lib/python3.13/site-packages/langchain_google_genai/_genai_extension.py:286\u001b[39m, in \u001b[36mbuild_generative_service\u001b[39m\u001b[34m(credentials, api_key, client_options, client_info, transport)\u001b[39m\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbuild_generative_service\u001b[39m(\n\u001b[32m    273\u001b[39m     credentials: Optional[credentials.Credentials] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    274\u001b[39m     api_key: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    277\u001b[39m     transport: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    278\u001b[39m ) -> v1betaGenerativeServiceClient:\n\u001b[32m    279\u001b[39m     config = _prepare_config(\n\u001b[32m    280\u001b[39m         credentials=credentials,\n\u001b[32m    281\u001b[39m         api_key=api_key,\n\u001b[32m   (...)\u001b[39m\u001b[32m    284\u001b[39m         client_info=client_info,\n\u001b[32m    285\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mv1betaGenerativeServiceClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/everyday_repo/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:698\u001b[39m, in \u001b[36mGenerativeServiceClient.__init__\u001b[39m\u001b[34m(self, credentials, transport, client_options, client_info)\u001b[39m\n\u001b[32m    689\u001b[39m     transport_init: Union[\n\u001b[32m    690\u001b[39m         Type[GenerativeServiceTransport],\n\u001b[32m    691\u001b[39m         Callable[..., GenerativeServiceTransport],\n\u001b[32m   (...)\u001b[39m\u001b[32m    695\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m cast(Callable[..., GenerativeServiceTransport], transport)\n\u001b[32m    696\u001b[39m     )\n\u001b[32m    697\u001b[39m     \u001b[38;5;66;03m# initialize with the provided callable or the passed in class\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m698\u001b[39m     \u001b[38;5;28mself\u001b[39m._transport = \u001b[43mtransport_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcredentials_file\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client_options\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcredentials_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_endpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscopes\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client_options\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscopes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient_cert_source_for_mtls\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client_cert_source\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquota_project_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client_options\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquota_project_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    706\u001b[39m \u001b[43m        \u001b[49m\u001b[43malways_use_jwt_access\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapi_audience\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client_options\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapi_audience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33masync\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m._transport):\n\u001b[32m    711\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m CLIENT_LOGGING_SUPPORTED \u001b[38;5;129;01mand\u001b[39;00m _LOGGER.isEnabledFor(\n\u001b[32m    712\u001b[39m         std_logging.DEBUG\n\u001b[32m    713\u001b[39m     ):  \u001b[38;5;66;03m# pragma: NO COVER\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/everyday_repo/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py:235\u001b[39m, in \u001b[36mGenerativeServiceGrpcTransport.__init__\u001b[39m\u001b[34m(self, host, credentials, credentials_file, scopes, channel, api_mtls_endpoint, client_cert_source, ssl_channel_credentials, client_cert_source_for_mtls, quota_project_id, client_info, always_use_jwt_access, api_audience)\u001b[39m\n\u001b[32m    230\u001b[39m             \u001b[38;5;28mself\u001b[39m._ssl_channel_credentials = grpc.ssl_channel_credentials(\n\u001b[32m    231\u001b[39m                 certificate_chain=cert, private_key=key\n\u001b[32m    232\u001b[39m             )\n\u001b[32m    234\u001b[39m \u001b[38;5;66;03m# The base transport sets the host, credentials and scopes\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcredentials_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscopes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscopes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquota_project_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquota_project_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m    \u001b[49m\u001b[43malways_use_jwt_access\u001b[49m\u001b[43m=\u001b[49m\u001b[43malways_use_jwt_access\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_audience\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_audience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._grpc_channel:\n\u001b[32m    247\u001b[39m     \u001b[38;5;66;03m# initialize with the provided callable or the default channel\u001b[39;00m\n\u001b[32m    248\u001b[39m     channel_init = channel \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).create_channel\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/everyday_repo/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/base.py:105\u001b[39m, in \u001b[36mGenerativeServiceTransport.__init__\u001b[39m\u001b[34m(self, host, credentials, credentials_file, scopes, quota_project_id, client_info, always_use_jwt_access, api_audience, **kwargs)\u001b[39m\n\u001b[32m    101\u001b[39m     credentials, _ = google.auth.load_credentials_from_file(\n\u001b[32m    102\u001b[39m         credentials_file, **scopes_kwargs, quota_project_id=quota_project_id\n\u001b[32m    103\u001b[39m     )\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m credentials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._ignore_credentials:\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     credentials, _ = \u001b[43mgoogle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mscopes_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquota_project_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquota_project_id\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m     \u001b[38;5;66;03m# Don't apply audience if the credentials file passed from user.\u001b[39;00m\n\u001b[32m    109\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(credentials, \u001b[33m\"\u001b[39m\u001b[33mwith_gdch_audience\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/everyday_repo/lib/python3.13/site-packages/google/auth/_default.py:739\u001b[39m, in \u001b[36mdefault\u001b[39m\u001b[34m(scopes, request, quota_project_id, default_scopes)\u001b[39m\n\u001b[32m    731\u001b[39m             _LOGGER.warning(\n\u001b[32m    732\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mNo project ID could be determined. Consider running \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    733\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m`gcloud config set project` or setting the \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    734\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33menvironment variable\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    735\u001b[39m                 environment_vars.PROJECT,\n\u001b[32m    736\u001b[39m             )\n\u001b[32m    737\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m credentials, effective_project_id\n\u001b[32m--> \u001b[39m\u001b[32m739\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)\n",
      "\u001b[31mDefaultCredentialsError\u001b[39m: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information."
     ]
    }
   ],
   "source": [
    "# create the model to use\n",
    "model=init_chat_model(\"google_genai:gemini-2.5-flash-lite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "97c63695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful functions\n",
    "def strip_markdown_backticks(text):\n",
    "    \"\"\"\n",
    "    Strip markdown code block backticks from text.\n",
    "    Handles both triple backticks (```) and triple backticks with language specifier.\n",
    "    \"\"\"\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Remove starting backticks (with or without language specifier)\n",
    "    if text.startswith('```'):\n",
    "        # Find the end of the first line (after language specifier if present)\n",
    "        first_newline = text.find('\\n')\n",
    "        if first_newline != -1:\n",
    "            text = text[first_newline + 1:]\n",
    "        else:\n",
    "            # No newline found, just remove the backticks\n",
    "            text = text[3:]\n",
    "    \n",
    "    # Remove ending backticks\n",
    "    if text.endswith('```'):\n",
    "        text = text[:-3]\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def markdown_to_pdf(markdown_text, output_path):\n",
    "    \"\"\"\n",
    "    Convert markdown to PDF using weasyprint (best formatting)\n",
    "    \"\"\"\n",
    "    import markdown\n",
    "    from weasyprint import HTML, CSS\n",
    "    from weasyprint.text.fonts import FontConfiguration\n",
    "    \n",
    "    # Convert markdown to HTML\n",
    "    html = markdown.markdown(markdown_text)\n",
    "    \n",
    "    # Add basic styling\n",
    "    styled_html = f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "        <meta charset=\"utf-8\">\n",
    "        <style>\n",
    "            body {{ \n",
    "                font-family: Arial, sans-serif; \n",
    "                margin: 40px; \n",
    "                line-height: 1; \n",
    "                text-align: justify;\n",
    "                text-justify: inter-word;\n",
    "            }}\n",
    "            h1, h2, h3 {{ \n",
    "                color: #333; \n",
    "                text-align: left;\n",
    "            }}\n",
    "            h1 {{ border-bottom: 2px solid #333; }}\n",
    "            h2 {{ border-bottom: 1px solid #666; }}\n",
    "            ul {{ \n",
    "                margin-left: 20px; \n",
    "                text-align: justify;\n",
    "            }}\n",
    "            li {{\n",
    "                text-align: justify;\n",
    "                margin-bottom: 3px;\n",
    "            }}\n",
    "            p {{\n",
    "                text-align: justify;\n",
    "                text-justify: inter-word;\n",
    "            }}\n",
    "            strong {{ font-weight: bold; }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        {html}\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert to PDF\n",
    "    HTML(string=styled_html).write_pdf(output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e71b253",
   "metadata": {},
   "source": [
    "# Overview of the process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687fc304",
   "metadata": {},
   "source": [
    "The structure is as follows:\n",
    "\n",
    "- get information about the company (values and specialization - it uses standard web search)\n",
    "- reads the resume (a word document with all the information we have)\n",
    "- reads the job description as a simple text file\n",
    "- generates the resume. Includes a summary paragraph\n",
    "- generates a cover letter.\n",
    "- passes it through an agent that acts as a recruiter\n",
    "- passes it through an ATS generator to confirm that it is ATS compatible\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522a6106",
   "metadata": {},
   "source": [
    "## Get the job description details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5444d27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting information for job https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/4314922582\n"
     ]
    }
   ],
   "source": [
    "#parse the page with the job description\n",
    "# langchain gets the public description of the job\n",
    "# make sure we are not behind a proxy\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "print(f\"Getting information for job {jd_url}\")\n",
    "loader = WebBaseLoader(jd_url )\n",
    "loader.default_parser=\"html.parser\"\n",
    "docs = loader.load()\n",
    "# docs[0]\n",
    "\n",
    "\n",
    "# from langchain_unstructured import UnstructuredLoader\n",
    "# loader = UnstructuredLoader(web_url=jd_url)\n",
    "# docs=loader.load()\n",
    "\n",
    "description = docs[0].page_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "991d4bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/4314922582'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGroup Head - Scientific Innovation (RX & AI and Computational Sciences)\\n\\n\\n\\n\\n\\n\\n\\n                Novartis\\n              \\n\\n\\n              Basel, Switzerland\\n            \\n\\n\\n\\n          \\n\\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n\\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n\\n      1 week ago\\n  \\n        \\n\\n          26 applicants\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n          See who Novartis has hired for this role\\n        \\n\\n\\n\\n\\n\\n\\n      Apply\\n      \\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJoin or sign in to find your next job\\nJoin to apply for the Group Head - Scientific Innovation (RX & AI and Computational Sciences) role at Novartis\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                      Not you?\\n              \\n\\n\\n\\n\\n\\n                    Remove photo\\n                  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFirst name\\n\\n\\n\\n\\n\\nLast name\\n\\n\\n\\n\\n\\n\\n\\nEmail\\n\\n\\n\\n\\n\\n\\n\\nPassword (6+ characters)\\n\\n \\n\\n\\n\\n                By clicking Agree & Join, you agree to the LinkedIn User Agreement, Privacy Policy and Cookie Policy.\\n              \\n\\n                Continue\\n              \\n\\n                Agree & Join\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nor\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n                        Apply on company website\\n                    \\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\nSecurity verification\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Already on LinkedIn? Sign in\\n          \\n\\n\\n\\n\\n\\n\\n                Save\\n            \\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                      Report this job\\n                    \\n    \\n\\n\\n\\n\\n \\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSummaryNovartis has embraced a bold strategy to drive a company-wide digital transformation. Our objective is to position Novartis as an industry leader by proactively adopting digital technologies that foster innovative approaches to hasten drug discovery and development. By utilizing both internal and external R&D data with the power of data science, predictive models, generative AI, and machine learning, our objective is to identify new targets, create more effective therapeutic molecules, better predict drug pharmacokinetics and safety risks, refine clinical trial design, and significantly shorten development cycles. The AICS team leads BR in exploring and applying advanced AI and ML methodologies to generate novel drug discovery insights, and to speed and improve drug discovery efficiency whilst focusing on patients’ needs.AI & Computational Sciences (AICS) partners with drug discovery teams, raises the level of AI expertise across Biomedical Research (BR) and ensures that BR science keeps up with the rapidly evolving ecosystem of AI technologies by connecting with AI leaders in academia and industry.This role leads the Scientific Innovation group within AICS and is tasked with leading a group of scientists with significant domain expertise in biomedical research along with being AI-native, and also act as the primary SME liaison of AICS to BR Disease Areas (DAs) and Function Areas (FAs).About The RolePurpose of the roleScientific Innovation Group LeadLead a team of AI researchers who also bring significant domain area expertiseWork with AI modelers to ensure models are robust and performantWork with Engineering and Product Development resources to ensure delivery of AI solutionsRaise the awareness of AI applications to aid in answering key research questions across BRHelp position AI-aided drug discovery contributions to deliver and support progress of Biomedical Research’s portfolio, enable new kinds of therapeutic discoveries, shorten cycle times, and increase efficiency.Collaboration & partnershipAct as the primary liaison as a subject matter expert for AICS, for identification of research questions, solution design, and evangelism of results and impacts of AI across Biomedical ResearchRegularly communicate, engage, align with AICS teams, broader data science community, and senior scientists.Initiate and lead key high-value internal collaborations across BR.Help to design translatable metrics for AI models that will lead to tangible impact in collaboration with BR DAs and FAs.What You’ll Bring To The Role10+ years of significant experience in innovation, development, deployment and continuous support of Machine Learning data management and modelingExpertise in bringing advanced analytics insights and actions to a large Research organization & deep understanding of Drug developmentPassion for understanding emerging technologies with pragmatic insight into where those technologies can be integrated into business solutionsAbility to balance requirements, manage expectations, and drive effective results using a proactive attitude towards identifying and resolving issuesEntrepreneurial spirit with can-do, pro-active attitudeStrong organizational, problem-solving, and influencing skills and demonstrated track record of exceptional teamworkExcellent interpersonal, communication, and presentation skillsAbility to execute and prioritize well in a complex matrixed environment.Operational and functional leadershipNovartis is committed to building an outstanding, inclusive work environment and diverse teams representative of the patients and communities we serve.Why Novartis: Helping people with disease and their families takes more than innovative science. It takes a community of smart, passionate people like you. Collaborating, supporting and inspiring each other. Combining to achieve breakthroughs that change patients’ lives. Ready to create a brighter future together? https://www.novartis.com/about/strategy/people-and-cultureJoin our Novartis Network: Not the right Novartis role for you? Sign up to our talent community to stay connected and learn about suitable career opportunities as soon as they come up: https://talentnetwork.novartis.com/networkBenefits and Rewards: Read our handbook to learn about all the ways we’ll help you thrive personally and professionally: https://www.novartis.com/careers/benefits-rewards\\n        \\n\\n\\n        \\n            Show more\\n          \\n\\n          \\n\\n\\n\\n        \\n            Show less\\n          \\n\\n          \\n\\n \\n\\n\\n\\n\\n            Seniority level\\n          \\n\\n            Director\\n          \\n\\n\\n\\n            Employment type\\n          \\n\\n            Full-time\\n          \\n\\n\\n\\n              Job function\\n            \\n\\n              Product Management and Marketing\\n            \\n\\n\\n\\n              Industries\\n            \\n\\n            Pharmaceutical Manufacturing\\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReferrals increase your chances of interviewing at Novartis by 2x\\n\\n                See who you know\\n              \\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# description=\"\\n\".join([d.page_content for d in docs[0]])\n",
    "# print(f\"Job Description\\n{description} \")\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28763fc3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m      6\u001b[39m prompt=\u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[33mParse the following job description text \u001b[39m\n\u001b[32m      8\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m \n\u001b[32m     24\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# we use the same model as before but we dont use any tools\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m agent=create_agent(model=\u001b[43mmodel\u001b[49m,\n\u001b[32m     28\u001b[39m                    tools=[],\n\u001b[32m     29\u001b[39m                    system_prompt=\u001b[33m\"\u001b[39m\u001b[33mYou are a helpful assistant that extracts information from job descriptions. Be consice and accurate.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     30\u001b[39m )\n\u001b[32m     31\u001b[39m response = agent.invoke(\n\u001b[32m     32\u001b[39m   {\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [{\u001b[33m'\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m'\u001b[39m:\u001b[33m'\u001b[39m\u001b[33muser\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m: prompt}]}\n\u001b[32m     33\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# ask an LLM to extrac the following information:\n",
    "# job title\n",
    "# job description\n",
    "# job level \n",
    "# and return it as a json string\n",
    "prompt=f\"\"\"\n",
    "Parse the following job description text \n",
    "\n",
    "```\n",
    "{description}\n",
    "```\n",
    "\n",
    "and extract the following information\n",
    "company name.\n",
    "job title.\n",
    "job description and required qualifications, if the text includes description about the benefits\n",
    "and company values don't include them in the description. THe job description is typically found\n",
    "under sections like \"Responsibilities\", \"What you will do\", \"Qualifications\", \"Summary\", \"Role description\" etc.\n",
    "seniority level of the job (e.g Senior, Director etc).\n",
    "\n",
    "return the results as a json string that can be parsed directly using the keys `company_name`, `job_title`, \n",
    "`job_description` and `seniority_level` (e.g Senior, Director etc). If some information is missing return null for that field.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# we use the same model as before but we dont use any tools\n",
    "agent=create_agent(model=model,\n",
    "                   tools=[],\n",
    "                   system_prompt=\"You are a helpful assistant that extracts information from job descriptions. Be consice and accurate.\"\n",
    ")\n",
    "response = agent.invoke(\n",
    "  {\"messages\": [{'role':'user', 'content': prompt}]}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6647c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"company_name\": \"Novartis\",\n",
      "  \"job_title\": \"Group Head - Scientific Innovation (RX & AI and Computational Sciences)\",\n",
      "  \"job_description\": \"Novartis has embraced a bold strategy to drive a company-wide digital transformation. Our objective is to position Novartis as an industry leader by proactively adopting digital technologies that foster innovative approaches to hasten drug discovery and development. By utilizing both internal and external R&D data with the power of data science, predictive models, generative AI, and machine learning, our objective is to identify new targets, create more effective therapeutic molecules, better predict drug pharmacokinetics and safety risks, refine clinical trial design, and significantly shorten development cycles. The AICS team leads BR in exploring and applying advanced AI and ML methodologies to generate novel drug discovery insights, and to speed and improve drug discovery efficiency whilst focusing on patients’ needs.AI & Computational Sciences (AICS) partners with drug discovery teams, raises the level of AI expertise across Biomedical Research (BR) and ensures that BR science keeps up with the rapidly evolving ecosystem of AI technologies by connecting with AI leaders in academia and industry.This role leads the Scientific Innovation group within AICS and is tasked with leading a group of scientists with significant domain expertise in biomedical research along with being AI-native, and also act as the primary SME liaison of AICS to BR Disease Areas (DAs) and Function Areas (FAs).Purpose of the roleScientific Innovation Group LeadLead a team of AI researchers who also bring significant domain area expertiseWork with AI modelers to ensure models are robust and performantWork with Engineering and Product Development resources to ensure delivery of AI solutionsRaise the awareness of AI applications to aid in answering key research questions across BRHelp position AI-aided drug discovery contributions to deliver and support progress of Biomedical Research’s portfolio, enable new kinds of therapeutic discoveries, shorten cycle times, and increase efficiency.Collaboration & partnershipAct as the primary liaison as a subject matter expert for AICS, for identification of research questions, solution design, and evangelism of results and impacts of AI across Biomedical ResearchRegularly communicate, engage, align with AICS teams, broader data science community, and senior scientists.Initiate and lead key high-value internal collaborations across BR.Help to design translatable metrics for AI models that will lead to tangible impact in collaboration with BR DAs and FAs.What You’ll Bring To The Role10+ years of significant experience in innovation, development, deployment and continuous support of Machine Learning data management and modelingExpertise in bringing advanced analytics insights and actions to a large Research organization & deep understanding of Drug developmentPassion for understanding emerging technologies with pragmatic insight into where those technologies can be integrated into business solutionsAbility to balance requirements, manage expectations, and drive effective results using a proactive attitude towards identifying and resolving issuesEntrepreneurial spirit with can-do, pro-active attitudeStrong organizational, problem-solving, and influencing skills and demonstrated track record of exceptional teamworkExcellent interpersonal, communication, and presentation skillsAbility to execute and prioritize well in a complex matrixed environment.\",\n",
      "  \"seniority_level\": \"Director\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#parse out the JSON components\n",
    "job_description=strip_markdown_backticks(\n",
    "  response['messages'][-1].content\n",
    ")\n",
    "jd=json.loads(job_description)\n",
    "# print(f\"{job_description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c5c750",
   "metadata": {},
   "source": [
    "## Load the resume \n",
    "Let's load the document that contains the information about the resume. It is expected to be a Word document\n",
    "or a PDF document.\n",
    "We can use the parsers as tools in an agent, and expect the LLM to decide if it needs to parse the corresponding document. But this way we need to make a call to the LLM.\n",
    "To avoid using extra API calls, we will parse the document locally, and incorporate the contents in the context of the prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "28a05fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "if resume.endswith(\".docx\"):\n",
    "  loader=Docx2txtLoader( resume )\n",
    "if resume.endswith(\".pdf\"):\n",
    "  loader=PyPDFLoader(resume)\n",
    "resume_data=loader.load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e393141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Konstantinos Mavrommatis, PhD Bioinformatics expert | Data Scientist  Areuse, NE, Switzerland       |     mavrommatis.konstantinos@gmail.com  +41 79 248 9368  | /in/mavrommatiskonstantinos/  |  scholar.google.com/citations  Professional summary  Highly accomplished Bioinformatician with over 20 years of experience in genomics, metagenomics, and multi-omics data integration across academic, biotech, and pharmaceutical environments. Proven expertise in developing state-of-the-art bioinformatics pipelines, establishing robust data management systems, and deploying scalable HPC/cloud infrastructures to accelerate discovery and innovation. Extensive track record in multi-omics integration (genomics, transcriptomics, proteomics, metabolomics) for biomarker discovery, microbiome research, and translational science. Recognized for 350+ scientiﬁc publications, leadership of cross-functional teams, and delivery of innovative bioinformatics solutions that enable reproducibility, scalability, and actionable insights in R&D.  Education  Ph.D., in Molecular Biology and Biomedicine - University of Crete, Greece  1997 – 2002  M.Sc., in Biotechnology - University of Crete, Greece  1995 – 1997  B.Sc., in Biological Sciences - University of Athens, Greece  1995  Experience   Scientiﬁc Director, Applied Bioinformatics. | Bristol Myers Squibb, formerly Celgene. Boudry, CH/San Francisco, CA, USA | 2020 – Present • Designed, implemented and optimized bioinformatics pipelines and algorithms for multi-omics datasets (genomics, transcriptomics, metagenomics, metabolomics), enabling biomarker discovery, patient, classiﬁcation, community composition analysis, and functional annotation across 20+ clinical programs. • Established and maintained data management frameworks ensuring reproducibility, traceability, and accessibility of large-scale datasets, preparing them for machine learning and advanced analytics. • Integrated multi-omics datasets to uncover correlations and predictive insights, accelerating hypothesis-driven research and target identiﬁcation. • Partnered with (micro)biologists, data scientists, and product teams to design experiments, interpret results, and translate complex data into actionable recommendations. • Contributed to the automation of experimental analyses, streamlining molecular biology workﬂows to support reproducible and high-throughput experimentation. • Built and managed strategic collaborations with academia and biopharma, accelerating microbiome and biomarker discovery pipelines. • Delivered technical reports and presentations to communicate ﬁndings eﬀectively to internal and external stakeholders. • Directed strategic initiatives as a primary liaison between SMEs, IT, and R&D, designing and deploying cutting-edge data science and AI capabilities to accelerate drug discovery, development, and supply.  • Orchestrated the application of ML and advanced AI-driven algorithms for genomics analysis, biomarker identiﬁcation, and elucidation of on-target and oﬀ-target pathways. Supportclinical development strategies by characterizing patient populations and identifying biomarker candidates.  • Managed and co-led multiple external collaborations and strategic partnerships with academic institutions and major biopharma companies, leveraging cutting-edge data and analytic methodologies to advance drug discovery and validation pipelines. • Led and mentored a cross-functional team of scientists and data scientists, allocating resources and prioritizing a data science portfolio to support 10+ clinical trials and 20+ assets from therapeutic discovery through preclinical candidate programs, contributing to the organization's overall success through strategic decision-making on talent, resourcing, and capability development. • Deﬁned and executed enterprise-wide data engineering strategies, designing and scaling high-performance data platforms and pipelines that enabled advanced analytics and AI across R&D. • Delivered scalable data solutions for translational research, integrating structured and unstructured datasets to improve data quality, accelerate target identiﬁcation, and enable data-driven decision-making across global research programs..  • Built and managed strategic partnerships with academia and industry, advancing data engineering practices and accelerating delivery of enterprise-scale data platforms and analytics solutions. • Led and developed a multidisciplinary global team of engineers and scientists, fostering a culture of technical excellence, agile delivery, and innovation to scale enterprise data platforms across multiple business domains. • Directed cross-disciplinary teams, acting as a primary liaison between SMEs, IT, and research, to drive NGS analytics innovation for tens of clinical trials and 20+ assets. • Orchestrated the application of ML and advanced AI-driven algorithms for genomics analysis, biomarker identiﬁcation, and mode of action elucidation. • Contributed strategically to data ingestion, new assay validation, and cohort identiﬁcation by introducing methods for data analysis across all company programs. • Managed and co-led collaborative digital transformation projects with academic institutions and major biopharma partners, accelerating drug discovery and validation pipelines. • Led and mentored a cross-functional team of scientists and data scientists, allocating resources and prioritizing a data science portfolio to support 10+ clinical trials and 20+ assets from therapeutic discovery through preclinical candidate programs. •   Scientist to Sr Principal Scientist,  Bioinformatics | Bristol Myers Squibb, formerly Celgene. San Francisco, CA, USA | 2012 - 2020 • Developed and implemented state-of-the-art NGS and multi-omics pipelines, enabling biomarker discovery, patient stratiﬁcation, and functional annotation across multiple oncology indications. • Designed and deployed a cloud-enabled HPC research environment processing >1PB of omics data, reducing costs by 5x and improving throughput >10x, ensuring scalable and reproducible workﬂows. • Engineered a centralized data management system for >100,000 NGS samples, ensuring data integrity, version control, and compliance with reproducibility standards. • Advanced multi-omics integration approaches, combining molecular and transcriptomic datasets to support predictive modeling and early discovery.• Partnered with research teams to translate high-dimensional datasets into actionable insights, guiding R&D decision-making. • Contributed to automation of data analyses, standardizing sequencing workﬂows across sites and enabling reproducibility at scale. • Spearheaded the development of multiple new methods for NGS analysis and high-throughput data processing in oncology projects, working closely with research teams to uncover novel insights, that enabled the identiﬁcation of biomarkers and patient classiﬁcation schemas for 4+ indications • Contributed to the technology roadmap by designing and implementing a hybrid cloud research compute environment (AWS) that processed >1PB of data, reducing costs by 5x and increasing processing speed by >10x, directly enhancing enterprise IT system capabilities for data lake architecture.  • Designed and implemented a robust data management system for large NGS datasets, enabling ingestion and tracking of >100,000 NGS samples, establishing critical data infrastructure for R&D. • Collaborated closely with research teams to integrate molecule attribute data alongside biological data, informing early drug discovery and development • Directed strategic initiatives as a primary liaison between SMEs, IT, and R&D, designing and deploying cutting-edge data science and AI capabilities to accelerate drug discovery, development, and supply.  • Orchestrated the application of machine learning and AI-driven algorithms to enable the discovery of translational biomarkers and disease-relevant target candidates, and to improve the predictive validity of pre-clinical models.  • Managed and co-led multiple external collaborations and strategic partnerships with academic institutions and major biopharma companies, leveraging cutting-edge data and analytic methodologies to advance drug discovery and validation pipelines. • Shaped, mentored, and led a high-performing, multidisciplinary team of data scientists and computational biology experts, fostering an innovative work environment to support drug discovery and translational research across global sites. • Spearheaded the development of multiple new methods for NGS analysis and high-throughput data processing in oncology projects, working closely with research teams to uncover novel insights, that enabled the identiﬁcation of biomarkers and patient classiﬁcation schemas for 4+ indications • Contributed to the technology roadmap by designing and implementing a hybrid cloud research compute environment (AWS) that processed >1PB of data, reducing costs by 5x and increasing processing speed by >10x, directly enhancing enterprise IT system capabilities for data lake architecture.  • Designed and implemented a robust data management system for large NGS datasets, enabling ingestion and tracking of >100,000 NGS samples, establishing critical data infrastructure for R&D. • Collaborated closely with research teams to integrate molecule attribute data alongside biological data, informing early drug discovery and development • Developed and scaled enterprise-grade pipelines for large-scale NGS analysis, delivering data products that supported biomarker discovery and patient stratiﬁcation across multiple oncology indications • Designed and deployed a cloud-native platform processing >1PB of data, reducing costs ﬁvefold and increasing throughput tenfold, laying the foundation for enterprise-scale data lake architecture supporting global R&D and commercial functions.• Engineered a centralized data management system supporting >100,000 NGS samples, ensuring scalability, traceability, and compliance, and enabling secure, reusable data pipelines for research teams. • Integrated cross-domain datasets (molecular and biological), enabling enterprise-wide data reuse and enhancing decision-making in early-stage discovery and development • Spearheaded the development of at least 3 new methods for NGS analysis and high-throughput data processing in oncology projects, working closely with research teams to uncover novel insights, that enabled the identiﬁcation of biomarkers and patient classiﬁcation schemas for at least 4 indications • Designed and implemented a hybrid cloud research compute environment to enhance genomics data processing capabilities, processing >1PB of data, reducing costs by 5x, and increasing processing speed by >10x.  • Developed and implemented a robust data management system for large NGS datasets, enabling ingestion and tracking of >100,000 NGS samples. • Collaborated closely with research teams to integrate molecule attribute data alongside biological data, informing early drug discovery and development •     Research Scientist, Bioinformatics | DOE Joint Genome Institute, Walnut Creek, CA, USA | 2007 - 2012 • Directed development of bioinformatics solutions for metagenomics and microbial genomics, contributing to breakthroughs in microbiome-focused projects with environmental impact. • Developed and integrated novel algoriths for the analysis and interpretation of microbial and metagenomica analyses. • Designed and deployed high-throughput pipelines for large-scale genomic and metagenomic integration, ensuring data quality, reproducibility, and scalability for downstream analyses. • Delivered insights that resulted in 100+ publications and collaborations with external stakeholders in microbiome and genomics research. • Key contributor to the Integrated Microbial Genomes (IMG) platform, enhancing data accessibility, reproducibility, and usability through backend methods and user-facing tools. • Led the Omics group in developing innovative bioinformatics solutions for microbial species analysis, contributing to breakthroughs in metagenomics and genomics research which formed the basis for large microbial projects with health (HMP) and environmental impact.. • Created advanced algorithms and pipelines for large-scale genomic and metagenomic data integration, enhancing the quality of data analysis and enabling new discoveries in microbial genomics resulting in over 100 publications and collaborations with external stakeholders. • Key contributor to the Integrated Microbial Genomes (IMG) platform, developing backend methods and user interfaces for the analysis of complex datasets. • Led the Omics group in developing innovative bioinformatics solutions for microbial species analysis, contributing to breakthroughs in metagenomics and genomics research which formed the basis for large microbial projects with health (HMP) and environmental impact.. • Created advanced algorithms and pipelines for large-scale genomic and metagenomic data integration, enhancing the quality of data analysis and enabling new discoveries in microbial genomics resulting in over 100 publications and collaborations with external stakeholders.• Key contributor to the Integrated Microbial Genomes (IMG) platform, developing backend methods and user interfaces for the analysis of complex datasets. • Directed the development of scalable bioinformatics and data engineering solutions, enabling large-scale genomics and metagenomics projects that informed health and environmental research initiatives. • Designed high-throughput pipelines for genomic and metagenomic data integration, ensuring data quality, reliability, and scalability, which supported 100+ publications and external collaborations. • Contributed key components to the development of the Integrated Microbial Genomes platform, enhancing data accessibility and reusability through robust backend infrastructure and user-facing solutions. • Led the Omics group in developing innovative bioinformatics solutions for microbial species analysis, contributing to breakthroughs in metagenomics and genomics research which formed the basis for large microbial projects with health (HMP) and environmental impact.. • Created advanced algorithms and pipelines for large-scale genomic and metagenomic data integration, enhancing the quality of data analysis and enabling new discoveries in microbial genomics resulting in over 100 publications and collaborations with external stakeholders. • Key contributor to the Integrated Microbial Genomes (IMG) platform, developing backend methods and user interfaces for the analysis of complex datasets. •   Postdoctoral Fellow, Bioinformatics | DOE Joint Genome Institute, Walnut Creek, CA, USA | 2004 – 2007 • Developed algorithms and workﬂows for large-scale microbial genomics projects, supporting high-throughput sequencing and multi-omics data integration. • Contributed to the design and implementation of the IMG platform, ensuring accessibility and reproducibility of microbial genome data for global researchers.    Achievements & Awards   • Over 350 publications on genomics, metagenomics, oncology and method development. Citations 37154, h-index 67, i10-index 242 • 2019: Celgene Award: Designed and implemented a hybrid cloud research compute environment to enhance genomics data processing capabilities which processed >1PB of data, reducing processing costs by 5x and increasing processing speed by more than 10x. • 2016: Celgene Award: Developed a data management system for large NGS datasets, which enabled data ingestion and tracking of >100,000 NGS samples. • 2014: Excellence in Performance Award, Celgene: design and implementation of  in-house NGS analytical capabilities • 2005: Outstanding Performance Award, Lawrence Berkeley National Lab for the development of IMG   Skills  Data Science & AI Leadership: Strategic Planning & Deployment, Team Leadership & Mentoring, Cross-functional Collaboration, Stakeholder Engagement, Project Portfolio Management, AI/MLStrategy, Predictive Biology, Digital Transformation, Technology Roadmapping. Experience in talent hiring, development, and retention. Bioinformatics & Omics: Multi-omics integration (genomics, metagenomics, transcriptomics, proteomics, metabolomics), biomarker discovery, microbial genomics, microbiome data analysis. Biopharma Domain Expertise: Drug Discovery & Development Processes, Translational Research, Target Identiﬁcation & Validation,  Translational Biomarkers Discovery,   Pipelines & Workﬂow Development: High-throughput pipelines, workﬂow automation, workﬂow management tools (CWL; familiarity with Snakemake/Nextﬂow). Data Management: Large-scale dataset curation, reproducibility frameworks, data version control, cloud-based data lakes. Infrastructure: High-Performance Computing (HPC, UGE/SLURM), AWS cloud, hybrid infrastructures, DevOps, CI/CD. Programming & Tools: Python, R, Bash, Linux, Docker, Git, workﬂow automation. Analytics & Modeling: Statistical modeling, predictive analytics, machine learning data preparation, advanced data integration methods. Collaboration & Communication: Cross-functional teamwork with biologists, data scientists, and product developers; reporting and presenting complex results to diverse stakeholders.  Languages  Greek (Native) English (C2) French (B1) German (A1)  Interests  Reading, hiking, biking, basketball, modelism, carpentry, nutrition\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "resume_text=\"\"\n",
    "resume_data\n",
    "for p in resume_data:\n",
    "  resume_text = resume_text + p.page_content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f356df",
   "metadata": {},
   "source": [
    "## Search for the company of interest\n",
    "\n",
    "Get informaation about the company that is hiring. \n",
    "We are specifically looking for their values and mission statement, this can be used to tailor the resume and the cover letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7b3e8750",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h7/b5y3fd8s671_5jhxlq2cyj7c0000gn/T/ipykernel_82505/3200585184.py:3: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the `langchain-tavily package and should be used instead. To use it run `pip install -U `langchain-tavily` and import as `from `langchain_tavily import TavilySearch``.\n",
      "  search_tool=TavilySearchResults(max_results=2)\n"
     ]
    }
   ],
   "source": [
    "# create the search with the necessary prompt \n",
    "# to get information about the values of the company\n",
    "search_tool=TavilySearchResults(max_results=2)\n",
    "\n",
    "# test how it works:\n",
    "# search_results=search.invoke( f\"What is the focus of the company {company} and what are its values.\")\n",
    "# print(f\"Search returned\\n{search_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "477e75f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[search_tool]\n",
    "prompt=f\"\"\"What is company {jd.get('company_name')} specialized on?\n",
    " What are the values they are proud of?\n",
    " \n",
    " Return the results as a json string with keys:\n",
    " company_name,\n",
    " specialized_in which will be a string describing what the company specializes in\n",
    " values: a list of strings describing the values of the company\n",
    " \"\"\"\n",
    "agent=create_agent(model=model, \n",
    "                   tools=[search_tool],\n",
    "                   system_prompt=\"You are a helpful assistant that extracts information from the internet.  Be consice and accurate.\"\n",
    "                   )\n",
    "\n",
    "input_message={'role':'user', \n",
    "               'content': prompt}\n",
    "response = agent.invoke({\"messages\": [input_message]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "734adf98",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[83]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m company_values=strip_markdown_backticks(\n\u001b[32m      2\u001b[39m   response[\u001b[33m'\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m'\u001b[39m][-\u001b[32m1\u001b[39m].content\n\u001b[32m      3\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m company_values=\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompany_values\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/resume_writer/lib/python3.13/json/__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    341\u001b[39m     s = s.decode(detect_encoding(s), \u001b[33m'\u001b[39m\u001b[33msurrogatepass\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONDecoder\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/resume_writer/lib/python3.13/json/decoder.py:345\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w=WHITESPACE.match):\n\u001b[32m    341\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    342\u001b[39m \u001b[33;03m    containing a JSON document).\u001b[39;00m\n\u001b[32m    343\u001b[39m \n\u001b[32m    344\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m     obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    346\u001b[39m     end = _w(s, end).end()\n\u001b[32m    347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m end != \u001b[38;5;28mlen\u001b[39m(s):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/resume_writer/lib/python3.13/json/decoder.py:363\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    361\u001b[39m     obj, end = \u001b[38;5;28mself\u001b[39m.scan_once(s, idx)\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "company_values=strip_markdown_backticks(\n",
    "  response['messages'][-1].content\n",
    ")\n",
    "company_values=json.loads(company_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf060ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the company Novartis the values we got are {\n",
      " \"company_name\": \"Novartis\",\n",
      " \"specialized_in\": \"Novartis is a global healthcare company that provides medicines and treatments for various diseases and health conditions. They focus on research and development to create innovative therapies.\",\n",
      " \"values\": [\n",
      "  \"Innovation\",\n",
      "  \"Quality\",\n",
      "  \"Collaboration\",\n",
      "  \"Performance\",\n",
      "  \"Courage\",\n",
      "  \"Integrity\"\n",
      " ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# print(f\"For the company {jd.get('company_name')} the values we got are {company_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd8b2c5",
   "metadata": {},
   "source": [
    "## Create a professional summary based on the experience\n",
    "\n",
    "Using the working experience from the resume, create a professional summary that can be used at the top of the resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed77498b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=f\"\"\"\n",
    "Read the working experience and education from my resume:\n",
    "```\n",
    "{resume_text}\n",
    "```\n",
    "and create a compelling professional summary for a job application \n",
    "that fits the job description below as a reference but don't mention the job title \n",
    "or the company name.\n",
    "and keep it to 500 characters or less. \n",
    "The job description is:\n",
    "```\n",
    "{jd.get('job_description')}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "agent=create_agent(model, \n",
    "                   tools=[],\n",
    "                   system_prompt=\"You are an expert career coach. Be concise and critical. Don't be sycophantic.\"\n",
    ") \n",
    "\n",
    "response = agent.invoke(\n",
    "  {\"messages\": [{'role':'user', 'content': prompt}]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b782077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Professional Summary:\n",
      "Bioinformatics leader with 20+ years in multi-omics and AI/ML for drug discovery. Proven ability to design and deploy scalable data platforms, accelerate biomarker discovery, and lead cross-functional teams. Expertise in integrating genomics, transcriptomics, and AI to drive innovation and shorten development cycles, aligning with Novartis' digital transformation goals.\n"
     ]
    }
   ],
   "source": [
    "professional_summary=response['messages'][-1].content\n",
    "\n",
    "# print(f\"Professional Summary:\\n{professional_summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ddef57",
   "metadata": {},
   "source": [
    "## Work out the resume \n",
    "\n",
    "Create the resume based on the job description, the company values and specialization, and the professional summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "438b4376",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=f\"\"\"\n",
    "Write a tailored resume for this {jd.get(\"job_title\")} role at \n",
    "{jd.get(\"company_name\")}. \n",
    "Don't include an objective statement or references, \n",
    "but do include the professional summary {professional_summary}, \n",
    "Use my past work experience to extract 3-5 bullet points per role, \n",
    "and incorporate the most important keywords from the job description \n",
    "in those achievements. \n",
    "The job description is: {jd.get('job_description')}\n",
    "My resume is as follows: {resume_text}\n",
    "The company values are: {company_values}\n",
    "Focus on aligning my skills and experiences with the job requirements.\n",
    "Categorize the skills section into Technical Skills and Soft Skills.\n",
    "\n",
    "Return the resume in markdown format.\n",
    "include the sections\n",
    "- Professional Summary\n",
    "- Work Experience\n",
    "- Education\n",
    "- Skills\n",
    "- Awards\n",
    "- Languages\n",
    "- Interests\n",
    "\"\"\"\n",
    "\n",
    "agent=create_agent(model=model, \n",
    "                   tools=[],\n",
    "                   system_prompt=\"You are an expert career coach.  Be consice and accurate. Don't be sycophantic. \"\n",
    "                   )\n",
    "\n",
    "input_message={'role':'user', \n",
    "               'content': prompt}\n",
    "response = agent.invoke({\"messages\": [input_message]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22968d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw resume:\n",
      "```markdown\n",
      "# Konstantinos Mavrommatis, PhD\n",
      "\n",
      "**Bioinformatics Leader | Data Scientist**\n",
      "\n",
      "Areuse, NE, Switzerland | mavrommatis.konstantinos@gmail.com | +41 79 248 9368 | /in/mavrommatiskonstantinos/ | scholar.google.com/citations\n",
      "\n",
      "---\n",
      "\n",
      "## Professional Summary\n",
      "\n",
      "Bioinformatics leader with 20+ years in multi-omics and AI/ML for drug discovery. Proven ability to design and deploy scalable data platforms, accelerate biomarker discovery, and lead cross-functional teams. Expertise in integrating genomics, transcriptomics, and AI to drive innovation and shorten development cycles, aligning with Novartis' digital transformation goals.\n",
      "\n",
      "---\n",
      "\n",
      "## Work Experience\n",
      "\n",
      "**Scientific Director, Applied Bioinformatics** | Bristol Myers Squibb (formerly Celgene) | Boudry, CH / San Francisco, CA, USA | 2020 – Present\n",
      "\n",
      "*   Directed strategic initiatives as a primary liaison between SMEs, IT, and R&D, designing and deploying cutting-edge data science and AI capabilities to accelerate drug discovery and development.\n",
      "*   Orchestrated the application of ML and advanced AI-driven algorithms for genomics analysis, biomarker identification, and elucidation of on-target and off-target pathways, supporting clinical development strategies.\n",
      "*   Led and mentored a cross-functional team of scientists and data scientists, allocating resources and prioritizing a data science portfolio to support 10+ clinical trials and 20+ assets from therapeutic discovery through preclinical candidate programs.\n",
      "*   Defined and executed enterprise-wide data engineering strategies, designing and scaling high-performance data platforms and pipelines that enabled advanced analytics and AI across R&D.\n",
      "*   Managed and co-led multiple external collaborations and strategic partnerships with academic institutions and major biopharma companies, leveraging cutting-edge data and analytic methodologies to advance drug discovery and validation pipelines.\n",
      "\n",
      "**Scientist to Sr Principal Scientist, Bioinformatics** | Bristol Myers Squibb (formerly Celgene) | San Francisco, CA, USA | 2012 - 2020\n",
      "\n",
      "*   Directed strategic initiatives as a primary liaison between SMEs, IT, and R&D, designing and deploying cutting-edge data science and AI capabilities to accelerate drug discovery, development, and supply.\n",
      "*   Orchestrated the application of machine learning and AI-driven algorithms to enable the discovery of translational biomarkers and disease-relevant target candidates, and to improve the predictive validity of pre-clinical models.\n",
      "*   Shaped, mentored, and led a high-performing, multidisciplinary team of data scientists and computational biology experts, fostering an innovative work environment to support drug discovery and translational research.\n",
      "*   Designed and deployed a cloud-native platform processing >1PB of data, reducing costs fivefold and increasing throughput tenfold, laying the foundation for enterprise-scale data lake architecture supporting global R&D.\n",
      "*   Engineered a centralized data management system supporting >100,000 NGS samples, ensuring scalability, traceability, and compliance, and enabling secure, reusable data pipelines for research teams.\n",
      "\n",
      "**Research Scientist, Bioinformatics** | DOE Joint Genome Institute | Walnut Creek, CA, USA | 2007 - 2012\n",
      "\n",
      "*   Directed the development of scalable bioinformatics and data engineering solutions, enabling large-scale genomics and metagenomics projects that informed health and environmental research initiatives.\n",
      "*   Designed high-throughput pipelines for genomic and metagenomic data integration, ensuring data quality, reliability, and scalability, which supported 100+ publications and external collaborations.\n",
      "*   Contributed key components to the development of the Integrated Microbial Genomes platform, enhancing data accessibility and reusability through robust backend infrastructure and user-facing solutions.\n",
      "*   Led the Omics group in developing innovative bioinformatics solutions for microbial species analysis, contributing to breakthroughs in metagenomics and genomics research.\n",
      "\n",
      "**Postdoctoral Fellow, Bioinformatics** | DOE Joint Genome Institute | Walnut Creek, CA, USA | 2004 – 2007\n",
      "\n",
      "*   Developed algorithms and workflows for large-scale microbial genomics projects, supporting high-throughput sequencing and multi-omics data integration.\n",
      "*   Contributed to the design and implementation of the IMG platform, ensuring accessibility and reproducibility of microbial genome data for global researchers.\n",
      "\n",
      "---\n",
      "\n",
      "## Education\n",
      "\n",
      "**Ph.D., Molecular Biology and Biomedicine** | University of Crete, Greece | 1997 – 2002\n",
      "**M.Sc., Biotechnology** | University of Crete, Greece | 1995 – 1997\n",
      "**B.Sc., Biological Sciences** | University of Athens, Greece | 1995\n",
      "\n",
      "---\n",
      "\n",
      "## Skills\n",
      "\n",
      "**Technical Skills**\n",
      "\n",
      "*   **Data Science & AI:** AI/ML Strategy, Predictive Biology, Machine Learning Data Preparation, Advanced Data Integration Methods, Generative AI (familiarity)\n",
      "*   **Bioinformatics & Omics:** Multi-omics Integration (genomics, metagenomics, transcriptomics, proteomics, metabolomics), Biomarker Discovery, Microbial Genomics, Microbiome Data Analysis\n",
      "*   **Pipelines & Workflow Development:** High-throughput Pipelines, Workflow Automation, Workflow Management Tools (CWL; familiarity with Snakemake/Nextflow)\n",
      "*   **Data Management:** Large-scale Dataset Curation, Reproducibility Frameworks, Data Version Control, Cloud-based Data Lakes\n",
      "*   **Infrastructure:** High-Performance Computing (HPC, UGE/SLURM), AWS Cloud, Hybrid Infrastructures, DevOps, CI/CD\n",
      "*   **Programming & Tools:** Python, R, Bash, Linux, Docker, Git\n",
      "\n",
      "**Soft Skills**\n",
      "\n",
      "*   **Leadership & Strategy:** Strategic Planning & Deployment, Team Leadership & Mentoring, Project Portfolio Management, Technology Roadmapping, Talent Hiring, Development, and Retention.\n",
      "*   **Collaboration & Communication:** Cross-functional Collaboration, Stakeholder Engagement, Cross-functional Teamwork, Reporting and Presenting Complex Results.\n",
      "*   **Problem Solving & Execution:** Problem-Solving, Influencing Skills, Proactive Attitude, Execution and Prioritization in Complex Matrixed Environments.\n",
      "*   **Domain Expertise:** Drug Discovery & Development Processes, Translational Research, Target Identification & Validation, Translational Biomarkers Discovery.\n",
      "*   **Entrepreneurial Spirit:** Can-do, Pro-active Attitude, Passion for Emerging Technologies.\n",
      "\n",
      "---\n",
      "\n",
      "## Awards\n",
      "\n",
      "*   **2019:** Celgene Award: Designed and implemented a hybrid cloud research compute environment to enhance genomics data processing capabilities which processed >1PB of data, reducing processing costs by 5x and increasing processing speed by more than 10x.\n",
      "*   **2016:** Celgene Award: Developed a data management system for large NGS datasets, which enabled data ingestion and tracking of >100,000 NGS samples.\n",
      "*   **2014:** Excellence in Performance Award, Celgene: Design and implementation of in-house NGS analytical capabilities.\n",
      "*   **2005:** Outstanding Performance Award, Lawrence Berkeley National Lab for the development of IMG.\n",
      "\n",
      "---\n",
      "\n",
      "## Languages\n",
      "\n",
      "*   Greek (Native)\n",
      "*   English (C2)\n",
      "*   French (B1)\n",
      "*   German (A1)\n",
      "\n",
      "---\n",
      "\n",
      "## Interests\n",
      "\n",
      "Reading, hiking, biking, basketball, modelism, carpentry, nutrition\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "raw_resume=response['messages'][-1].content\n",
    "\n",
    "# print(f\"Raw resume:\\n{raw_resume}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cd010c",
   "metadata": {},
   "source": [
    "## Fine tune the resume \n",
    "\n",
    "Change the resume language to be more to the point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e2086e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=f\"\"\"\n",
    " Can you audit this entire resume {raw_resume} and point out areas where \n",
    " I am being too vague, too wordy, or not showing enough impact? \n",
    "\n",
    " I want your feedback on tone, structure, and how I can better emphasize \n",
    " leadership, results, or innovation.\n",
    " Please act like a hiring manager in industry for a {jd.get('job_level')} position. \n",
    " Based on this resume, what would make you more likely to invite me \n",
    " for an interview? What should I change, cut, or add to improve my chances?\n",
    "\"\"\"\n",
    "\n",
    "agent=create_agent(model=model, \n",
    "                   tools=[],\n",
    "                   system_prompt=\"You are an expert recruiter in a company. Be consice and accurate. Don't be sycophantic.\"\n",
    "                   )\n",
    "\n",
    "input_message={'role':'user', \n",
    "               'content': prompt}\n",
    "response = agent.invoke({\"messages\": [input_message]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f74ecabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tune points:\n",
      "This is a strong resume showcasing significant experience. Here's an audit focusing on vagueness, wordiness, impact, and areas for improvement, with a focus on emphasizing leadership, results, and innovation.\n",
      "\n",
      "## Resume Audit: Konstantinos Mavrommatis, PhD\n",
      "\n",
      "**Overall Impression:** The resume presents a highly experienced and accomplished bioinformatics leader with a clear trajectory in drug discovery and data science. The structure is logical, and the content demonstrates a deep understanding of the field.\n",
      "\n",
      "**Tone:** Professional and results-oriented.\n",
      "\n",
      "**Structure:** Well-organized and easy to follow.\n",
      "\n",
      "---\n",
      "\n",
      "### Areas for Improvement:\n",
      "\n",
      "**1. Professional Summary:**\n",
      "\n",
      "*   **Vagueness/Wordiness:** \"Proven ability to design and deploy scalable data platforms, accelerate biomarker discovery, and lead cross-functional teams.\" While true, it's a bit generic.\n",
      "*   **Impact:** Could be more specific about *what kind* of drug discovery and *what scale* of impact.\n",
      "*   **Leadership/Innovation Emphasis:** While \"leader\" is present, the summary could better highlight strategic vision and innovative contributions.\n",
      "*   **Suggestion:**\n",
      "    *   **Revised Example:** \"Visionary Bioinformatics Leader with 20+ years of experience driving AI/ML-driven drug discovery through advanced multi-omics integration. Proven track record in architecting and implementing enterprise-scale data platforms that accelerate biomarker identification and shorten development timelines. Adept at leading high-performing, cross-functional teams to achieve critical R&D milestones, directly aligning with strategic digital transformation goals.\"\n",
      "\n",
      "**2. Work Experience:**\n",
      "\n",
      "*   **Bristol Myers Squibb (formerly Celgene) - Scientific Director, Applied Bioinformatics:**\n",
      "    *   **Vagueness:** \"Directed strategic initiatives as a primary liaison...\" This is a common responsibility. Quantify the impact of these initiatives.\n",
      "    *   **Wordiness:** Some sentences are a bit long and could be more concise.\n",
      "    *   **Impact:** \"supporting clinical development strategies,\" \"supporting 10+ clinical trials and 20+ assets.\" This is good, but can you quantify the *impact* on these trials or assets? Did it lead to faster decisions, reduced costs, or improved outcomes?\n",
      "    *   **Leadership:** \"Led and mentored a cross-functional team...\" Good. Can you specify the size of the team and any notable achievements of that team under your leadership?\n",
      "    *   **Innovation:** \"designing and deploying cutting-edge data science and AI capabilities.\" Be more specific about *what* these capabilities were and *why* they were cutting-edge.\n",
      "    *   **Suggestion:**\n",
      "        *   Instead of \"Directed strategic initiatives as a primary liaison...\", try: \"Spearheaded strategic data science and AI initiatives, bridging R&D and IT to deliver novel capabilities that accelerated drug discovery, resulting in [quantifiable outcome, e.g., a 15% reduction in preclinical study timelines].\"\n",
      "        *   For the team leadership: \"Led and mentored a high-performing team of X scientists and data scientists, managing a portfolio that supported Y clinical trials and Z assets, directly contributing to [specific achievement, e.g., the advancement of two assets into Phase II trials].\"\n",
      "        *   For data engineering: \"Defined and executed enterprise-wide data engineering strategies, scaling high-performance data platforms and pipelines that enabled advanced analytics and AI across R&D, leading to [quantifiable benefit, e.g., a 30% increase in data accessibility for research teams].\"\n",
      "\n",
      "*   **Bristol Myers Squibb (formerly Celgene) - Scientist to Sr Principal Scientist, Bioinformatics:**\n",
      "    *   **Repetition:** The first bullet point is very similar to the first bullet point in the Scientific Director role.\n",
      "    *   **Vagueness/Impact:** \"improve the predictive validity of pre-clinical models.\" How much improvement?\n",
      "    *   **Innovation:** \"Designed and deployed a cloud-native platform processing >1PB of data...\" This is excellent impact. Ensure the \"reducing costs fivefold and increasing throughput tenfold\" is prominent.\n",
      "    *   **Suggestion:**\n",
      "        *   Combine or refine the first bullet point to avoid redundancy.\n",
      "        *   Quantify the \"improvement\" in predictive validity if possible.\n",
      "\n",
      "*   **DOE Joint Genome Institute - Research Scientist, Bioinformatics:**\n",
      "    *   **Impact:** \"informed health and environmental research initiatives.\" Can you give an example of a specific initiative or the scale of impact (e.g., number of researchers supported)?\n",
      "    *   **Innovation:** \"Contributed key components to the development of the Integrated Microbial Genomes platform.\" Highlight your specific contributions and their significance.\n",
      "\n",
      "*   **DOE Joint Genome Institute - Postdoctoral Fellow, Bioinformatics:**\n",
      "    *   **Impact:** Similar to the Research Scientist role, can you quantify the impact of the algorithms and workflows developed?\n",
      "\n",
      "**3. Skills:**\n",
      "\n",
      "*   **Vagueness:** \"AI/ML Strategy,\" \"Predictive Biology,\" \"Advanced Data Integration Methods.\" These are broad.\n",
      "*   **Innovation:** \"Generative AI (familiarity)\" is good to include.\n",
      "*   **Suggestion:**\n",
      "    *   For \"AI/ML Strategy,\" consider adding specific types of AI/ML used (e.g., deep learning, NLP, predictive modeling).\n",
      "    *   For \"Advanced Data Integration Methods,\" specify the types of data integrated.\n",
      "    *   Consider adding specific software/tools within categories (e.g., specific ML libraries like TensorFlow, PyTorch, scikit-learn).\n",
      "\n",
      "**4. Awards:**\n",
      "\n",
      "*   **Excellent Impact:** The awards are well-described and quantify significant achievements. This section is a strong asset.\n",
      "\n",
      "---\n",
      "\n",
      "### What Would Make Me More Likely to Invite You for an Interview?\n",
      "\n",
      "1.  **Quantified Impact:** While present, more explicit numbers and metrics tied to your achievements would be highly beneficial. For example, instead of \"accelerate drug discovery,\" state \"accelerated drug discovery by X%,\" or \"reduced time to candidate selection by Y months.\"\n",
      "2.  **Specific Examples of Innovation:** Highlight *novel* approaches or technologies you introduced. What made your solutions \"cutting-edge\"?\n",
      "3.  **Leadership Scope:** Clearly define the size and composition of teams you've led. Mention any significant challenges overcome or specific successes achieved by your teams under your guidance.\n",
      "4.  **Strategic Vision:** In the summary and experience sections, emphasize how your work aligned with broader company goals and contributed to strategic direction.\n",
      "5.  **Conciseness:** While the content is good, refining some bullet points for brevity would make it even more impactful.\n",
      "\n",
      "---\n",
      "\n",
      "### What Should I Change, Cut, or Add to Improve My Chances?\n",
      "\n",
      "*   **Change:**\n",
      "    *   **Refine bullet points:** Focus on action verbs and quantifiable results. Ensure each bullet point answers \"So what?\"\n",
      "    *   **Strengthen the Professional Summary:** Make it a compelling hook that immediately showcases your value proposition.\n",
      "    *   **Add specific tools/technologies:** Within your Technical Skills, list specific libraries, frameworks, or platforms you are proficient in (e.g., scikit-learn, TensorFlow, PyTorch, specific cloud services).\n",
      "\n",
      "*   **Cut:**\n",
      "    *   **Redundant phrasing:** Review for any repetitive statements, especially between the Scientific Director and Sr. Principal Scientist roles.\n",
      "    *   **Vague statements:** Replace general responsibilities with specific achievements.\n",
      "\n",
      "*   **Add:**\n",
      "    *   **Specific Project Examples:** If possible, briefly mention 1-2 key projects where you drove significant innovation or achieved remarkable results.\n",
      "    *   **Mentorship Impact:** Briefly touch on how you developed talent within your teams, beyond just \"mentoring.\" (e.g., \"Developed junior scientists into team leads,\" \"Implemented training programs that improved team skillsets\").\n",
      "    *   **Domain Expertise Nuances:** If there are specific therapeutic areas you have particular expertise in, consider mentioning them.\n",
      "\n",
      "By implementing these suggestions, you can further enhance the already strong impact of your resume and make it even more compelling to hiring managers.\n"
     ]
    }
   ],
   "source": [
    "fine_tune_points=response['messages'][-1].content\n",
    "\n",
    "print(f\"Fine tune points:\\n{fine_tune_points}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "61e7bb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=f\"\"\"\n",
    "Fine tune the following  resume for this {jd.get(\"job_title\")} role at \n",
    "{jd.get(\"company_name\")}. \n",
    "\n",
    "The job description is: {jd.get('job_description')}\n",
    "My resume is as follows: {raw_resume}\n",
    "The company values are: {company_values}\n",
    "Focus on aligning my skills and experiences with the job requirements\n",
    "and take into account the following feedback from a hiring manager:\n",
    "```\n",
    "{fine_tune_points}\n",
    "```\n",
    "Categorize the skills section into Technical Skills and Soft Skills.\n",
    "\n",
    "Return the resume in markdown format.\n",
    "include the sections\n",
    "- Professional Summary\n",
    "- Work Experience\n",
    "- Education\n",
    "- Skills\n",
    "- Awards\n",
    "- Languages\n",
    "- Interests\n",
    "\"\"\"\n",
    "agent=create_agent(model=model, \n",
    "                   tools=[],\n",
    "                   system_prompt=\"You are an expert career coach.  Be consice and accurate. Don't be sycophantic. \"\n",
    "                   )\n",
    "input_message={'role':'user',\n",
    "                'content': prompt}\n",
    "response = agent.invoke({\"messages\": [input_message]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9367865e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_resume=response['messages'][-1].content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c031bec0",
   "metadata": {},
   "source": [
    "## Work on the formatting of the resume\n",
    "\n",
    "Make sure it is in the ATS combatible format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3fd8f278",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt=f\"\"\"\n",
    "\n",
    "proofread my resume \n",
    "```\n",
    "{raw_resume} \n",
    "```\n",
    "and suggest improvements for clarity and readability.\n",
    "Check my resume for passive voice and rewrite it using stronger action verbs. \n",
    "Review my resume for redundant phrasing and suggest more concise alternatives.\n",
    "Check my resume for consistency in formatting, punctuation, and capitalization.\n",
    "Identify areas where I can add quantifiable metrics to strengthen my resume.\n",
    "Please check the spelling and grammar of my resume to ensure it's correct. \n",
    "If necessary rewrite the resume bullet points in a more engaging \n",
    "and natural way. Use nondramatic language.\n",
    "Vary sentence lengths and structures instead of following predictable patterns\n",
    "Make my resume sound less generic while still maintaining professionalism.\n",
    "Adjust my resume tone to sound more natural and engaging, \n",
    "avoiding stiff or overly polished language.\n",
    "Edit my resume to include a mix of sentence structures that \n",
    "feel more natural and human-written.\n",
    "\n",
    "Do not change the overall structure of the resume and its sections.\n",
    "\n",
    "Return the resume in markdown format.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "agent=create_agent(model=model, \n",
    "                   tools=[],\n",
    "                   system_prompt=\"You are an expert career coach and linguistics professional.  Be consice and accurate. Don't be sycophantic. \"\n",
    "                   )\n",
    "input_message={'role':'user',\n",
    "                'content': prompt}\n",
    "response = agent.invoke({\"messages\": [input_message]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7eaf6919",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_resume=response['messages'][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d4e0a6",
   "metadata": {},
   "source": [
    "## Make sure the resume is ATS compatible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3d06a8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_ats=f\"\"\"\n",
    "Optimize my resume to pass Applicant Tracking Systems (ATS) \n",
    "for the {jd.get('job_title')} role at \n",
    "{jd.get('company_name')}. \n",
    "Incorporate relevant keywords and phrases from the job description: {jd.get('job_description')}.\n",
    "Ensure the formatting is ATS-friendly by avoiding complex layouts, graphics, and tables.\n",
    "My current resume is: {fine_tuned_resume}\n",
    "\n",
    "Return the resume in markdown format.\n",
    "\"\"\"\n",
    "\n",
    "agent=create_agent(model=model, \n",
    "                   tools=[],\n",
    "                   system_prompt=\"You are file format expert. Be consice and accurate. Don't be sycophantic. Double check every suggestion. \"\n",
    "                   )\n",
    "input_message={'role':'user',\n",
    "                'content': pass_ats}\n",
    "response = agent.invoke({\"messages\": [input_message]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c983930d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final resume saved to Resume_Novartis.pdf\n"
     ]
    }
   ],
   "source": [
    "final_resume=response['messages'][-1].content\n",
    "\n",
    "final_resume=strip_markdown_backticks(final_resume)\n",
    "\n",
    "output_pdf=f\"Resume_{jd.get('company_name').replace(' ','_')}.pdf\"\n",
    "markdown_to_pdf(final_resume, output_pdf)\n",
    "print(f\"Final resume saved to {output_pdf}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e0eedf",
   "metadata": {},
   "source": [
    "## Prepare a cover letter\n",
    "\n",
    "Write a cover letter based on the resume and the job description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c4577a42",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      1\u001b[39m prompt=\u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[33mDraft a cover letter based on the resume and the job description.\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[33mThe job description is:\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[33m```\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;132;01m{\u001b[39;00mjd.get(\u001b[33m'\u001b[39m\u001b[33mjob_description\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[33m``` \u001b[39m\n\u001b[32m      7\u001b[39m \u001b[33mand my resume is\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[33m```\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;132;01m{\u001b[39;00mfinal_resume\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\n\u001b[32m     10\u001b[39m \u001b[33m```.\u001b[39m\n\u001b[32m     11\u001b[39m \n\u001b[32m     12\u001b[39m \u001b[33mFocus on aligning my skills and experiences with the job requirements \u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[33mand the company values \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mcompany_values\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m(\u001b[33m'\u001b[39m\u001b[33mcompany_values\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[33mEmphasize how my background makes me a strong fit for the role at\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;132;01m{\u001b[39;00mjd.get(\u001b[33m'\u001b[39m\u001b[33mcompany_name\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\n\u001b[32m     16\u001b[39m \n\u001b[32m     17\u001b[39m \u001b[33mStructure the cover letter with an introduction, body paragraphs highlighting key qualifications,\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[33mand a conclusion expressing enthusiasm for the position.\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[33mAvoid generic statements and tailor the content specifically to the job and company.\u001b[39m\n\u001b[32m     20\u001b[39m \n\u001b[32m     21\u001b[39m \u001b[33mReturn the cover letter in markdown format.\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     26\u001b[39m agent=create_agent(model=model, \n\u001b[32m     27\u001b[39m                    tools=[],\n\u001b[32m     28\u001b[39m                    system_prompt=\u001b[33m\"\u001b[39m\u001b[33mYou are an expert hiring manager. Be consice and accurate. Don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt be sycophantic.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     29\u001b[39m                    )\n\u001b[32m     31\u001b[39m input_message={\u001b[33m'\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m'\u001b[39m:\u001b[33m'\u001b[39m\u001b[33muser\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     32\u001b[39m                 \u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m: prompt}  \n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "prompt=f\"\"\"\n",
    "Draft a cover letter based on the resume and the job description.\n",
    "The job description is:\n",
    "```\n",
    "{jd.get('job_description')}\n",
    "``` \n",
    "and my resume is\n",
    "```\n",
    "{final_resume} \n",
    "```.\n",
    "\n",
    "Focus on aligning my skills and experiences with the job requirements \n",
    "and the company values {company_values.get('company_values')}.\n",
    "Emphasize how my background makes me a strong fit for the role at\n",
    "{jd.get('company_name')}.\n",
    "\n",
    "Structure the cover letter with an introduction, body paragraphs highlighting key qualifications,\n",
    "and a conclusion expressing enthusiasm for the position.\n",
    "Avoid generic statements and tailor the content specifically to the job and company.\n",
    "\n",
    "Return the cover letter in markdown format.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "agent=create_agent(model=model, \n",
    "                   tools=[],\n",
    "                   system_prompt=\"You are an expert hiring manager. Be consice and accurate. Don't be sycophantic.\"\n",
    "                   )\n",
    "\n",
    "input_message={'role':'user',\n",
    "                'content': prompt}  \n",
    "response = agent.invoke({\"messages\": [input_message]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645d2840",
   "metadata": {},
   "outputs": [],
   "source": [
    "cover_letter=response['messages'][-1].content\n",
    "\n",
    "cover_letter=strip_markdown_backticks(cover_letter)\n",
    "output_pdf=f\"Cover_Letter_{jd.get('company_name').replace(' ','_')}.pdf\"\n",
    "markdown_to_pdf(cover_letter, output_pdf)\n",
    "print(f\"Final cover letter saved to {output_pdf}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "everyday_repo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
