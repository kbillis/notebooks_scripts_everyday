{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df9284d1",
   "metadata": {},
   "source": [
    "## Use Gen/AI to prepare CV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeeb7f4f",
   "metadata": {},
   "source": [
    "Libraries\n",
    "\n",
    "parse websites for jobs (for example linkedin), \n",
    "read links for jobs\n",
    "read my previous cv info\n",
    "prepare cv customised for this jobs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ca13dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pathlib in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (1.0.1)\n",
      "Requirement already satisfied: openai in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (2.6.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from openai) (0.11.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from openai) (2.12.3)\n",
      "Requirement already satisfied: sniffio in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Requirement already satisfied: anthropic in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (0.72.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from anthropic) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from anthropic) (1.9.0)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.15 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from anthropic) (0.17.0)\n",
      "Requirement already satisfied: httpx<1,>=0.25.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from anthropic) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from anthropic) (0.11.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from anthropic) (2.12.3)\n",
      "Requirement already satisfied: sniffio in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from anthropic) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from anthropic) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from anyio<5,>=3.5.0->anthropic) (3.11)\n",
      "Requirement already satisfied: certifi in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from httpx<1,>=0.25.0->anthropic) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->anthropic) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.2)\n",
      "Requirement already satisfied: ollama in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (0.6.0)\n",
      "Requirement already satisfied: httpx>=0.27 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.9 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from ollama) (2.12.3)\n",
      "Requirement already satisfied: anyio in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from httpx>=0.27->ollama) (4.11.0)\n",
      "Requirement already satisfied: certifi in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from httpx>=0.27->ollama) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from httpx>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from httpx>=0.27->ollama) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from pydantic>=2.9->ollama) (2.41.4)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from pydantic>=2.9->ollama) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from pydantic>=2.9->ollama) (0.4.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from anyio->httpx>=0.27->ollama) (1.3.1)\n",
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†ã \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†ô \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†π \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†∏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†º \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†¥ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling 667b0c1932bc: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 4.9 GB                         \u001b[K\n",
      "pulling 948af2743fc7: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.5 KB                         \u001b[K\n",
      "pulling 0ba8f0e314b4: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  12 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   96 B                         \u001b[K\n",
      "pulling 455f34728c9b: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  487 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n",
      "Requirement already satisfied: transformers in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (4.57.1)\n",
      "Requirement already satisfied: torch in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (2.7.1)\n",
      "Requirement already satisfied: accelerate in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (1.11.0)\n",
      "Requirement already satisfied: filelock in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from transformers) (2.3.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from transformers) (2025.10.23)\n",
      "Requirement already satisfied: requests in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: psutil in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from accelerate) (7.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from requests->transformers) (2025.10.5)\n"
     ]
    }
   ],
   "source": [
    "# Base requirements\n",
    "!pip install pathlib\n",
    "\n",
    "# For OpenAI\n",
    "!pip install openai\n",
    "\n",
    "# For Anthropic Claude\n",
    "!pip install anthropic\n",
    "\n",
    "# For Local Models (Ollama) - FREE!\n",
    "!pip install ollama\n",
    "# Then install Ollama: https://ollama.ai/\n",
    "!ollama pull llama3.1:8b\n",
    "\n",
    "# For HuggingFace (FREE, local)\n",
    "!pip install transformers torch accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67f3dd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# üìÑ AI-POWERED CV CUSTOMIZER V2\n",
    "# Uses organized/deduplicated CV files as reference\n",
    "# Supports multiple LLM providers (OpenAI, Anthropic, Ollama, HuggingFace)\n",
    "# CPU & GPU Compatible\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375b8026",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227620f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# User selects LLM provider explicitly\n",
    "LLM_PROVIDER = input(\"Select LLM provider (openai, anthropic, ollama, huggingface): \").strip().lower()\n",
    "\n",
    "# Set environment variable for consistency (optional)\n",
    "os.environ['LLM_PROVIDER'] = LLM_PROVIDER\n",
    "\n",
    "# Toggle this to True when running in Colab\n",
    "USE_COLAB = True\n",
    "\n",
    "if USE_COLAB:\n",
    "    # Colab-specific paths\n",
    "    ORGANIZED_FILES_PATH = \"/content/organized_filesdeduplicated\"\n",
    "    OUTPUT_BASE_PATH = \"/content/output_cv\"\n",
    "\n",
    "    # Prompt for API keys\n",
    "    openai_key = input(\"Enter your OpenAI API key (leave blank if not using): \").strip()\n",
    "    anthropic_key = input(\"Enter your Anthropic API key (leave blank if not using): \").strip()\n",
    "    huggingface_key = input(\"Enter your HuggingFace API key (leave blank if not using): \").strip()\n",
    "\n",
    "    # Set environment variables\n",
    "    os.environ['OPENAI_API_KEY'] = openai_key\n",
    "    os.environ['ANTHROPIC_API_KEY'] = anthropic_key\n",
    "    os.environ['HUGGINGFACE_API_KEY'] = huggingface_key\n",
    "\n",
    "else:\n",
    "    # Local paths\n",
    "    ORGANIZED_FILES_PATH = \"/Users/kbillis/bin/Awesome-CV/organized_filesdeduplicated\"\n",
    "    OUTPUT_BASE_PATH = \"/Users/kbillis/bin/Awesome-CV/examples/AI/\"\n",
    "\n",
    "# Unified config class\n",
    "class Config:\n",
    "    \"\"\"Configuration for CV generation.\"\"\"\n",
    "\n",
    "    ORGANIZED_FILES_PATH = ORGANIZED_FILES_PATH\n",
    "    OUTPUT_BASE_PATH = OUTPUT_BASE_PATH\n",
    "\n",
    "    LLM_PROVIDER = LLM_PROVIDER\n",
    "\n",
    "    OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY', '')\n",
    "    ANTHROPIC_API_KEY = os.environ.get('ANTHROPIC_API_KEY', '')\n",
    "    HUGGINGFACE_API_KEY = os.environ.get('HUGGINGFACE_API_KEY', '')\n",
    "\n",
    "    OPENAI_MODEL = 'gpt-4-turbo-preview'\n",
    "    ANTHROPIC_MODEL = 'claude-3-5-sonnet-20241022'\n",
    "    OLLAMA_MODEL = 'llama3.1:8b'\n",
    "    HUGGINGFACE_MODEL = 'mistralai/Mixtral-8x7B-Instruct-v0.1'\n",
    "\n",
    "    USE_GPU = True\n",
    "    MAX_TOKENS = 2000\n",
    "    TEMPERATURE = 0.3\n",
    "\n",
    "    CV_CATEGORIES = ['summary', 'experience', 'key_skills', 'education', 'additional_info']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6f78b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# LLM PROVIDERS (Same as before - reusing)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "class LLMProvider:\n",
    "    \"\"\"Base class for LLM providers.\"\"\"\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class OpenAIProvider(LLMProvider):\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        try:\n",
    "            from openai import OpenAI\n",
    "            self.client = OpenAI(api_key=config.OPENAI_API_KEY)\n",
    "            print(f\"‚úÖ OpenAI initialized: {config.OPENAI_MODEL}\")\n",
    "        except ImportError:\n",
    "            print(\"‚ùå pip install openai\")\n",
    "            self.client = None\n",
    "    \n",
    "    def generate(self, prompt: str) -> str:\n",
    "        if not self.client:\n",
    "            return \"Error: OpenAI not initialized\"\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.config.OPENAI_MODEL,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=self.config.MAX_TOKENS,\n",
    "                temperature=self.config.TEMPERATURE\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\"\n",
    "\n",
    "class AnthropicProvider(LLMProvider):\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        try:\n",
    "            from anthropic import Anthropic\n",
    "            self.client = Anthropic(api_key=config.ANTHROPIC_API_KEY)\n",
    "            print(f\"‚úÖ Anthropic initialized: {config.ANTHROPIC_MODEL}\")\n",
    "        except ImportError:\n",
    "            print(\"‚ùå pip install anthropic\")\n",
    "            self.client = None\n",
    "    \n",
    "    def generate(self, prompt: str) -> str:\n",
    "        if not self.client:\n",
    "            return \"Error: Anthropic not initialized\"\n",
    "        try:\n",
    "            message = self.client.messages.create(\n",
    "                model=self.config.ANTHROPIC_MODEL,\n",
    "                max_tokens=self.config.MAX_TOKENS,\n",
    "                temperature=self.config.TEMPERATURE,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            return message.content[0].text\n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\"\n",
    "\n",
    "class OllamaProvider(LLMProvider):\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        try:\n",
    "            import ollama\n",
    "            self.client = ollama\n",
    "            models = self.client.list()\n",
    "            print(f\"‚úÖ Ollama initialized: {config.OLLAMA_MODEL}\")\n",
    "        except ImportError:\n",
    "            print(\"‚ùå pip install ollama\")\n",
    "            self.client = None\n",
    "    \n",
    "    def generate(self, prompt: str) -> str:\n",
    "        if not self.client:\n",
    "            return \"Error: Ollama not initialized\"\n",
    "        try:\n",
    "            response = self.client.generate(\n",
    "                model=self.config.OLLAMA_MODEL,\n",
    "                prompt=prompt,\n",
    "                options={\n",
    "                    'temperature': self.config.TEMPERATURE,\n",
    "                    'num_predict': self.config.MAX_TOKENS,\n",
    "                }\n",
    "            )\n",
    "            return response['response']\n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\"\n",
    "\n",
    "class HuggingFaceProvider(LLMProvider):\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        try:\n",
    "            import torch\n",
    "            from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "            \n",
    "            device = \"cuda\" if torch.cuda.is_available() and config.USE_GPU else \"cpu\"\n",
    "            print(f\"üñ•Ô∏è  Device: {device}\")\n",
    "            print(f\"üì• Loading {config.HUGGINGFACE_MODEL}...\")\n",
    "            \n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(config.HUGGINGFACE_MODEL)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                config.HUGGINGFACE_MODEL,\n",
    "                torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "                device_map=\"auto\" if device == \"cuda\" else None,\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "            \n",
    "            self.pipeline = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=self.model,\n",
    "                tokenizer=self.tokenizer,\n",
    "                device=0 if device == \"cuda\" else -1\n",
    "            )\n",
    "            print(f\"‚úÖ HuggingFace loaded on {device}\")\n",
    "        except ImportError:\n",
    "            print(\"‚ùå pip install transformers torch\")\n",
    "            self.model = None\n",
    "    \n",
    "    def generate(self, prompt: str) -> str:\n",
    "        if not self.model:\n",
    "            return \"Error: HuggingFace not initialized\"\n",
    "        try:\n",
    "            outputs = self.pipeline(\n",
    "                prompt,\n",
    "                max_new_tokens=self.config.MAX_TOKENS,\n",
    "                temperature=self.config.TEMPERATURE,\n",
    "                do_sample=True,\n",
    "                top_p=0.9\n",
    "            )\n",
    "            return outputs[0]['generated_text'][len(prompt):]\n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\"\n",
    "\n",
    "def get_llm_provider(config: Config) -> LLMProvider:\n",
    "    \"\"\"Factory function to get LLM provider.\"\"\"\n",
    "    provider = config.LLM_PROVIDER.lower()\n",
    "    \n",
    "    if provider == 'openai':\n",
    "        return OpenAIProvider(config)\n",
    "    elif provider == 'anthropic':\n",
    "        return AnthropicProvider(config)\n",
    "    elif provider == 'ollama':\n",
    "        return OllamaProvider(config)\n",
    "    elif provider == 'huggingface':\n",
    "        return HuggingFaceProvider(config)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown provider: {provider}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b8fede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# ORGANIZED FILES ANALYZER\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "class OrganizedCVAnalyzer:\n",
    "    \"\"\"Analyze organized CV files by category.\"\"\"\n",
    "    \n",
    "    def __init__(self, organized_path: str):\n",
    "        self.organized_path = Path(organized_path)\n",
    "        self.categories = {}\n",
    "        \n",
    "    def load_all_categories(self) -> Dict[str, List[Dict]]:\n",
    "        \"\"\"Load all CV files organized by category.\"\"\"\n",
    "        print(\"\\nüîç Loading organized CV files...\")\n",
    "        \n",
    "        for category_dir in self.organized_path.iterdir():\n",
    "            if not category_dir.is_dir():\n",
    "                continue\n",
    "            \n",
    "            category = category_dir.name\n",
    "            files = []\n",
    "            \n",
    "            for tex_file in category_dir.glob(\"*.tex\"):\n",
    "                if tex_file.name == \"INDEX.md\":\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    with open(tex_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                        content = f.read()\n",
    "                    \n",
    "                    # Extract source directory from filename\n",
    "                    # Format: SourceDir__filename.tex\n",
    "                    parts = tex_file.stem.split('__')\n",
    "                    source_dir = parts[0] if len(parts) > 1 else 'unknown'\n",
    "                    original_name = '__'.join(parts[1:]) if len(parts) > 1 else tex_file.stem\n",
    "                    \n",
    "                    files.append({\n",
    "                        'path': str(tex_file),\n",
    "                        'filename': tex_file.name,\n",
    "                        'source_dir': source_dir,\n",
    "                        'original_name': original_name,\n",
    "                        'content': content,\n",
    "                        'size': len(content)\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Error reading {tex_file}: {e}\")\n",
    "            \n",
    "            self.categories[category] = files\n",
    "            print(f\"   ‚úÖ {category:20s}: {len(files):3d} files\")\n",
    "        \n",
    "        return self.categories\n",
    "    \n",
    "    def find_relevant_files(\n",
    "        self,\n",
    "        job_description: str,\n",
    "        category: str,\n",
    "        top_k: int = 5\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Find most relevant files for a category based on job description.\"\"\"\n",
    "        \n",
    "        if category not in self.categories:\n",
    "            print(f\"‚ö†Ô∏è  Category '{category}' not found\")\n",
    "            return []\n",
    "        \n",
    "        files = self.categories[category]\n",
    "        \n",
    "        # Extract keywords from job description\n",
    "        keywords = self._extract_keywords(job_description)\n",
    "        \n",
    "        # Score each file\n",
    "        scored_files = []\n",
    "        for file_data in files:\n",
    "            score = self._calculate_relevance_score(\n",
    "                file_content=file_data['content'],\n",
    "                keywords=keywords\n",
    "            )\n",
    "            scored_files.append((file_data, score))\n",
    "        \n",
    "        # Sort by score\n",
    "        scored_files.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top K\n",
    "        top_files = [f for f, score in scored_files[:top_k]]\n",
    "        \n",
    "        print(f\"   üìã Top {len(top_files)} files for {category}:\")\n",
    "        for i, (file_data, score) in enumerate(scored_files[:top_k], 1):\n",
    "            print(f\"      {i}. {file_data['source_dir']:30s} (score: {score})\")\n",
    "        \n",
    "        return top_files\n",
    "    \n",
    "    def _extract_keywords(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract keywords from text.\"\"\"\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Domain-specific keywords\n",
    "        keywords = [\n",
    "            # Bioinformatics\n",
    "            'bioinformatics', 'genomics', 'rna', 'dna', 'sequencing', 'ngs',\n",
    "            'cancer', 'biomarker', 'omics', 'proteomics', 'transcriptomics',\n",
    "            'single-cell', 'bulk-rna', 'chip-seq', 'atac-seq',\n",
    "            \n",
    "            # Data Science\n",
    "            'data science', 'machine learning', 'deep learning', 'ai',\n",
    "            'statistical', 'modeling', 'prediction', 'classification',\n",
    "            'clustering', 'nlp', 'computer vision',\n",
    "            \n",
    "            # Programming\n",
    "            'python', 'r', 'java', 'scala', 'c++', 'julia',\n",
    "            'sql', 'bash', 'perl', 'javascript',\n",
    "            \n",
    "            # Tools & Platforms\n",
    "            'aws', 'azure', 'gcp', 'cloud', 'docker', 'kubernetes',\n",
    "            'spark', 'hadoop', 'kafka', 'airflow', 'mlflow',\n",
    "            'tensorflow', 'pytorch', 'scikit-learn',\n",
    "            \n",
    "            # Domains\n",
    "            'healthcare', 'pharma', 'clinical', 'drug discovery',\n",
    "            'precision medicine', 'personalized medicine',\n",
    "            'immunotherapy', 'oncology',\n",
    "            \n",
    "            # Roles\n",
    "            'scientist', 'engineer', 'architect', 'lead', 'manager',\n",
    "            'analyst', 'developer', 'researcher'\n",
    "        ]\n",
    "        \n",
    "        found_keywords = [kw for kw in keywords if kw in text]\n",
    "        \n",
    "        # Also extract from phrases\n",
    "        skill_patterns = [\n",
    "            r'\\b(python|r|java|scala|sql)\\b',\n",
    "            r'\\b(aws|azure|gcp)\\b',\n",
    "            r'\\b(docker|kubernetes|spark)\\b',\n",
    "            r'\\b(machine learning|deep learning|ai)\\b',\n",
    "            r'\\b(rna-?seq|chip-?seq|atac-?seq)\\b'\n",
    "        ]\n",
    "        \n",
    "        for pattern in skill_patterns:\n",
    "            matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "            found_keywords.extend([m.lower() for m in matches])\n",
    "        \n",
    "        return list(set(found_keywords))\n",
    "    \n",
    "    def _calculate_relevance_score(\n",
    "        self,\n",
    "        file_content: str,\n",
    "        keywords: List[str]\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate relevance score for a file.\"\"\"\n",
    "        file_content_lower = file_content.lower()\n",
    "        \n",
    "        score = 0.0\n",
    "        \n",
    "        for keyword in keywords:\n",
    "            # Count occurrences\n",
    "            count = file_content_lower.count(keyword)\n",
    "            \n",
    "            # Weight by keyword importance (simple heuristic)\n",
    "            weight = 1.0\n",
    "            if keyword in ['python', 'r', 'machine learning', 'bioinformatics']:\n",
    "                weight = 2.0\n",
    "            elif keyword in ['aws', 'docker', 'sql']:\n",
    "                weight = 1.5\n",
    "            \n",
    "            score += count * weight\n",
    "        \n",
    "        # Normalize by file length\n",
    "        if len(file_content) > 0:\n",
    "            score = score / (len(file_content) / 1000)  # per 1000 chars\n",
    "        \n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ab12ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# CV GENERATOR V2\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "class CVGeneratorV2:\n",
    "    \"\"\"Generate customized CV files using organized reference files.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm: LLMProvider, analyzer: OrganizedCVAnalyzer, config: Config):\n",
    "        self.llm = llm\n",
    "        self.analyzer = analyzer\n",
    "        self.config = config\n",
    "    \n",
    "    def generate_cv_files(\n",
    "        self,\n",
    "        job_description: str,\n",
    "        job_title: str,\n",
    "        company_name: str,\n",
    "        output_dir: Optional[str] = None\n",
    "    ) -> Dict[str, str]:\n",
    "        \"\"\"Generate all CV files for a job application.\"\"\"\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üöÄ GENERATING CUSTOMIZED CV\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"   Job Title: {job_title}\")\n",
    "        print(f\"   Company: {company_name}\")\n",
    "        \n",
    "        # Create output directory\n",
    "        if not output_dir:\n",
    "            timestamp = datetime.now().strftime(\"%Y_%m\")\n",
    "            safe_title = re.sub(r'[^\\w\\s-]', '', job_title).replace(' ', '_')\n",
    "            safe_company = re.sub(r'[^\\w\\s-]', '', company_name).replace(' ', '_')\n",
    "            output_dir = f\"{self.config.OUTPUT_BASE_PATH}/{safe_title}_{safe_company}_{timestamp}\"\n",
    "        \n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"   Output: {output_path}\")\n",
    "        \n",
    "        generated_files = {}\n",
    "        \n",
    "        # Generate each category\n",
    "        for category in self.config.CV_CATEGORIES:\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"üìù Generating {category.upper()}\")\n",
    "            print(f\"{'='*70}\")\n",
    "            \n",
    "            # Find relevant reference files\n",
    "            reference_files = self.analyzer.find_relevant_files(\n",
    "                job_description=job_description,\n",
    "                category=category,\n",
    "                top_k=5\n",
    "            )\n",
    "            \n",
    "            if not reference_files:\n",
    "                print(f\"   ‚ö†Ô∏è  No reference files found for {category}\")\n",
    "                continue\n",
    "            \n",
    "            # Generate content\n",
    "            content = self._generate_file_content(\n",
    "                category=category,\n",
    "                job_description=job_description,\n",
    "                job_title=job_title,\n",
    "                company_name=company_name,\n",
    "                reference_files=reference_files\n",
    "            )\n",
    "            \n",
    "            if content and \"Error:\" not in content:\n",
    "                filename = f\"{category}.tex\"\n",
    "                filepath = output_path / filename\n",
    "                \n",
    "                with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                    f.write(content)\n",
    "                \n",
    "                generated_files[filename] = str(filepath)\n",
    "                print(f\"   ‚úÖ Saved: {filename}\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå Failed: {content}\")\n",
    "        \n",
    "        # Generate main resume file\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"üìÑ Generating main resume file\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        resume_content = self._generate_resume_main(job_title, company_name)\n",
    "        \n",
    "        safe_title = re.sub(r'[^\\w\\s-]', '', job_title).replace(' ', '_')\n",
    "        resume_filename = f\"resume_konstantinos_billis_{safe_title}.tex\"\n",
    "        resume_path = output_path / resume_filename\n",
    "        \n",
    "        with open(resume_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(resume_content)\n",
    "        \n",
    "        generated_files['main_resume'] = str(resume_path)\n",
    "        print(f\"   ‚úÖ Saved: {resume_filename}\")\n",
    "        \n",
    "        # Generate README\n",
    "        readme_content = self._generate_readme(\n",
    "            job_title, company_name, job_description\n",
    "        )\n",
    "        \n",
    "        readme_path = output_path / \"README.md\"\n",
    "        with open(readme_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(readme_content)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Generated {len(generated_files)} files in: {output_path}\")\n",
    "        \n",
    "        return generated_files\n",
    "    \n",
    "    def _generate_file_content(\n",
    "        self,\n",
    "        category: str,\n",
    "        job_description: str,\n",
    "        job_title: str,\n",
    "        company_name: str,\n",
    "        reference_files: List[Dict]\n",
    "    ) -> str:\n",
    "        \"\"\"Generate content for a specific category.\"\"\"\n",
    "        \n",
    "        # Prepare reference examples\n",
    "        reference_examples = []\n",
    "        for i, ref_file in enumerate(reference_files[:3], 1):\n",
    "            ref_content = ref_file['content'][:1500]  # Limit length\n",
    "            reference_examples.append(\n",
    "                f\"\\n### Example {i} (from {ref_file['source_dir']}):\\n{ref_content}\\n\"\n",
    "            )\n",
    "        \n",
    "        reference_text = '\\n'.join(reference_examples)\n",
    "        \n",
    "        # Category-specific instructions\n",
    "        category_instructions = {\n",
    "            'summary': \"Create a compelling professional summary (3-5 sentences) that highlights expertise relevant to this role.\",\n",
    "            'experience': \"List work experience in reverse chronological order using \\\\cventry commands. Focus on achievements relevant to the job.\",\n",
    "            'key_skills': \"Organize skills by category (Technical, Tools, Domain Expertise) using \\\\cvskill commands. Prioritize skills mentioned in the job description.\",\n",
    "            'education': \"List educational background using \\\\cventry commands. Include relevant coursework or thesis topics if applicable.\",\n",
    "            'additional_info': \"Include publications, certifications, awards, or other relevant information using \\\\cvhonor or \\\\cventry commands.\"\n",
    "        }\n",
    "        \n",
    "        instruction = category_instructions.get(category, \"Generate appropriate content for this section.\")\n",
    "        \n",
    "        prompt = f\"\"\"You are an expert CV writer creating LaTeX files for the Awesome-CV template.\n",
    "\n",
    "**Task:** Generate a {category}.tex file customized for this job application.\n",
    "\n",
    "**Job Information:**\n",
    "- Title: {job_title}\n",
    "- Company: {company_name}\n",
    "\n",
    "**Job Description (extract key requirements):**\n",
    "{job_description[:1500]}\n",
    "\n",
    "**Reference Examples (from successful CVs):**\n",
    "{reference_text}\n",
    "\n",
    "**Instructions:**\n",
    "1. {instruction}\n",
    "2. Use ONLY Awesome-CV LaTeX commands:\n",
    "   - \\\\cventry{{date}}{{title}}{{company}}{{location}}{{description}}\n",
    "   - \\\\cvskill{{category}}{{skills list}}\n",
    "   - \\\\cvhonor{{award}}{{organization}}{{location}}{{date}}\n",
    "3. Tailor content to match job requirements - use similar keywords and emphasize relevant experience\n",
    "4. Keep formatting consistent with examples\n",
    "5. DO NOT include \"Here is the generated `key_skills.tex` file:\", \\\\documentclass, \\\\begin{{document}},  \\\\end{{document}}\n",
    "6. Return ONLY the LaTeX content, no explanations or markdown. Same format and structure as examples. Only change content based on job description.\n",
    "7. In resume tex file, use full path of files\n",
    "8. Prepare all required files\n",
    " \n",
    "\n",
    "Generate the {category}.tex content now:\n",
    "\"\"\"\n",
    "        \n",
    "        print(f\"   ü§ñ Generating with {self.config.LLM_PROVIDER}...\")\n",
    "        return self.llm.generate(prompt)\n",
    "    \n",
    "    def _generate_resume_main(self, job_title: str, company_name: str) -> str:\n",
    "        \"\"\"Generate the main resume.tex file.\"\"\"\n",
    "        \n",
    "        return r\"\"\"%!TEX TS-program = xelatex\n",
    "%!TEX encoding = UTF-8 Unicode\n",
    "\n",
    "\\documentclass[11pt, a4paper]{awesome-cv}\n",
    "\n",
    "\\geometry{left=1.4cm, top=1cm, right=1.4cm, bottom=1.2cm, footskip=.5cm}\n",
    "\n",
    "\\fontdir[fonts/]\n",
    "\n",
    "\\colorlet{awesome}{awesome-nephritis}\n",
    "\n",
    "\\renewcommand{\\acvHeaderSocialSep}{\\quad\\textbar\\quad}\n",
    "\n",
    "%-------------------------------------------------------------------------------\n",
    "%\tPERSONAL INFORMATION\n",
    "%-------------------------------------------------------------------------------\n",
    "\\photo[circle,noedge,right]{/Users/kbillis/bin/Awesome-CV/examples/photos/kbillis_2024_profphoto.jpg}\n",
    "\n",
    "\\name{Konstantinos}{Billis}\n",
    "\\position{PhD in Bioinformatics{\\enskip\\cdotp\\enskip}Data Solutions Developer in Healthcare \\& Life Sciences}\n",
    "\n",
    "\\address{Zurich, CH (Permit: B) - Citizenships: Greek (EU), British}\n",
    "\\dateofbirth{Aug 16, 1983}\n",
    "\\email{billis.konstantinos@gmail.com}\n",
    "\\homepage{kbillis.github.io/}\n",
    "\\linkedin{konstantinos-billis-phd-baa23a22}\n",
    "\\googlescholar{YsQgBgUAAAAJ&view_op=list_works&sortby=pubdate}{ }\n",
    "\n",
    "%-------------------------------------------------------------------------------\n",
    "\\begin{document}\n",
    "\\newpage\n",
    "\n",
    "\\makecvheader[C]\n",
    "\n",
    "\\makecvfooter\n",
    "  {\\today}\n",
    "  {Konstantinos Billis, PhD~~~¬∑~~~R√©sum√©}\n",
    "  {\\thepage}\n",
    "\n",
    "%-------------------------------------------------------------------------------\n",
    "%\tCV/RESUME CONTENT\n",
    "%\tEach section is imported separately\n",
    "%-------------------------------------------------------------------------------\n",
    "\\input{summary.tex}\n",
    "\\input{key_skills.tex}\n",
    "\\input{experience.tex}\n",
    "\\input{education.tex}\n",
    "\\input{additional_info.tex}\n",
    "\n",
    "\\end{document}\n",
    "\"\"\"\n",
    "    \n",
    "    def _generate_readme(self, job_title: str, company_name: str, job_description: str) -> str:\n",
    "        \"\"\"Generate README for the CV directory.\"\"\"\n",
    "        return f\"\"\"# CV for {job_title} at {company_name}\n",
    "\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Using: AI-Powered CV Customizer V2\n",
    "\n",
    "## Job Description\n",
    "\n",
    "{job_description[:500]}...\n",
    "\n",
    "## Files Generated\n",
    "\n",
    "- `resume_konstantinos_billis_*.tex` - Main resume file\n",
    "- `summary.tex` - Professional summary\n",
    "- `experience.tex` - Work experience  \n",
    "- `key_skills.tex` - Key skills\n",
    "- `education.tex` - Education\n",
    "- `additional_info.tex` - Publications, awards, etc.\n",
    "\n",
    "## Compilation\n",
    "```bash\n",
    "cd \"{os.path.basename(self.config.OUTPUT_BASE_PATH)}\"\n",
    "xelatex resume_konstantinos_billis_*.tex\n",
    "```\n",
    "\n",
    "## Customization\n",
    "\n",
    "Generated using:\n",
    "- LLM Provider: {self.config.LLM_PROVIDER}\n",
    "- Reference files from organized CV database\n",
    "- Keyword matching and relevance scoring\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Review all generated files\n",
    "2. Customize any sections as needed\n",
    "3. Compile to PDF\n",
    "4. Proofread final output\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e4bd87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting information for job https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/4314922582\n"
     ]
    }
   ],
   "source": [
    "#parse the page with the job description\n",
    "# langchain gets the public description of the job\n",
    "# make sure we are not behind a proxy\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "jd_url=\"https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/4314922582\"\n",
    "print(f\"Getting information for job {jd_url}\")\n",
    "loader = WebBaseLoader(jd_url )\n",
    "loader.default_parser=\"html.parser\"\n",
    "docs = loader.load()\n",
    "# docs[0]\n",
    "\n",
    "\n",
    "# from langchain_unstructured import UnstructuredLoader\n",
    "# loader = UnstructuredLoader(web_url=jd_url)\n",
    "# docs=loader.load()\n",
    "\n",
    "description = docs[0].page_content\n",
    "# print(f\"Job Description\\n{description} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cf70f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìÑ AI-POWERED CV CUSTOMIZER V2\n",
      "======================================================================\n",
      "\n",
      "‚öôÔ∏è  Configuration:\n",
      "   LLM Provider: ollama\n",
      "   Organized Files: /Users/kbillis/bin/Awesome-CV/organized_filesdeduplicated\n",
      "   Output Path: /Users/kbillis/bin/Awesome-CV/examples/AI/\n",
      "\n",
      "ü§ñ Initializing LLM...\n",
      "‚úÖ Ollama initialized: llama3.1:8b\n",
      "\n",
      "üìö Loading organized CV files...\n",
      "\n",
      "üîç Loading organized CV files...\n",
      "   ‚úÖ experience          :  48 files\n",
      "   ‚úÖ education           :   9 files\n",
      "   ‚úÖ additional_info     :   7 files\n",
      "   ‚úÖ key_skills          :  47 files\n",
      "   ‚úÖ summary             :  39 files\n",
      "\n",
      "======================================================================\n",
      "üöÄ GENERATING CUSTOMIZED CV\n",
      "======================================================================\n",
      "   Job Title: Senior Bioinformatics Scientist\n",
      "   Company: Roche\n",
      "   Output: /Users/kbillis/bin/Awesome-CV/examples/AI/Senior_Bioinformatics_Scientist_Roche_2025_11\n",
      "\n",
      "======================================================================\n",
      "üìù Generating SUMMARY\n",
      "======================================================================\n",
      "   üìã Top 5 files for summary:\n",
      "      1. Plants_2025_03                 (score: 83.67181153533711)\n",
      "      2. Bioinformatics_2025_10_SIB     (score: 80.64516129032258)\n",
      "      3. DS_Healthcare_2025_04          (score: 79.8283261802575)\n",
      "      4. Bioinformatics_2025_06         (score: 79.4590025359256)\n",
      "      5. Bioinformatics_2025_05         (score: 78.99461400359066)\n",
      "   ü§ñ Generating with ollama...\n",
      "   ‚úÖ Saved: summary.tex\n",
      "\n",
      "======================================================================\n",
      "üìù Generating EXPERIENCE\n",
      "======================================================================\n",
      "   üìã Top 5 files for experience:\n",
      "      1. client_Platform_Nvidia_Med_2025_10 (score: 96.48230988206588)\n",
      "      2. Bioinformatics_2025_10_SIB     (score: 93.50202126066776)\n",
      "      3. LEAD_data_engineer_JJ_2025_10  (score: 93.02087597216537)\n",
      "      4. client_Platform_Med_2025_10    (score: 90.89951548346325)\n",
      "      5. Bioinformatics_2025_09         (score: 90.34341524889729)\n",
      "   ü§ñ Generating with ollama...\n",
      "   ‚úÖ Saved: experience.tex\n",
      "\n",
      "======================================================================\n",
      "üìù Generating KEY_SKILLS\n",
      "======================================================================\n",
      "   üìã Top 5 files for key_skills:\n",
      "      1. client_Platform_Med_2025_10    (score: 74.4944283945522)\n",
      "      2. LEAD_data_engineer_JJ_2025_10  (score: 74.47257383966245)\n",
      "      3. business_Data_Prec_Med_2025_10 (score: 74.2818671454219)\n",
      "      4. client_Platform_Nvidia_Med_2025_10 (score: 72.75350370981039)\n",
      "      5. Plants_2025_09                 (score: 67.68720029509406)\n",
      "   ü§ñ Generating with ollama...\n",
      "   ‚úÖ Saved: key_skills.tex\n",
      "\n",
      "======================================================================\n",
      "üìù Generating EDUCATION\n",
      "======================================================================\n",
      "   üìã Top 5 files for education:\n",
      "      1. Bioinformatics_2025_10_SIB     (score: 59.03398926654741)\n",
      "      2. Plants_2025_09                 (score: 57.30866884294523)\n",
      "      3. Bioinformatics_2025_10_RNA     (score: 54.20054200542005)\n",
      "      4. Data_Science_Novartis_2025_08  (score: 51.97342711996873)\n",
      "      5. LEAD_data_engineer_JJ_2025_10  (score: 48.58142246404975)\n",
      "   ü§ñ Generating with ollama...\n"
     ]
    }
   ],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# MAIN WORKFLOW\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "def main(job_description: str, job_title: str, company_name: str):\n",
    "    \"\"\"Main execution workflow.\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"üìÑ AI-POWERED CV CUSTOMIZER V2\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Initialize\n",
    "    config = Config()\n",
    "    \n",
    "    print(f\"\\n‚öôÔ∏è  Configuration:\")\n",
    "    print(f\"   LLM Provider: {config.LLM_PROVIDER}\")\n",
    "    print(f\"   Organized Files: {config.ORGANIZED_FILES_PATH}\")\n",
    "    print(f\"   Output Path: {config.OUTPUT_BASE_PATH}\")\n",
    "    \n",
    "    # Initialize LLM\n",
    "    print(f\"\\nü§ñ Initializing LLM...\")\n",
    "    llm = get_llm_provider(config)\n",
    "    \n",
    "    # Initialize Analyzer\n",
    "    print(f\"\\nüìö Loading organized CV files...\")\n",
    "    analyzer = OrganizedCVAnalyzer(config.ORGANIZED_FILES_PATH)\n",
    "    analyzer.load_all_categories()\n",
    "    \n",
    "    # Generate CV\n",
    "    generator = CVGeneratorV2(llm, analyzer, config)\n",
    "    \n",
    "    generated_files = generator.generate_cv_files(\n",
    "        job_description=job_description,\n",
    "        job_title=job_title,\n",
    "        company_name=company_name\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ CV GENERATION COMPLETE!\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nGenerated files:\")\n",
    "    for filename, filepath in generated_files.items():\n",
    "        print(f\"   üìÑ {filename}\")\n",
    "    \n",
    "    print(\"\\nüí° Next steps:\")\n",
    "    print(\"   1. Review generated files\")\n",
    "    print(\"   2. Compile: xelatex resume_*.tex\")\n",
    "    print(\"   3. Customize if needed\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example job application\n",
    "    job_description = \"\"\"\n",
    "    Senior Bioinformatics Scientist - Cancer Genomics\n",
    "    \n",
    "    Join our Oncology Research team to lead bioinformatics analysis of \n",
    "    multi-omics cancer data. The ideal candidate will have extensive \n",
    "    experience in cancer genomics, RNA-seq analysis, and biomarker discovery.\n",
    "    \n",
    "    Key Responsibilities:\n",
    "    - Lead bioinformatics analysis of multi-omics cancer data\n",
    "    - Develop computational pipelines for biomarker discovery\n",
    "    - Collaborate with wet-lab scientists and clinicians\n",
    "    - Mentor junior bioinformaticians\n",
    "    \n",
    "    Required Skills:\n",
    "    - PhD in Bioinformatics, Computational Biology, or related field\n",
    "    - 5+ years experience in cancer genomics\n",
    "    - Expert in Python, R, and Unix/Linux\n",
    "    - Experience with RNA-seq, WGS, WES analysis\n",
    "    - Strong publication record\n",
    "    - Knowledge of AWS/cloud computing\n",
    "    \n",
    "    Preferred:\n",
    "    - Experience with single-cell sequencing\n",
    "    - Knowledge of immunotherapy biomarkers\n",
    "    - Machine learning experience\n",
    "    \"\"\"\n",
    "    \n",
    "    job_title = \"Senior Bioinformatics Scientist\"\n",
    "    company_name = \"Roche\"\n",
    "    \n",
    "    main(job_description, job_title, company_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "everyday_repo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
