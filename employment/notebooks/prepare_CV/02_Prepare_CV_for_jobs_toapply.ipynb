{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "df9284d1",
      "metadata": {
        "id": "df9284d1"
      },
      "source": [
        "## Use Gen/AI to prepare CV"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eeeb7f4f",
      "metadata": {
        "id": "eeeb7f4f"
      },
      "source": [
        "### libraries\n",
        "use GPU!!! may be google colab! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "4ca13dc7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ca13dc7",
        "outputId": "1a06f4c1-954c-4d2b-b409-950ed8ce4b45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pathlib in /usr/local/lib/python3.12/dist-packages (1.0.1)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Requirement already satisfied: anthropic in /usr/local/lib/python3.12/dist-packages (0.72.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: docstring-parser<1,>=0.15 in /usr/local/lib/python3.12/dist-packages (from anthropic) (0.17.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (0.11.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from anthropic) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->anthropic) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.2)\n",
            "Requirement already satisfied: ollama in /usr/local/lib/python3.12/dist-packages (0.6.0)\n",
            "Requirement already satisfied: httpx>=0.27 in /usr/local/lib/python3.12/dist-packages (from ollama) (0.28.1)\n",
            "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.12/dist-packages (from ollama) (2.11.10)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (0.4.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27->ollama) (1.3.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "\n",
            "Installing Ollama Server Application...\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "install_libraries = True\n",
        "if install_libraries:\n",
        "    # --- Base Python Requirements ---\n",
        "    !pip install pathlib\n",
        "    !pip install openai\n",
        "    !pip install anthropic\n",
        "    !pip install ollama\n",
        "    !pip install transformers torch accelerate\n",
        "\n",
        "    # --- ‚ö†Ô∏è CRITICAL: Install Ollama Server Application ---\n",
        "    # This downloads and installs the *Ollama executable* on the Colab VM.\n",
        "    print(\"\\nInstalling Ollama Server Application...\")\n",
        "    !curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "    # --- ‚ùå REMOVE THIS LINE ---\n",
        "    # !ollama pull llama3.1:8b\n",
        "    # This must be done *after* the server is started in the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "67f3dd82",
      "metadata": {
        "id": "67f3dd82"
      },
      "outputs": [],
      "source": [
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import glob\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# üìÑ AI-POWERED CV CUSTOMIZER V2\n",
        "# Uses organized/deduplicated CV files as reference\n",
        "# Supports multiple LLM providers (OpenAI, Anthropic, Ollama, HuggingFace)\n",
        "# CPU & GPU Compatible\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "\n",
        "## TODO:   from langchain_community.document_loaders import WebBaseLoader\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "375b8026",
      "metadata": {
        "id": "375b8026"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "R9Zr3GFFXphj",
      "metadata": {
        "id": "R9Zr3GFFXphj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "def setup_ollama_in_colab(model_name: str = 'llama3.1:8b'):\n",
        "    \"\"\"Starts the Ollama server and pulls the required model.\"\"\"\n",
        "\n",
        "    print(\"\\n--- üõ†Ô∏è Starting Ollama Server and Pulling Model ---\")\n",
        "\n",
        "    # 1. Set Environment Variable and Start the Ollama Service\n",
        "    ollama_host = '127.0.0.1:11434'\n",
        "    os.environ['OLLAMA_HOST'] = ollama_host\n",
        "    print(f\"1. Setting OLLAMA_HOST environment variable to: {ollama_host}\")\n",
        "\n",
        "    # Start the Ollama server process in the background\n",
        "    print(\"2. Starting 'ollama serve' in background...\")\n",
        "    try:\n",
        "        # Popen is critical here to keep the process running after this cell finishes\n",
        "        subprocess.Popen(['ollama', 'serve'], start_new_session=True)\n",
        "        time.sleep(5) # Wait for the server to start\n",
        "        print(\"   -> Server started successfully on 127.0.0.1:11434.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå ERROR: Failed to start server: {e}\")\n",
        "        return\n",
        "\n",
        "    # 3. Pull the Model\n",
        "    print(f\"3. Pulling model: {model_name}. This may take some time...\")\n",
        "    try:\n",
        "        subprocess.run(\n",
        "            ['ollama', 'pull', model_name],\n",
        "            check=True,\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE,\n",
        "            text=True\n",
        "        )\n",
        "        print(f\"‚úÖ Successfully pulled model: {model_name}\")\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"‚ùå ERROR: Failed to pull model {model_name}. Check model name and internet connection.\")\n",
        "        print(f\"   Stderr: {e.stderr.strip()}\")\n",
        "\n",
        "    print(\"--- ‚úÖ Ollama Setup Complete ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "227620f3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "227620f3",
        "outputId": "27351e4d-697b-4c1c-9200-a80a0386ca80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Select LLM provider (openai, anthropic, ollama, huggingface): ollama\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "file_mappings.json\n",
            "ORGANIZATION_REPORT.md\n",
            "statistics.csv\n",
            "additional_info\n",
            "education\n",
            "experience\n",
            "key_skills\n",
            "summary\n",
            "Enter your OpenAI API key (leave blank if not using): \n",
            "Enter your Anthropic API key (leave blank if not using): \n",
            "Enter your HuggingFace API key (leave blank if not using): \n",
            "\n",
            "--- üõ†Ô∏è Starting Ollama Server and Pulling Model ---\n",
            "1. Setting OLLAMA_HOST environment variable to: 127.0.0.1:11434\n",
            "2. Starting 'ollama serve' in background...\n",
            "   -> Server started successfully on 127.0.0.1:11434.\n",
            "3. Pulling model: llama3.1:8b. This may take some time...\n",
            "‚úÖ Successfully pulled model: llama3.1:8b\n",
            "--- ‚úÖ Ollama Setup Complete ---\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# User selects LLM provider explicitly\n",
        "LLM_PROVIDER = input(\"Select LLM provider (openai, anthropic, ollama, huggingface): \").strip().lower()\n",
        "\n",
        "# Set environment variable for consistency (optional)\n",
        "os.environ['LLM_PROVIDER'] = LLM_PROVIDER\n",
        "\n",
        "# Toggle this to True when running in Colab\n",
        "USE_COLAB = True\n",
        "\n",
        "if USE_COLAB:\n",
        "    from google.colab import drive\n",
        "\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Colab-specific paths\n",
        "    ORGANIZED_FILES_PATH = \"/content/drive/MyDrive/Documents/work/jobs_work_applications_and_supporting_docs/AI/CVS_awesome_files/\"\n",
        "    OUTPUT_BASE_PATH = \"/content/drive/MyDrive/Documents/work/jobs_work_applications_and_supporting_docs/AI/output/\"\n",
        "\n",
        "    import os\n",
        "    # List files in the folder\n",
        "    for filename in os.listdir(ORGANIZED_FILES_PATH):\n",
        "      print(filename)\n",
        "\n",
        "\n",
        "    # Prompt for API keys\n",
        "    openai_key = input(\"Enter your OpenAI API key (leave blank if not using): \").strip()\n",
        "    anthropic_key = input(\"Enter your Anthropic API key (leave blank if not using): \").strip()\n",
        "    huggingface_key = input(\"Enter your HuggingFace API key (leave blank if not using): \").strip()\n",
        "\n",
        "    # Set environment variables\n",
        "    os.environ['OPENAI_API_KEY'] = openai_key\n",
        "    os.environ['ANTHROPIC_API_KEY'] = anthropic_key\n",
        "    os.environ['HUGGINGFACE_API_KEY'] = huggingface_key\n",
        "\n",
        "    if LLM_PROVIDER == \"ollama\":\n",
        "        # --- EXECUTE THE FUNCTION ---\n",
        "        # Pass the model name you need from your Config\n",
        "        setup_ollama_in_colab(model_name='llama3.1:8b')\n",
        "\n",
        "else:\n",
        "    # Local paths\n",
        "    ORGANIZED_FILES_PATH = \"/Users/kbillis/bin/Awesome-CV/organized_filesdeduplicated\"\n",
        "    OUTPUT_BASE_PATH = \"/Users/kbillis/bin/Awesome-CV/examples/AI/\"\n",
        "\n",
        "\n",
        "# Unified config class\n",
        "class Config:\n",
        "    \"\"\"Configuration for CV generation.\"\"\"\n",
        "\n",
        "    ORGANIZED_FILES_PATH = ORGANIZED_FILES_PATH\n",
        "    OUTPUT_BASE_PATH = OUTPUT_BASE_PATH\n",
        "\n",
        "    LLM_PROVIDER = LLM_PROVIDER\n",
        "\n",
        "    OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY', '')\n",
        "    ANTHROPIC_API_KEY = os.environ.get('ANTHROPIC_API_KEY', '')\n",
        "    HUGGINGFACE_API_KEY = os.environ.get('HUGGINGFACE_API_KEY', '')\n",
        "\n",
        "    OPENAI_MODEL = 'gpt-4'\n",
        "    ANTHROPIC_MODEL = 'claude-3-5-sonnet-20241022'\n",
        "    OLLAMA_MODEL = 'llama3.1:8b'\n",
        "    HUGGINGFACE_MODEL = 'mistralai/Mixtral-8x7B-Instruct-v0.1'\n",
        "\n",
        "    USE_GPU = True\n",
        "    MAX_TOKENS = 2000\n",
        "    TEMPERATURE = 0.3\n",
        "\n",
        "    CV_CATEGORIES = ['summary', 'experience', 'key_skills', 'education', 'additional_info']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1gqL22d-bwib",
      "metadata": {
        "id": "1gqL22d-bwib"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8oPup2JObykd",
      "metadata": {
        "id": "8oPup2JObykd"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# test your openai key\n",
        "# import os\n",
        "# from openai import OpenAI\n",
        "\n",
        "# client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "#      api_key=openai_key,\n",
        "# )\n",
        "\n",
        "# response = client.responses.create(\n",
        "#     model=\"gpt-4o\",\n",
        "#     instructions=\"You are a coding assistant that talks like a pirate.\",\n",
        "#     input=\"How do I check if a Python object is an instance of a class?\",\n",
        "# )\n",
        "\n",
        "# print(response.output_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "f6f78b00",
      "metadata": {
        "id": "f6f78b00"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# LLM PROVIDERS (Same as before - reusing)\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "class LLMProvider:\n",
        "    \"\"\"Base class for LLM providers.\"\"\"\n",
        "    def generate(self, prompt: str) -> str:\n",
        "        raise NotImplementedError\n",
        "\n",
        "class OpenAIProvider(LLMProvider):\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        try:\n",
        "            from openai import OpenAI\n",
        "            self.client = OpenAI(api_key=config.OPENAI_API_KEY)\n",
        "            print(f\"‚úÖ OpenAI initialized: {config.OPENAI_MODEL}\")\n",
        "        except ImportError:\n",
        "            print(\"‚ùå pip install openai\")\n",
        "            self.client = None\n",
        "\n",
        "    def generate(self, prompt: str) -> str:\n",
        "        if not self.client:\n",
        "            return \"Error: OpenAI not initialized\"\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.config.OPENAI_MODEL,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                max_tokens=self.config.MAX_TOKENS,\n",
        "                temperature=self.config.TEMPERATURE\n",
        "            )\n",
        "            return response.choices[0].message.content\n",
        "        except Exception as e:\n",
        "            return f\"Error: {e}\"\n",
        "\n",
        "class AnthropicProvider(LLMProvider):\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        try:\n",
        "            from anthropic import Anthropic\n",
        "            self.client = Anthropic(api_key=config.ANTHROPIC_API_KEY)\n",
        "            print(f\"‚úÖ Anthropic initialized: {config.ANTHROPIC_MODEL}\")\n",
        "        except ImportError:\n",
        "            print(\"‚ùå pip install anthropic\")\n",
        "            self.client = None\n",
        "\n",
        "    def generate(self, prompt: str) -> str:\n",
        "        if not self.client:\n",
        "            return \"Error: Anthropic not initialized\"\n",
        "        try:\n",
        "            message = self.client.messages.create(\n",
        "                model=self.config.ANTHROPIC_MODEL,\n",
        "                max_tokens=self.config.MAX_TOKENS,\n",
        "                temperature=self.config.TEMPERATURE,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "            )\n",
        "            return message.content[0].text\n",
        "        except Exception as e:\n",
        "            return f\"Error: {e}\"\n",
        "\n",
        "\n",
        "import os\n",
        "import requests # <-- You might need this for health checks later, but not strictly for the client\n",
        "\n",
        "class OllamaProvider(LLMProvider):\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "\n",
        "        # 1. Define the local URL (Ollama's default)\n",
        "        # You can add this to your Config class for consistency if you want,\n",
        "        # but the default is usually fine for local use.\n",
        "        self.base_url = os.environ.get('OLLAMA_HOST', 'http://127.0.0.1:11434')\n",
        "\n",
        "        try:\n",
        "            from ollama import Client # <-- Import the Client class\n",
        "\n",
        "            # 2. Instantiate the Client explicitly with the host URL\n",
        "            self.client = Client(host=self.base_url)\n",
        "\n",
        "            # 3. Perform a health check/list models using the Client instance\n",
        "            # Note: client.list() is the correct method for the instantiated Client\n",
        "            models = self.client.list()\n",
        "\n",
        "            print(f\"‚úÖ Ollama initialized on {self.base_url} with model: {config.OLLAMA_MODEL}\")\n",
        "\n",
        "        except ImportError:\n",
        "            print(\"‚ùå pip install ollama\")\n",
        "            self.client = None\n",
        "        except requests.exceptions.ConnectionError:\n",
        "            # Catch the error if the server is not actually running at the host\n",
        "            print(f\"‚ùå ConnectionError: Ollama server not running at {self.base_url}\")\n",
        "            self.client = None\n",
        "        except Exception as e:\n",
        "            # Catch other potential errors (like no models pulled)\n",
        "            print(f\"‚ùå Error initializing Ollama client: {e}\")\n",
        "            self.client = None\n",
        "\n",
        "    def generate(self, prompt: str) -> str:\n",
        "        if not self.client:\n",
        "            return \"Error: Ollama not initialized\"\n",
        "        try:\n",
        "            # Use the client instance's generate method\n",
        "            response = self.client.generate(\n",
        "                model=self.config.OLLAMA_MODEL,\n",
        "                prompt=prompt,\n",
        "                options={\n",
        "                    'temperature': self.config.TEMPERATURE,\n",
        "                    'num_predict': self.config.MAX_TOKENS,\n",
        "                }\n",
        "            )\n",
        "            return response['response']\n",
        "        except Exception as e:\n",
        "            return f\"Error during generation: {e}\"\n",
        "\n",
        "\n",
        "class OllamaProvider_local(LLMProvider):\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        try:\n",
        "            import ollama\n",
        "            self.client = ollama\n",
        "            models = self.client.list()\n",
        "            print(f\"‚úÖ Ollama initialized: {config.OLLAMA_MODEL}\")\n",
        "        except ImportError:\n",
        "            print(\"‚ùå pip install ollama\")\n",
        "            self.client = None\n",
        "\n",
        "    def generate(self, prompt: str) -> str:\n",
        "        if not self.client:\n",
        "            return \"Error: Ollama not initialized\"\n",
        "        try:\n",
        "            response = self.client.generate(\n",
        "                model=self.config.OLLAMA_MODEL,\n",
        "                prompt=prompt,\n",
        "                options={\n",
        "                    'temperature': self.config.TEMPERATURE,\n",
        "                    'num_predict': self.config.MAX_TOKENS,\n",
        "                }\n",
        "            )\n",
        "            return response['response']\n",
        "        except Exception as e:\n",
        "            return f\"Error: {e}\"\n",
        "\n",
        "class HuggingFaceProvider(LLMProvider):\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        try:\n",
        "            import torch\n",
        "            from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "            device = \"cuda\" if torch.cuda.is_available() and config.USE_GPU else \"cpu\"\n",
        "            print(f\"üñ•Ô∏è  Device: {device}\")\n",
        "            print(f\"üì• Loading {config.HUGGINGFACE_MODEL}...\")\n",
        "\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(config.HUGGINGFACE_MODEL)\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                config.HUGGINGFACE_MODEL,\n",
        "                torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "                device_map=\"auto\" if device == \"cuda\" else None,\n",
        "                low_cpu_mem_usage=True\n",
        "            )\n",
        "\n",
        "            self.pipeline = pipeline(\n",
        "                \"text-generation\",\n",
        "                model=self.model,\n",
        "                tokenizer=self.tokenizer,\n",
        "                device=0 if device == \"cuda\" else -1\n",
        "            )\n",
        "            print(f\"‚úÖ HuggingFace loaded on {device}\")\n",
        "        except ImportError:\n",
        "            print(\"‚ùå pip install transformers torch\")\n",
        "            self.model = None\n",
        "\n",
        "    def generate(self, prompt: str) -> str:\n",
        "        if not self.model:\n",
        "            return \"Error: HuggingFace not initialized\"\n",
        "        try:\n",
        "            outputs = self.pipeline(\n",
        "                prompt,\n",
        "                max_new_tokens=self.config.MAX_TOKENS,\n",
        "                temperature=self.config.TEMPERATURE,\n",
        "                do_sample=True,\n",
        "                top_p=0.9\n",
        "            )\n",
        "            return outputs[0]['generated_text'][len(prompt):]\n",
        "        except Exception as e:\n",
        "            return f\"Error: {e}\"\n",
        "\n",
        "def get_llm_provider(config: Config) -> LLMProvider:\n",
        "    \"\"\"Factory function to get LLM provider.\"\"\"\n",
        "    provider = config.LLM_PROVIDER.lower()\n",
        "\n",
        "    if provider == 'openai':\n",
        "        return OpenAIProvider(config)\n",
        "    elif provider == 'anthropic':\n",
        "        return AnthropicProvider(config)\n",
        "    elif provider == 'ollama':\n",
        "        return OllamaProvider(config)\n",
        "    elif provider == 'huggingface':\n",
        "        return HuggingFaceProvider(config)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown provider: {provider}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "9b8fede5",
      "metadata": {
        "id": "9b8fede5"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# ORGANIZED FILES ANALYZER\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "class OrganizedCVAnalyzer:\n",
        "    \"\"\"Analyze organized CV files by category.\"\"\"\n",
        "\n",
        "    def __init__(self, organized_path: str):\n",
        "        self.organized_path = Path(organized_path)\n",
        "        self.categories = {}\n",
        "\n",
        "    def load_all_categories(self) -> Dict[str, List[Dict]]:\n",
        "        \"\"\"Load all CV files organized by category.\"\"\"\n",
        "        print(\"\\nüîç Loading organized CV files...\")\n",
        "\n",
        "        for category_dir in self.organized_path.iterdir():\n",
        "            if not category_dir.is_dir():\n",
        "                continue\n",
        "\n",
        "            category = category_dir.name\n",
        "            files = []\n",
        "\n",
        "            for tex_file in category_dir.glob(\"*.tex\"):\n",
        "                if tex_file.name == \"INDEX.md\":\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    with open(tex_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                        content = f.read()\n",
        "\n",
        "                    # Extract source directory from filename\n",
        "                    # Format: SourceDir__filename.tex\n",
        "                    parts = tex_file.stem.split('__')\n",
        "                    source_dir = parts[0] if len(parts) > 1 else 'unknown'\n",
        "                    original_name = '__'.join(parts[1:]) if len(parts) > 1 else tex_file.stem\n",
        "\n",
        "                    files.append({\n",
        "                        'path': str(tex_file),\n",
        "                        'filename': tex_file.name,\n",
        "                        'source_dir': source_dir,\n",
        "                        'original_name': original_name,\n",
        "                        'content': content,\n",
        "                        'size': len(content)\n",
        "                    })\n",
        "                except Exception as e:\n",
        "                    print(f\"   ‚ö†Ô∏è  Error reading {tex_file}: {e}\")\n",
        "\n",
        "            self.categories[category] = files\n",
        "            print(f\"   ‚úÖ {category:20s}: {len(files):3d} files\")\n",
        "\n",
        "        return self.categories\n",
        "\n",
        "    def find_relevant_files(\n",
        "        self,\n",
        "        job_description: str,\n",
        "        category: str,\n",
        "        top_k: int = 5\n",
        "    ) -> List[Dict]:\n",
        "        \"\"\"Find most relevant files for a category based on job description.\"\"\"\n",
        "\n",
        "        if category not in self.categories:\n",
        "            print(f\"‚ö†Ô∏è  Category '{category}' not found\")\n",
        "            return []\n",
        "\n",
        "        files = self.categories[category]\n",
        "\n",
        "        # Extract keywords from job description\n",
        "        keywords = self._extract_keywords(job_description)\n",
        "\n",
        "        # Score each file\n",
        "        scored_files = []\n",
        "        for file_data in files:\n",
        "            score = self._calculate_relevance_score(\n",
        "                file_content=file_data['content'],\n",
        "                keywords=keywords\n",
        "            )\n",
        "            scored_files.append((file_data, score))\n",
        "\n",
        "        # Sort by score\n",
        "        scored_files.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Return top K\n",
        "        top_files = [f for f, score in scored_files[:top_k]]\n",
        "\n",
        "        print(f\"   üìã Top {len(top_files)} files for {category}:\")\n",
        "        for i, (file_data, score) in enumerate(scored_files[:top_k], 1):\n",
        "            print(f\"      {i}. {file_data['source_dir']:30s} (score: {score})\")\n",
        "\n",
        "        return top_files\n",
        "\n",
        "    def _extract_keywords(self, text: str) -> List[str]:\n",
        "        \"\"\"Extract keywords from text.\"\"\"\n",
        "        text = text.lower()\n",
        "\n",
        "        # Domain-specific keywords\n",
        "        keywords = [\n",
        "            # Bioinformatics\n",
        "            'bioinformatics', 'genomics', 'rna', 'dna', 'sequencing', 'ngs',\n",
        "            'cancer', 'biomarker', 'omics', 'proteomics', 'transcriptomics',\n",
        "            'single-cell', 'bulk-rna', 'chip-seq', 'atac-seq',\n",
        "\n",
        "            # Data Science\n",
        "            'data science', 'machine learning', 'deep learning', 'ai',\n",
        "            'statistical', 'modeling', 'prediction', 'classification',\n",
        "            'clustering', 'nlp', 'computer vision',\n",
        "\n",
        "            # Programming\n",
        "            'python', 'r', 'java', 'scala', 'c++', 'julia',\n",
        "            'sql', 'bash', 'perl', 'javascript',\n",
        "\n",
        "            # Tools & Platforms\n",
        "            'aws', 'azure', 'gcp', 'cloud', 'docker', 'kubernetes',\n",
        "            'spark', 'hadoop', 'kafka', 'airflow', 'mlflow',\n",
        "            'tensorflow', 'pytorch', 'scikit-learn',\n",
        "\n",
        "            # Domains\n",
        "            'healthcare', 'pharma', 'clinical', 'drug discovery',\n",
        "            'precision medicine', 'personalized medicine',\n",
        "            'immunotherapy', 'oncology',\n",
        "\n",
        "            # Roles\n",
        "            'scientist', 'engineer', 'architect', 'lead', 'manager',\n",
        "            'analyst', 'developer', 'researcher'\n",
        "        ]\n",
        "\n",
        "        found_keywords = [kw for kw in keywords if kw in text]\n",
        "\n",
        "        # Also extract from phrases\n",
        "        skill_patterns = [\n",
        "            r'\\b(python|r|java|scala|sql)\\b',\n",
        "            r'\\b(aws|azure|gcp)\\b',\n",
        "            r'\\b(docker|kubernetes|spark)\\b',\n",
        "            r'\\b(machine learning|deep learning|ai)\\b',\n",
        "            r'\\b(rna-?seq|chip-?seq|atac-?seq)\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in skill_patterns:\n",
        "            matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "            found_keywords.extend([m.lower() for m in matches])\n",
        "\n",
        "        return list(set(found_keywords))\n",
        "\n",
        "    def _calculate_relevance_score(\n",
        "        self,\n",
        "        file_content: str,\n",
        "        keywords: List[str]\n",
        "    ) -> float:\n",
        "        \"\"\"Calculate relevance score for a file.\"\"\"\n",
        "        file_content_lower = file_content.lower()\n",
        "\n",
        "        score = 0.0\n",
        "\n",
        "        for keyword in keywords:\n",
        "            # Count occurrences\n",
        "            count = file_content_lower.count(keyword)\n",
        "\n",
        "            # Weight by keyword importance (simple heuristic)\n",
        "            weight = 1.0\n",
        "            if keyword in ['python', 'r', 'machine learning', 'bioinformatics']:\n",
        "                weight = 2.0\n",
        "            elif keyword in ['aws', 'docker', 'sql']:\n",
        "                weight = 1.5\n",
        "\n",
        "            score += count * weight\n",
        "\n",
        "        # Normalize by file length\n",
        "        if len(file_content) > 0:\n",
        "            score = score / (len(file_content) / 1000)  # per 1000 chars\n",
        "\n",
        "        return score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "4ab12ffc",
      "metadata": {
        "id": "4ab12ffc"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# CV GENERATOR V2\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "class CVGeneratorV2:\n",
        "    \"\"\"Generate customized CV files using organized reference files.\"\"\"\n",
        "\n",
        "    def __init__(self, llm: LLMProvider, analyzer: OrganizedCVAnalyzer, config: Config):\n",
        "        self.llm = llm\n",
        "        self.analyzer = analyzer\n",
        "        self.config = config\n",
        "\n",
        "    def generate_cv_files(\n",
        "        self,\n",
        "        job_description: str,\n",
        "        job_title: str,\n",
        "        company_name: str,\n",
        "        output_dir: Optional[str] = None\n",
        "    ) -> Dict[str, str]:\n",
        "        \"\"\"Generate all CV files for a job application.\"\"\"\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"üöÄ GENERATING CUSTOMIZED CV\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"   Job Title: {job_title}\")\n",
        "        print(f\"   Company: {company_name}\")\n",
        "\n",
        "        # Create output directory\n",
        "        if not output_dir:\n",
        "            timestamp = datetime.now().strftime(\"%Y_%m\")\n",
        "            safe_title = re.sub(r'[^\\w\\s-]', '', job_title).replace(' ', '_')\n",
        "            safe_company = re.sub(r'[^\\w\\s-]', '', company_name).replace(' ', '_')\n",
        "            output_dir = f\"{self.config.OUTPUT_BASE_PATH}/{safe_title}_{safe_company}_{timestamp}\"\n",
        "\n",
        "        output_path = Path(output_dir)\n",
        "        output_path.mkdir(parents=True, exist_ok=True)\n",
        "        print(f\"   Output: {output_path}\")\n",
        "\n",
        "        generated_files = {}\n",
        "\n",
        "        # Generate each category\n",
        "        for category in self.config.CV_CATEGORIES:\n",
        "            print(f\"\\n{'='*70}\")\n",
        "            print(f\"üìù Generating {category.upper()}\")\n",
        "            print(f\"{'='*70}\")\n",
        "\n",
        "            # Find relevant reference files\n",
        "            reference_files = self.analyzer.find_relevant_files(\n",
        "                job_description=job_description,\n",
        "                category=category,\n",
        "                top_k=5\n",
        "            )\n",
        "\n",
        "            if not reference_files:\n",
        "                print(f\"   ‚ö†Ô∏è  No reference files found for {category}\")\n",
        "                continue\n",
        "\n",
        "            # Generate content\n",
        "            content = self._generate_file_content(\n",
        "                category=category,\n",
        "                job_description=job_description,\n",
        "                job_title=job_title,\n",
        "                company_name=company_name,\n",
        "                reference_files=reference_files\n",
        "            )\n",
        "\n",
        "            if content and \"Error:\" not in content:\n",
        "                filename = f\"{category}.tex\"\n",
        "                filepath = output_path / filename\n",
        "\n",
        "                with open(filepath, 'w', encoding='utf-8') as f:\n",
        "                    f.write(content)\n",
        "\n",
        "                generated_files[filename] = str(filepath)\n",
        "                print(f\"   ‚úÖ Saved: {filename}\")\n",
        "            else:\n",
        "                print(f\"   ‚ùå Failed: {content}\")\n",
        "\n",
        "        # Generate main resume file\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"üìÑ Generating main resume file\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        resume_content = self._generate_resume_main(job_title, company_name)\n",
        "\n",
        "        safe_title = re.sub(r'[^\\w\\s-]', '', job_title).replace(' ', '_')\n",
        "        resume_filename = f\"resume_konstantinos_billis_{safe_title}.tex\"\n",
        "        resume_path = output_path / resume_filename\n",
        "\n",
        "        with open(resume_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(resume_content)\n",
        "\n",
        "        generated_files['main_resume'] = str(resume_path)\n",
        "        print(f\"   ‚úÖ Saved: {resume_filename}\")\n",
        "\n",
        "        # Generate README\n",
        "        readme_content = self._generate_readme(\n",
        "            job_title, company_name, job_description\n",
        "        )\n",
        "\n",
        "        readme_path = output_path / \"README.md\"\n",
        "        with open(readme_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(readme_content)\n",
        "\n",
        "        print(f\"\\n‚úÖ Generated {len(generated_files)} files in: {output_path}\")\n",
        "\n",
        "        return generated_files\n",
        "\n",
        "    def _generate_file_content(\n",
        "        self,\n",
        "        category: str,\n",
        "        job_description: str,\n",
        "        job_title: str,\n",
        "        company_name: str,\n",
        "        reference_files: List[Dict]\n",
        "    ) -> str:\n",
        "        \"\"\"Generate content for a specific category.\"\"\"\n",
        "\n",
        "        # Prepare reference examples\n",
        "        reference_examples = []\n",
        "        for i, ref_file in enumerate(reference_files[:3], 1):\n",
        "            ref_content = ref_file['content'][:1500]  # Limit length\n",
        "            reference_examples.append(\n",
        "                f\"\\n### Example {i} (from {ref_file['source_dir']}):\\n{ref_content}\\n\"\n",
        "            )\n",
        "\n",
        "        reference_text = '\\n'.join(reference_examples)\n",
        "\n",
        "        # Category-specific instructions\n",
        "        category_instructions = {\n",
        "            'summary': \"Create a compelling professional summary (3-5 sentences) similar to Reference Examples and based on job adv\",\n",
        "            'experience': \"Collect all work experiences from Reference Examples, there are 4-5 entries, report those in reverse chronological order using \\\\cventry commands and only adjust the \\\\cvitems of Reference Examples based on the job adv.\",\n",
        "            'key_skills': \"Organize skills by 2 or 3 categories using \\\\cvskill commands as on Reference Examples. Prioritize skills mentioned in the job description and Reference Examples.\",\n",
        "            'education': \"List educational background using \\\\cventry commands. Include relevant coursework or thesis topics if applicable, always based on Reference Examples.\",\n",
        "            'additional_info': \"Include publications, certifications, awards, or other relevant information using \\\\cvhonor or \\\\cventry commands, always based on Reference Examples.\"\n",
        "        }\n",
        "\n",
        "        instruction = category_instructions.get(category, \"Generate appropriate content for this section.\")\n",
        "\n",
        "        prompt = f\"\"\"You are an expert CV writer creating LaTeX files for the Awesome-CV template.\n",
        "\n",
        "**Task:** Generate a {category}.tex file customized for this job application.\n",
        "\n",
        "**Job Information:**\n",
        "- Title: {job_title}\n",
        "- Company: {company_name}\n",
        "\n",
        "**Job Description (extract key requirements):**\n",
        "{job_description[:1500]}\n",
        "\n",
        "**Reference Examples (from successful CVs):**\n",
        "{reference_text}\n",
        "\n",
        "**Instructions:**\n",
        "1. {instruction}\n",
        "2. Use ONLY Awesome-CV LaTeX commands:\n",
        "   - \\\\cventry{{date}}{{title}}{{company}}{{location}}{{description}}\n",
        "   - \\\\cvskill{{category}}{{skills list}}\n",
        "   - \\\\cvhonor{{award}}{{organization}}{{location}}{{date}}\n",
        "3. Use similar keywords to job description, and Tailor content of Reference Examples to match job requirements\n",
        "4. Keep formatting consistent with Reference Examples\n",
        "5. DO NOT include \"Here is the generated `key_skills.tex` file:\", \\\\documentclass, \\\\begin{{document}},  \\\\end{{document}}\n",
        "6. Return ONLY the LaTeX content, no explanations or markdown. Same format and structure as examples. Only select relevant experiences of the Reference Examples and change content based on job description.\n",
        "7. In resume tex file, use full path of files\n",
        "8. Prepare all required files\n",
        "\n",
        "\n",
        "Generate the {category}.tex content now:\n",
        "\"\"\"\n",
        "\n",
        "        print(f\"   ü§ñ Generating with {self.config.LLM_PROVIDER}...\")\n",
        "        return self.llm.generate(prompt)\n",
        "\n",
        "    def _generate_resume_main(self, job_title: str, company_name: str) -> str:\n",
        "        \"\"\"Generate the main resume.tex file.\"\"\"\n",
        "\n",
        "        return r\"\"\"%!TEX TS-program = xelatex\n",
        "%!TEX encoding = UTF-8 Unicode\n",
        "\n",
        "\\documentclass[11pt, a4paper]{awesome-cv}\n",
        "\n",
        "\\geometry{left=1.4cm, top=1cm, right=1.4cm, bottom=1.2cm, footskip=.5cm}\n",
        "\n",
        "\\fontdir[fonts/]\n",
        "\n",
        "\\colorlet{awesome}{awesome-nephritis}\n",
        "\n",
        "\\renewcommand{\\acvHeaderSocialSep}{\\quad\\textbar\\quad}\n",
        "\n",
        "%-------------------------------------------------------------------------------\n",
        "%\tPERSONAL INFORMATION\n",
        "%-------------------------------------------------------------------------------\n",
        "\\photo[circle,noedge,right]{/Users/kbillis/bin/Awesome-CV/examples/photos/kbillis_2024_profphoto.jpg}\n",
        "\n",
        "\\name{Konstantinos}{Billis}\n",
        "\\position{PhD in Bioinformatics{\\enskip\\cdotp\\enskip}Data Solutions Developer in Healthcare \\& Life Sciences}\n",
        "\n",
        "\\address{Zurich, CH (Permit: B) - Citizenships: Greek (EU), British}\n",
        "\\dateofbirth{Aug 16, 1983}\n",
        "\\email{billis.konstantinos@gmail.com}\n",
        "\\homepage{kbillis.github.io/}\n",
        "\\linkedin{konstantinos-billis-phd-baa23a22}\n",
        "\\googlescholar{YsQgBgUAAAAJ&view_op=list_works&sortby=pubdate}{ }\n",
        "\n",
        "%-------------------------------------------------------------------------------\n",
        "\\begin{document}\n",
        "\\newpage\n",
        "\n",
        "\\makecvheader[C]\n",
        "\n",
        "\\makecvfooter\n",
        "  {\\today}\n",
        "  {Konstantinos Billis, PhD~~~¬∑~~~R√©sum√©}\n",
        "  {\\thepage}\n",
        "\n",
        "%-------------------------------------------------------------------------------\n",
        "%\tCV/RESUME CONTENT\n",
        "%\tEach section is imported separately\n",
        "%-------------------------------------------------------------------------------\n",
        "\\input{summary.tex}\n",
        "\\input{key_skills.tex}\n",
        "\\input{experience.tex}\n",
        "\\input{education.tex}\n",
        "\\input{additional_info.tex}\n",
        "\n",
        "\\end{document}\n",
        "\"\"\"\n",
        "\n",
        "    def _generate_readme(self, job_title: str, company_name: str, job_description: str) -> str:\n",
        "        \"\"\"Generate README for the CV directory.\"\"\"\n",
        "        return f\"\"\"# CV for {job_title} at {company_name}\n",
        "\n",
        "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "Using: AI-Powered CV Customizer V2\n",
        "\n",
        "## Job Description\n",
        "\n",
        "{job_description[:500]}...\n",
        "\n",
        "## Files Generated\n",
        "\n",
        "- `resume_konstantinos_billis_*.tex` - Main resume file\n",
        "- `summary.tex` - Professional summary\n",
        "- `experience.tex` - Work experience\n",
        "- `key_skills.tex` - Key skills\n",
        "- `education.tex` - Education\n",
        "- `additional_info.tex` - Publications, awards, etc.\n",
        "\n",
        "## Compilation\n",
        "```bash\n",
        "cd \"{os.path.basename(self.config.OUTPUT_BASE_PATH)}\"\n",
        "xelatex resume_konstantinos_billis_*.tex\n",
        "```\n",
        "\n",
        "## Customization\n",
        "\n",
        "Generated using:\n",
        "- LLM Provider: {self.config.LLM_PROVIDER}\n",
        "- Reference files from organized CV database\n",
        "- Keyword matching and relevance scoring\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "1. Review all generated files\n",
        "2. Customize any sections as needed\n",
        "3. Compile to PDF\n",
        "4. Proofread final output\n",
        "\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "0e4bd87d",
      "metadata": {
        "id": "0e4bd87d"
      },
      "outputs": [],
      "source": [
        "read_adv = False\n",
        "if read_adv:\n",
        "  #parse the page with the job description\n",
        "  # langchain gets the public description of the job\n",
        "  # make sure we are not behind a proxy\n",
        "  from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "  jd_url=\"https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/4314922582\"\n",
        "  print(f\"Getting information for job {jd_url}\")\n",
        "  loader = WebBaseLoader(jd_url )\n",
        "  loader.default_parser=\"html.parser\"\n",
        "  docs = loader.load()\n",
        "  # docs[0]\n",
        "\n",
        "\n",
        "  # from langchain_unstructured import UnstructuredLoader\n",
        "  # loader = UnstructuredLoader(web_url=jd_url)\n",
        "  # docs=loader.load()\n",
        "\n",
        "  description = docs[0].page_content\n",
        "  # print(f\"Job Description\\n{description} \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "49cf70f0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49cf70f0",
        "outputId": "c25eeff0-451f-4195-f528-4d67a79170e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "üìÑ AI-POWERED CV CUSTOMIZER V2\n",
            "======================================================================\n",
            "\n",
            "‚öôÔ∏è  Configuration:\n",
            "   LLM Provider: ollama\n",
            "   Organized Files: /content/drive/MyDrive/Documents/work/jobs_work_applications_and_supporting_docs/AI/CVS_awesome_files/\n",
            "   Output Path: /content/drive/MyDrive/Documents/work/jobs_work_applications_and_supporting_docs/AI/output/\n",
            "\n",
            "ü§ñ Initializing LLM...\n",
            "‚úÖ Ollama initialized on 127.0.0.1:11434 with model: llama3.1:8b\n",
            "\n",
            "üìö Loading organized CV files...\n",
            "\n",
            "üîç Loading organized CV files...\n",
            "   ‚úÖ additional_info     :   7 files\n",
            "   ‚úÖ education           :   9 files\n",
            "   ‚úÖ experience          :  48 files\n",
            "   ‚úÖ key_skills          :  47 files\n",
            "   ‚úÖ summary             :  39 files\n",
            "\n",
            "======================================================================\n",
            "üöÄ GENERATING CUSTOMIZED CV\n",
            "======================================================================\n",
            "   Job Title: Senior Bioinformatics Scientist\n",
            "   Company: Roche\n",
            "   Output: /content/drive/MyDrive/Documents/work/jobs_work_applications_and_supporting_docs/AI/output/Senior_Bioinformatics_Scientist_Roche_2025_11\n",
            "\n",
            "======================================================================\n",
            "üìù Generating SUMMARY\n",
            "======================================================================\n",
            "   üìã Top 5 files for summary:\n",
            "      1. Plants_2025_03                 (score: 83.67181153533711)\n",
            "      2. Bioinformatics_2025_10_SIB     (score: 80.64516129032258)\n",
            "      3. DS_Healthcare_2025_04          (score: 79.8283261802575)\n",
            "      4. Bioinformatics_2025_06         (score: 79.4590025359256)\n",
            "      5. Bioinformatics_2025_05         (score: 78.99461400359066)\n",
            "   ü§ñ Generating with ollama...\n",
            "   ‚úÖ Saved: summary.tex\n",
            "\n",
            "======================================================================\n",
            "üìù Generating EXPERIENCE\n",
            "======================================================================\n",
            "   üìã Top 5 files for experience:\n",
            "      1. client_Platform_Nvidia_Med_2025_10 (score: 96.48230988206588)\n",
            "      2. Bioinformatics_2025_10_SIB     (score: 93.50202126066776)\n",
            "      3. LEAD_data_engineer_JJ_2025_10  (score: 93.02087597216537)\n",
            "      4. client_Platform_Med_2025_10    (score: 90.89951548346325)\n",
            "      5. Bioinformatics_2025_09         (score: 90.34341524889729)\n",
            "   ü§ñ Generating with ollama...\n",
            "   ‚úÖ Saved: experience.tex\n",
            "\n",
            "======================================================================\n",
            "üìù Generating KEY_SKILLS\n",
            "======================================================================\n",
            "   üìã Top 5 files for key_skills:\n",
            "      1. client_Platform_Med_2025_10    (score: 74.4944283945522)\n",
            "      2. LEAD_data_engineer_JJ_2025_10  (score: 74.47257383966245)\n",
            "      3. business_Data_Prec_Med_2025_10 (score: 74.2818671454219)\n",
            "      4. client_Platform_Nvidia_Med_2025_10 (score: 72.75350370981039)\n",
            "      5. Plants_2025_09                 (score: 67.68720029509406)\n",
            "   ü§ñ Generating with ollama...\n",
            "   ‚úÖ Saved: key_skills.tex\n",
            "\n",
            "======================================================================\n",
            "üìù Generating EDUCATION\n",
            "======================================================================\n",
            "   üìã Top 5 files for education:\n",
            "      1. Bioinformatics_2025_10_SIB     (score: 59.03398926654741)\n",
            "      2. Plants_2025_09                 (score: 57.30866884294523)\n",
            "      3. Bioinformatics_2025_10_RNA     (score: 54.20054200542005)\n",
            "      4. Data_Science_Novartis_2025_08  (score: 51.97342711996873)\n",
            "      5. LEAD_data_engineer_JJ_2025_10  (score: 48.58142246404975)\n",
            "   ü§ñ Generating with ollama...\n",
            "   ‚úÖ Saved: education.tex\n",
            "\n",
            "======================================================================\n",
            "üìù Generating ADDITIONAL_INFO\n",
            "======================================================================\n",
            "   üìã Top 5 files for additional_info:\n",
            "      1. Bioinformatics_2025_10_SIB     (score: 63.05922454196847)\n",
            "      2. Bioinformatics_2025_09_cancer  (score: 56.053811659192824)\n",
            "      3. Bioinformatics_2025_09_biomarkers (score: 54.92638731596829)\n",
            "      4. Data_Analytics_Med_2025_08     (score: 54.80225988700565)\n",
            "      5. Bioinformatics_2025_10_RNA     (score: 54.80225988700565)\n",
            "   ü§ñ Generating with ollama...\n",
            "   ‚úÖ Saved: additional_info.tex\n",
            "\n",
            "======================================================================\n",
            "üìÑ Generating main resume file\n",
            "======================================================================\n",
            "   ‚úÖ Saved: resume_konstantinos_billis_Senior_Bioinformatics_Scientist.tex\n",
            "\n",
            "‚úÖ Generated 6 files in: /content/drive/MyDrive/Documents/work/jobs_work_applications_and_supporting_docs/AI/output/Senior_Bioinformatics_Scientist_Roche_2025_11\n",
            "\n",
            "======================================================================\n",
            "‚úÖ CV GENERATION COMPLETE!\n",
            "======================================================================\n",
            "\n",
            "Generated files:\n",
            "   üìÑ summary.tex\n",
            "   üìÑ experience.tex\n",
            "   üìÑ key_skills.tex\n",
            "   üìÑ education.tex\n",
            "   üìÑ additional_info.tex\n",
            "   üìÑ main_resume\n",
            "\n",
            "üí° Next steps:\n",
            "   1. Review generated files\n",
            "   2. Compile: xelatex resume_*.tex\n",
            "   3. Customize if needed\n"
          ]
        }
      ],
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# MAIN WORKFLOW\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "def main(job_description: str, job_title: str, company_name: str):\n",
        "    \"\"\"Main execution workflow.\"\"\"\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"üìÑ AI-POWERED CV CUSTOMIZER V2\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Initialize\n",
        "    config = Config()\n",
        "\n",
        "    print(f\"\\n‚öôÔ∏è  Configuration:\")\n",
        "    print(f\"   LLM Provider: {config.LLM_PROVIDER}\")\n",
        "    print(f\"   Organized Files: {config.ORGANIZED_FILES_PATH}\")\n",
        "    print(f\"   Output Path: {config.OUTPUT_BASE_PATH}\")\n",
        "\n",
        "    # Initialize LLM\n",
        "    print(f\"\\nü§ñ Initializing LLM...\")\n",
        "    llm = get_llm_provider(config)\n",
        "\n",
        "    # Initialize Analyzer\n",
        "    print(f\"\\nüìö Loading organized CV files...\")\n",
        "    analyzer = OrganizedCVAnalyzer(config.ORGANIZED_FILES_PATH)\n",
        "    analyzer.load_all_categories()\n",
        "\n",
        "    # Generate CV\n",
        "    generator = CVGeneratorV2(llm, analyzer, config)\n",
        "\n",
        "    generated_files = generator.generate_cv_files(\n",
        "        job_description=job_description,\n",
        "        job_title=job_title,\n",
        "        company_name=company_name\n",
        "    )\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"‚úÖ CV GENERATION COMPLETE!\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nGenerated files:\")\n",
        "    for filename, filepath in generated_files.items():\n",
        "        print(f\"   üìÑ {filename}\")\n",
        "\n",
        "    print(\"\\nüí° Next steps:\")\n",
        "    print(\"   1. Review generated files\")\n",
        "    print(\"   2. Compile: xelatex resume_*.tex\")\n",
        "    print(\"   3. Customize if needed\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Example job application\n",
        "    job_description = \"\"\"\n",
        "    Senior Bioinformatics Scientist - Cancer Genomics\n",
        "\n",
        "    Join our Oncology Research team to lead bioinformatics analysis of\n",
        "    multi-omics cancer data. The ideal candidate will have extensive\n",
        "    experience in cancer genomics, RNA-seq analysis, and biomarker discovery.\n",
        "\n",
        "    Key Responsibilities:\n",
        "    - Lead bioinformatics analysis of multi-omics cancer data\n",
        "    - Develop computational pipelines for biomarker discovery\n",
        "    - Collaborate with wet-lab scientists and clinicians\n",
        "    - Mentor junior bioinformaticians\n",
        "\n",
        "    Required Skills:\n",
        "    - PhD in Bioinformatics, Computational Biology, or related field\n",
        "    - 5+ years experience in cancer genomics\n",
        "    - Expert in Python, R, and Unix/Linux\n",
        "    - Experience with RNA-seq, WGS, WES analysis\n",
        "    - Strong publication record\n",
        "    - Knowledge of AWS/cloud computing\n",
        "\n",
        "    Preferred:\n",
        "    - Experience with single-cell sequencing\n",
        "    - Knowledge of immunotherapy biomarkers\n",
        "    - Machine learning experience\n",
        "    \"\"\"\n",
        "\n",
        "    job_title = \"Senior Bioinformatics Scientist\"\n",
        "    company_name = \"Roche\"\n",
        "\n",
        "    main(job_description, job_title, company_name)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "everyday_repo",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
