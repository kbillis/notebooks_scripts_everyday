{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "df9284d1",
      "metadata": {
        "id": "df9284d1"
      },
      "source": [
        "## Use Gen/AI to prepare CV"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eeeb7f4f",
      "metadata": {
        "id": "eeeb7f4f"
      },
      "source": [
        "### libraries\n",
        "use GPU!!! may be google colab! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ca13dc7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ca13dc7",
        "outputId": "1a06f4c1-954c-4d2b-b409-950ed8ce4b45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pathlib in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (1.0.1)\n",
            "Requirement already satisfied: openai in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (2.6.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from openai) (0.11.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from openai) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Requirement already satisfied: anthropic in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (0.72.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from anthropic) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: docstring-parser<1,>=0.15 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from anthropic) (0.17.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from anthropic) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from anthropic) (0.11.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from anthropic) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from anthropic) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from anyio<5,>=3.5.0->anthropic) (3.11)\n",
            "Requirement already satisfied: certifi in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from httpx<1,>=0.25.0->anthropic) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->anthropic) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.2)\n",
            "Requirement already satisfied: ollama in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (0.6.0)\n",
            "Requirement already satisfied: httpx>=0.27 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from ollama) (0.28.1)\n",
            "Requirement already satisfied: pydantic>=2.9 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from ollama) (2.12.3)\n",
            "Requirement already satisfied: anyio in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from httpx>=0.27->ollama) (4.11.0)\n",
            "Requirement already satisfied: certifi in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from httpx>=0.27->ollama) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from httpx>=0.27->ollama) (1.0.9)\n",
            "Requirement already satisfied: idna in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from httpx>=0.27->ollama) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from pydantic>=2.9->ollama) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from pydantic>=2.9->ollama) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from pydantic>=2.9->ollama) (0.4.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from anyio->httpx>=0.27->ollama) (1.3.1)\n",
            "Requirement already satisfied: transformers in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (4.57.1)\n",
            "Requirement already satisfied: torch in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (2.7.1)\n",
            "Requirement already satisfied: accelerate in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (1.11.0)\n",
            "Requirement already satisfied: filelock in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from transformers) (2.3.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from transformers) (2025.10.23)\n",
            "Requirement already satisfied: requests in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from transformers) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from torch) (80.9.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: psutil in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from accelerate) (7.1.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/kbillis/miniconda3/envs/everyday_repo/lib/python3.13/site-packages (from requests->transformers) (2025.10.5)\n",
            "\n",
            "Installing Ollama Server Application...\n",
            "\u001b[1m\u001b[31mERROR:\u001b[m This script is intended to run on Linux only.\n"
          ]
        }
      ],
      "source": [
        "install_libraries = True\n",
        "\n",
        "# Toggle this to True when running in Colab\n",
        "USE_COLAB = False\n",
        "\n",
        "\n",
        "if install_libraries:\n",
        "    # --- Base Python Requirements ---\n",
        "    !pip install pathlib\n",
        "    !pip install openai\n",
        "    !pip install anthropic\n",
        "    !pip install ollama\n",
        "    !pip install transformers torch accelerate\n",
        "\n",
        "    # --- âš ï¸ CRITICAL: Install Ollama Server Application ---\n",
        "    # This downloads and installs the *Ollama executable* on the Colab VM.\n",
        "    print(\"\\nInstalling Ollama Server Application...\")\n",
        "    !curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "    # --- âŒ REMOVE THIS LINE ---\n",
        "    # !ollama pull llama3.1:8b\n",
        "    # This must be done *after* the server is started in the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67f3dd82",
      "metadata": {
        "id": "67f3dd82"
      },
      "outputs": [],
      "source": [
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import glob\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# ðŸ“„ AI-POWERED CV CUSTOMIZER V2\n",
        "# Uses organized/deduplicated CV files as reference\n",
        "# Supports multiple LLM providers (OpenAI, Anthropic, Ollama, HuggingFace)\n",
        "# CPU & GPU Compatible\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "\n",
        "## TODO:   from langchain_community.document_loaders import WebBaseLoader\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "375b8026",
      "metadata": {
        "id": "375b8026"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "R9Zr3GFFXphj",
      "metadata": {
        "id": "R9Zr3GFFXphj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "def setup_ollama_in_colab(model_name: str = 'llama3.1:8b'):\n",
        "    \"\"\"Starts the Ollama server and pulls the required model.\"\"\"\n",
        "\n",
        "    print(\"\\n--- ðŸ› ï¸ Starting Ollama Server and Pulling Model ---\")\n",
        "\n",
        "    # 1. Set Environment Variable and Start the Ollama Service\n",
        "    ollama_host = '127.0.0.1:11434'\n",
        "    os.environ['OLLAMA_HOST'] = ollama_host\n",
        "    print(f\"1. Setting OLLAMA_HOST environment variable to: {ollama_host}\")\n",
        "\n",
        "    # Start the Ollama server process in the background\n",
        "    print(\"2. Starting 'ollama serve' in background...\")\n",
        "    try:\n",
        "        # Popen is critical here to keep the process running after this cell finishes\n",
        "        subprocess.Popen(['ollama', 'serve'], start_new_session=True)\n",
        "        time.sleep(5) # Wait for the server to start\n",
        "        print(\"   -> Server started successfully on 127.0.0.1:11434.\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ERROR: Failed to start server: {e}\")\n",
        "        return\n",
        "\n",
        "    # 3. Pull the Model\n",
        "    print(f\"3. Pulling model: {model_name}. This may take some time...\")\n",
        "    try:\n",
        "        subprocess.run(\n",
        "            ['ollama', 'pull', model_name],\n",
        "            check=True,\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE,\n",
        "            text=True\n",
        "        )\n",
        "        print(f\"âœ… Successfully pulled model: {model_name}\")\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"âŒ ERROR: Failed to pull model {model_name}. Check model name and internet connection.\")\n",
        "        print(f\"   Stderr: {e.stderr.strip()}\")\n",
        "\n",
        "    print(\"--- âœ… Ollama Setup Complete ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "227620f3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "227620f3",
        "outputId": "27351e4d-697b-4c1c-9200-a80a0386ca80"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# User selects LLM provider explicitly\n",
        "LLM_PROVIDER = input(\"Select LLM provider (openai, anthropic, ollama, huggingface): \").strip().lower()\n",
        "\n",
        "# Set environment variable for consistency (optional)\n",
        "os.environ['LLM_PROVIDER'] = LLM_PROVIDER\n",
        "\n",
        "\n",
        "if USE_COLAB:\n",
        "    from google.colab import drive\n",
        "\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Colab-specific paths\n",
        "    ORGANIZED_FILES_PATH = \"/content/drive/MyDrive/Documents/work/jobs_work_applications_and_supporting_docs/AI/CVS_awesome_files/\"\n",
        "    OUTPUT_BASE_PATH = \"/content/drive/MyDrive/Documents/work/jobs_work_applications_and_supporting_docs/AI/output/\"\n",
        "\n",
        "    import os\n",
        "    # List files in the folder\n",
        "    for filename in os.listdir(ORGANIZED_FILES_PATH):\n",
        "      print(filename)\n",
        "\n",
        "\n",
        "    # Prompt for API keys\n",
        "    openai_key = input(\"Enter your OpenAI API key (leave blank if not using): \").strip()\n",
        "    anthropic_key = input(\"Enter your Anthropic API key (leave blank if not using): \").strip()\n",
        "    huggingface_key = input(\"Enter your HuggingFace API key (leave blank if not using): \").strip()\n",
        "\n",
        "    # Set environment variables\n",
        "    os.environ['OPENAI_API_KEY'] = openai_key\n",
        "    os.environ['ANTHROPIC_API_KEY'] = anthropic_key\n",
        "    os.environ['HUGGINGFACE_API_KEY'] = huggingface_key\n",
        "\n",
        "    if LLM_PROVIDER == \"ollama\":\n",
        "        # --- EXECUTE THE FUNCTION ---\n",
        "        # Pass the model name you need from your Config\n",
        "        setup_ollama_in_colab(model_name='llama3.1:8b')\n",
        "\n",
        "else:\n",
        "    # Local paths\n",
        "    ORGANIZED_FILES_PATH = \"/Users/kbillis/bin/Awesome-CV/organized_filesdeduplicated\"\n",
        "    OUTPUT_BASE_PATH = \"/Users/kbillis/bin/Awesome-CV/examples/AI/\"\n",
        "\n",
        "\n",
        "# Unified config class\n",
        "class Config:\n",
        "    \"\"\"Configuration for CV generation.\"\"\"\n",
        "\n",
        "    ORGANIZED_FILES_PATH = ORGANIZED_FILES_PATH\n",
        "    OUTPUT_BASE_PATH = OUTPUT_BASE_PATH\n",
        "\n",
        "    LLM_PROVIDER = LLM_PROVIDER\n",
        "\n",
        "    OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY', '')\n",
        "    ANTHROPIC_API_KEY = os.environ.get('ANTHROPIC_API_KEY', '')\n",
        "    HUGGINGFACE_API_KEY = os.environ.get('HUGGINGFACE_API_KEY', '')\n",
        "\n",
        "    OPENAI_MODEL = 'gpt-4'\n",
        "    ANTHROPIC_MODEL = 'claude-3-5-sonnet-20241022'\n",
        "    OLLAMA_MODEL = 'llama3.1:8b'\n",
        "    HUGGINGFACE_MODEL = 'mistralai/Mixtral-8x7B-Instruct-v0.1'\n",
        "\n",
        "    USE_GPU = True\n",
        "    MAX_TOKENS = 2000\n",
        "    TEMPERATURE = 0.3\n",
        "\n",
        "    CV_CATEGORIES = ['summary', 'experience', 'key_skills', 'education', 'additional_info']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1gqL22d-bwib",
      "metadata": {
        "id": "1gqL22d-bwib"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8oPup2JObykd",
      "metadata": {
        "id": "8oPup2JObykd"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# test your openai key\n",
        "# import os\n",
        "# from openai import OpenAI\n",
        "\n",
        "# client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "#      api_key=openai_key,\n",
        "# )\n",
        "\n",
        "# response = client.responses.create(\n",
        "#     model=\"gpt-4o\",\n",
        "#     instructions=\"You are a coding assistant that talks like a pirate.\",\n",
        "#     input=\"How do I check if a Python object is an instance of a class?\",\n",
        "# )\n",
        "\n",
        "# print(response.output_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6f78b00",
      "metadata": {
        "id": "f6f78b00"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# LLM PROVIDERS (Same as before - reusing)\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "class LLMProvider:\n",
        "    \"\"\"Base class for LLM providers.\"\"\"\n",
        "    def generate(self, prompt: str) -> str:\n",
        "        raise NotImplementedError\n",
        "\n",
        "class OpenAIProvider(LLMProvider):\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        try:\n",
        "            from openai import OpenAI\n",
        "            self.client = OpenAI(api_key=config.OPENAI_API_KEY)\n",
        "            print(f\"âœ… OpenAI initialized: {config.OPENAI_MODEL}\")\n",
        "        except ImportError:\n",
        "            print(\"âŒ pip install openai\")\n",
        "            self.client = None\n",
        "\n",
        "    def generate(self, prompt: str) -> str:\n",
        "        if not self.client:\n",
        "            return \"Error: OpenAI not initialized\"\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.config.OPENAI_MODEL,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                max_tokens=self.config.MAX_TOKENS,\n",
        "                temperature=self.config.TEMPERATURE\n",
        "            )\n",
        "            return response.choices[0].message.content\n",
        "        except Exception as e:\n",
        "            return f\"Error: {e}\"\n",
        "\n",
        "class AnthropicProvider(LLMProvider):\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        try:\n",
        "            from anthropic import Anthropic\n",
        "            self.client = Anthropic(api_key=config.ANTHROPIC_API_KEY)\n",
        "            print(f\"âœ… Anthropic initialized: {config.ANTHROPIC_MODEL}\")\n",
        "        except ImportError:\n",
        "            print(\"âŒ pip install anthropic\")\n",
        "            self.client = None\n",
        "\n",
        "    def generate(self, prompt: str) -> str:\n",
        "        if not self.client:\n",
        "            return \"Error: Anthropic not initialized\"\n",
        "        try:\n",
        "            message = self.client.messages.create(\n",
        "                model=self.config.ANTHROPIC_MODEL,\n",
        "                max_tokens=self.config.MAX_TOKENS,\n",
        "                temperature=self.config.TEMPERATURE,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "            )\n",
        "            return message.content[0].text\n",
        "        except Exception as e:\n",
        "            return f\"Error: {e}\"\n",
        "\n",
        "\n",
        "class OllamaProvider(LLMProvider):\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "\n",
        "        # 1. Define the local URL (Ollama's default)\n",
        "        # You can add this to your Config class for consistency if you want,\n",
        "        # but the default is usually fine for local use.\n",
        "        self.base_url = os.environ.get('OLLAMA_HOST', 'http://127.0.0.1:11434')\n",
        "\n",
        "        try:\n",
        "            from ollama import Client # <-- Import the Client class\n",
        "\n",
        "            # 2. Instantiate the Client explicitly with the host URL\n",
        "            self.client = Client(host=self.base_url)\n",
        "\n",
        "            # 3. Perform a health check/list models using the Client instance\n",
        "            # Note: client.list() is the correct method for the instantiated Client\n",
        "            models = self.client.list()\n",
        "\n",
        "            print(f\"âœ… Ollama initialized on {self.base_url} with model: {config.OLLAMA_MODEL}\")\n",
        "\n",
        "        except ImportError:\n",
        "            print(\"âŒ pip install ollama\")\n",
        "            self.client = None\n",
        "        except requests.exceptions.ConnectionError:\n",
        "            # Catch the error if the server is not actually running at the host\n",
        "            print(f\"âŒ ConnectionError: Ollama server not running at {self.base_url}\")\n",
        "            self.client = None\n",
        "        except Exception as e:\n",
        "            # Catch other potential errors (like no models pulled)\n",
        "            print(f\"âŒ Error initializing Ollama client: {e}\")\n",
        "            self.client = None\n",
        "\n",
        "    def generate(self, prompt: str) -> str:\n",
        "        if not self.client:\n",
        "            return \"Error: Ollama not initialized\"\n",
        "        try:\n",
        "            # Use the client instance's generate method\n",
        "            response = self.client.generate(\n",
        "                model=self.config.OLLAMA_MODEL,\n",
        "                prompt=prompt,\n",
        "                options={\n",
        "                    'temperature': self.config.TEMPERATURE,\n",
        "                    'num_predict': self.config.MAX_TOKENS,\n",
        "                }\n",
        "            )\n",
        "            return response['response']\n",
        "        except Exception as e:\n",
        "            return f\"Error during generation: {e}\"\n",
        "\n",
        "\n",
        "class OllamaProvider_local(LLMProvider):\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        try:\n",
        "            import ollama\n",
        "            self.client = ollama\n",
        "            models = self.client.list()\n",
        "            print(f\"âœ… Ollama initialized: {config.OLLAMA_MODEL}\")\n",
        "        except ImportError:\n",
        "            print(\"âŒ pip install ollama\")\n",
        "            self.client = None\n",
        "\n",
        "    def generate(self, prompt: str) -> str:\n",
        "        if not self.client:\n",
        "            return \"Error: Ollama not initialized\"\n",
        "        try:\n",
        "            response = self.client.generate(\n",
        "                model=self.config.OLLAMA_MODEL,\n",
        "                prompt=prompt,\n",
        "                options={\n",
        "                    'temperature': self.config.TEMPERATURE,\n",
        "                    'num_predict': self.config.MAX_TOKENS,\n",
        "                }\n",
        "            )\n",
        "            return response['response']\n",
        "        except Exception as e:\n",
        "            return f\"Error: {e}\"\n",
        "\n",
        "class HuggingFaceProvider(LLMProvider):\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        try:\n",
        "            import torch\n",
        "            from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "            device = \"cuda\" if torch.cuda.is_available() and config.USE_GPU else \"cpu\"\n",
        "            print(f\"ðŸ–¥ï¸  Device: {device}\")\n",
        "            print(f\"ðŸ“¥ Loading {config.HUGGINGFACE_MODEL}...\")\n",
        "\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(config.HUGGINGFACE_MODEL)\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                config.HUGGINGFACE_MODEL,\n",
        "                torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "                device_map=\"auto\" if device == \"cuda\" else None,\n",
        "                low_cpu_mem_usage=True\n",
        "            )\n",
        "\n",
        "            self.pipeline = pipeline(\n",
        "                \"text-generation\",\n",
        "                model=self.model,\n",
        "                tokenizer=self.tokenizer,\n",
        "                device=0 if device == \"cuda\" else -1\n",
        "            )\n",
        "            print(f\"âœ… HuggingFace loaded on {device}\")\n",
        "        except ImportError:\n",
        "            print(\"âŒ pip install transformers torch\")\n",
        "            self.model = None\n",
        "\n",
        "    def generate(self, prompt: str) -> str:\n",
        "        if not self.model:\n",
        "            return \"Error: HuggingFace not initialized\"\n",
        "        try:\n",
        "            outputs = self.pipeline(\n",
        "                prompt,\n",
        "                max_new_tokens=self.config.MAX_TOKENS,\n",
        "                temperature=self.config.TEMPERATURE,\n",
        "                do_sample=True,\n",
        "                top_p=0.9\n",
        "            )\n",
        "            return outputs[0]['generated_text'][len(prompt):]\n",
        "        except Exception as e:\n",
        "            return f\"Error: {e}\"\n",
        "\n",
        "def get_llm_provider(config: Config) -> LLMProvider:\n",
        "    \"\"\"Factory function to get LLM provider.\"\"\"\n",
        "    provider = config.LLM_PROVIDER.lower()\n",
        "\n",
        "    if provider == 'openai':\n",
        "        return OpenAIProvider(config)\n",
        "    elif provider == 'anthropic':\n",
        "        return AnthropicProvider(config)\n",
        "    elif provider == 'ollama':\n",
        "        return OllamaProvider(config)\n",
        "    elif provider == 'huggingface':\n",
        "        return HuggingFaceProvider(config)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown provider: {provider}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b8fede5",
      "metadata": {
        "id": "9b8fede5"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# ORGANIZED FILES ANALYZER\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "class OrganizedCVAnalyzer:\n",
        "    \"\"\"Analyze organized CV files by category.\"\"\"\n",
        "\n",
        "    def __init__(self, organized_path: str):\n",
        "        self.organized_path = Path(organized_path)\n",
        "        self.categories = {}\n",
        "\n",
        "    def load_all_categories(self) -> Dict[str, List[Dict]]:\n",
        "        \"\"\"Load all CV files organized by category.\"\"\"\n",
        "        print(\"\\nðŸ” Loading organized CV files...\")\n",
        "\n",
        "        for category_dir in self.organized_path.iterdir():\n",
        "            if not category_dir.is_dir():\n",
        "                continue\n",
        "\n",
        "            category = category_dir.name\n",
        "            files = []\n",
        "\n",
        "            for tex_file in category_dir.glob(\"*.tex\"):\n",
        "                if tex_file.name == \"INDEX.md\":\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    with open(tex_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                        content = f.read()\n",
        "\n",
        "                    # Extract source directory from filename\n",
        "                    # Format: SourceDir__filename.tex\n",
        "                    parts = tex_file.stem.split('__')\n",
        "                    source_dir = parts[0] if len(parts) > 1 else 'unknown'\n",
        "                    original_name = '__'.join(parts[1:]) if len(parts) > 1 else tex_file.stem\n",
        "\n",
        "                    files.append({\n",
        "                        'path': str(tex_file),\n",
        "                        'filename': tex_file.name,\n",
        "                        'source_dir': source_dir,\n",
        "                        'original_name': original_name,\n",
        "                        'content': content,\n",
        "                        'size': len(content)\n",
        "                    })\n",
        "                except Exception as e:\n",
        "                    print(f\"   âš ï¸  Error reading {tex_file}: {e}\")\n",
        "\n",
        "            self.categories[category] = files\n",
        "            print(f\"   âœ… {category:20s}: {len(files):3d} files\")\n",
        "\n",
        "        return self.categories\n",
        "\n",
        "    def find_relevant_files(\n",
        "        self,\n",
        "        job_description: str,\n",
        "        category: str,\n",
        "        top_k: int = 5\n",
        "    ) -> List[Dict]:\n",
        "        \"\"\"Find most relevant files for a category based on job description.\"\"\"\n",
        "\n",
        "        if category not in self.categories:\n",
        "            print(f\"âš ï¸  Category '{category}' not found\")\n",
        "            return []\n",
        "\n",
        "        files = self.categories[category]\n",
        "\n",
        "        # Extract keywords from job description\n",
        "        keywords = self._extract_keywords(job_description)\n",
        "\n",
        "        # Score each file\n",
        "        scored_files = []\n",
        "        for file_data in files:\n",
        "            score = self._calculate_relevance_score(\n",
        "                file_content=file_data['content'],\n",
        "                keywords=keywords\n",
        "            )\n",
        "            scored_files.append((file_data, score))\n",
        "\n",
        "        # Sort by score\n",
        "        scored_files.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Return top K\n",
        "        top_files = [f for f, score in scored_files[:top_k]]\n",
        "\n",
        "        print(f\"   ðŸ“‹ Top {len(top_files)} files for {category}:\")\n",
        "        for i, (file_data, score) in enumerate(scored_files[:top_k], 1):\n",
        "            print(f\"      {i}. {file_data['source_dir']:30s} (score: {score})\")\n",
        "\n",
        "        return top_files\n",
        "\n",
        "    def _extract_keywords(self, text: str) -> List[str]:\n",
        "        \"\"\"Extract keywords from text.\"\"\"\n",
        "        text = text.lower()\n",
        "\n",
        "        # Domain-specific keywords\n",
        "        keywords = [\n",
        "            # Bioinformatics\n",
        "            'bioinformatics', 'genomics', 'rna', 'dna', 'sequencing', 'ngs',\n",
        "            'cancer', 'biomarker', 'omics', 'proteomics', 'transcriptomics',\n",
        "            'single-cell', 'bulk-rna', 'chip-seq', 'atac-seq',\n",
        "\n",
        "            # Data Science\n",
        "            'data science', 'machine learning', 'deep learning', 'ai',\n",
        "            'statistical', 'modeling', 'prediction', 'classification',\n",
        "            'clustering', 'nlp', 'computer vision',\n",
        "\n",
        "            # Programming\n",
        "            'python', 'r', 'java', 'scala', 'c++', 'julia',\n",
        "            'sql', 'bash', 'perl', 'javascript',\n",
        "\n",
        "            # Tools & Platforms\n",
        "            'aws', 'azure', 'gcp', 'cloud', 'docker', 'kubernetes',\n",
        "            'spark', 'hadoop', 'kafka', 'airflow', 'mlflow',\n",
        "            'tensorflow', 'pytorch', 'scikit-learn',\n",
        "\n",
        "            # Domains\n",
        "            'healthcare', 'pharma', 'clinical', 'drug discovery',\n",
        "            'precision medicine', 'personalized medicine',\n",
        "            'immunotherapy', 'oncology',\n",
        "\n",
        "            # Roles\n",
        "            'scientist', 'engineer', 'architect', 'lead', 'manager',\n",
        "            'analyst', 'developer', 'researcher'\n",
        "        ]\n",
        "\n",
        "        found_keywords = [kw for kw in keywords if kw in text]\n",
        "\n",
        "        # Also extract from phrases\n",
        "        skill_patterns = [\n",
        "            r'\\b(python|r|java|scala|sql)\\b',\n",
        "            r'\\b(aws|azure|gcp)\\b',\n",
        "            r'\\b(docker|kubernetes|spark)\\b',\n",
        "            r'\\b(machine learning|deep learning|ai)\\b',\n",
        "            r'\\b(rna-?seq|chip-?seq|atac-?seq)\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in skill_patterns:\n",
        "            matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "            found_keywords.extend([m.lower() for m in matches])\n",
        "\n",
        "        return list(set(found_keywords))\n",
        "\n",
        "    def _calculate_relevance_score(\n",
        "        self,\n",
        "        file_content: str,\n",
        "        keywords: List[str]\n",
        "    ) -> float:\n",
        "        \"\"\"Calculate relevance score for a file.\"\"\"\n",
        "        file_content_lower = file_content.lower()\n",
        "\n",
        "        score = 0.0\n",
        "\n",
        "        for keyword in keywords:\n",
        "            # Count occurrences\n",
        "            count = file_content_lower.count(keyword)\n",
        "\n",
        "            # Weight by keyword importance (simple heuristic)\n",
        "            weight = 1.0\n",
        "            if keyword in ['python', 'r', 'machine learning', 'bioinformatics']:\n",
        "                weight = 2.0\n",
        "            elif keyword in ['aws', 'docker', 'sql']:\n",
        "                weight = 1.5\n",
        "\n",
        "            score += count * weight\n",
        "\n",
        "        # Normalize by file length\n",
        "        if len(file_content) > 0:\n",
        "            score = score / (len(file_content) / 1000)  # per 1000 chars\n",
        "\n",
        "        return score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb06c2d1",
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "LLMProvider() takes no arguments",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 655\u001b[39m\n\u001b[32m    652\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    653\u001b[39m     \u001b[38;5;66;03m# Initialize components\u001b[39;00m\n\u001b[32m    654\u001b[39m     config = Config()\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m     llm = \u001b[43mLLMProvider\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    656\u001b[39m     analyzer = OrganizedCVAnalyzer(config)\n\u001b[32m    657\u001b[39m     generator = CVGeneratorV3(llm, analyzer, config)\n",
            "\u001b[31mTypeError\u001b[39m: LLMProvider() takes no arguments"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ab12ffc",
      "metadata": {
        "id": "4ab12ffc"
      },
      "outputs": [],
      "source": [
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# CV GENERATOR V2\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "class CVGeneratorV2:\n",
        "    \"\"\"Generate customized CV files using organized reference files.\"\"\"\n",
        "\n",
        "    def __init__(self, llm: LLMProvider, analyzer: OrganizedCVAnalyzer, config: Config):\n",
        "        self.llm = llm\n",
        "        self.analyzer = analyzer\n",
        "        self.config = config\n",
        "\n",
        "    def generate_cv_files(\n",
        "        self,\n",
        "        job_description: str,\n",
        "        job_title: str,\n",
        "        company_name: str,\n",
        "        output_dir: Optional[str] = None\n",
        "    ) -> Dict[str, str]:\n",
        "        \"\"\"Generate all CV files for a job application.\"\"\"\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"ðŸš€ GENERATING CUSTOMIZED CV\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"   Job Title: {job_title}\")\n",
        "        print(f\"   Company: {company_name}\")\n",
        "\n",
        "        # Create output directory\n",
        "        if not output_dir:\n",
        "            timestamp = datetime.now().strftime(\"%Y_%m\")\n",
        "            safe_title = re.sub(r'[^\\w\\s-]', '', job_title).replace(' ', '_')\n",
        "            safe_company = re.sub(r'[^\\w\\s-]', '', company_name).replace(' ', '_')\n",
        "            output_dir = f\"{self.config.OUTPUT_BASE_PATH}/{safe_title}_{safe_company}_{timestamp}\"\n",
        "\n",
        "        output_path = Path(output_dir)\n",
        "        output_path.mkdir(parents=True, exist_ok=True)\n",
        "        print(f\"   Output: {output_path}\")\n",
        "\n",
        "        generated_files = {}\n",
        "\n",
        "        # Generate each category\n",
        "        for category in self.config.CV_CATEGORIES:\n",
        "            print(f\"\\n{'='*70}\")\n",
        "            print(f\"ðŸ“ Generating {category.upper()}\")\n",
        "            print(f\"{'='*70}\")\n",
        "\n",
        "            # Find relevant reference files\n",
        "            reference_files = self.analyzer.find_relevant_files(\n",
        "                job_description=job_description,\n",
        "                category=category,\n",
        "                top_k=5\n",
        "            )\n",
        "\n",
        "            if not reference_files:\n",
        "                print(f\"   âš ï¸  No reference files found for {category}\")\n",
        "                continue\n",
        "\n",
        "            # Generate content\n",
        "            content = self._generate_file_content(\n",
        "                category=category,\n",
        "                job_description=job_description,\n",
        "                job_title=job_title,\n",
        "                company_name=company_name,\n",
        "                reference_files=reference_files\n",
        "            )\n",
        "\n",
        "            if content and \"Error:\" not in content:\n",
        "                filename = f\"{category}.tex\"\n",
        "                filepath = output_path / filename\n",
        "\n",
        "                with open(filepath, 'w', encoding='utf-8') as f:\n",
        "                    f.write(content)\n",
        "\n",
        "                generated_files[filename] = str(filepath)\n",
        "                print(f\"   âœ… Saved: {filename}\")\n",
        "            else:\n",
        "                print(f\"   âŒ Failed: {content}\")\n",
        "\n",
        "        # Generate main resume file\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"ðŸ“„ Generating main resume file\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        resume_content = self._generate_resume_main(job_title, company_name)\n",
        "\n",
        "        safe_title = re.sub(r'[^\\w\\s-]', '', job_title).replace(' ', '_')\n",
        "        resume_filename = f\"resume_konstantinos_billis_{safe_title}.tex\"\n",
        "        resume_path = output_path / resume_filename\n",
        "\n",
        "        with open(resume_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(resume_content)\n",
        "\n",
        "        generated_files['main_resume'] = str(resume_path)\n",
        "        print(f\"   âœ… Saved: {resume_filename}\")\n",
        "\n",
        "        # Generate README\n",
        "        readme_content = self._generate_readme(\n",
        "            job_title, company_name, job_description\n",
        "        )\n",
        "\n",
        "        readme_path = output_path / \"README.md\"\n",
        "        with open(readme_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(readme_content)\n",
        "\n",
        "        print(f\"\\nâœ… Generated {len(generated_files)} files in: {output_path}\")\n",
        "\n",
        "        return generated_files\n",
        "\n",
        "    def _generate_file_content(\n",
        "        self,\n",
        "        category: str,\n",
        "        job_description: str,\n",
        "        job_title: str,\n",
        "        company_name: str,\n",
        "        reference_files: List[Dict]\n",
        "    ) -> str:\n",
        "        \"\"\"Generate content for a specific category.\"\"\"\n",
        "\n",
        "        # Prepare reference examples\n",
        "        reference_examples = []\n",
        "        for i, ref_file in enumerate(reference_files[:3], 1):\n",
        "            ref_content = ref_file['content'][:1500]  # Limit length\n",
        "            reference_examples.append(\n",
        "                f\"\\n### Example {i} (from {ref_file['source_dir']}):\\n{ref_content}\\n\"\n",
        "            )\n",
        "\n",
        "        reference_text = '\\n'.join(reference_examples)\n",
        "\n",
        "        # Category-specific instructions\n",
        "        category_instructions = {\n",
        "            'summary': \"Create a compelling professional summary (3-5 sentences) similar to Summaries of the Reference Examples and based on job adv\",\n",
        "            \n",
        "            'experience': \"- Use exact titles, names of companies and dates from Reference Examples\\\n",
        "                - Include department/faculty if it clarifies specialization \\\n",
        "                - report in same order using \\\\cventry commands \\\n",
        "                - and only adjust the \\\\cvitems of Reference Examples based on the job adv.\",\n",
        "            \n",
        "            'key_skills': \"Organize skills by 2 or 3 categories - Use exact same names of Reference Examples- using \\\\cvskill commands as on Reference Examples. \\\n",
        "                Prioritize skills mentioned in the job description and Reference Examples.\",\n",
        "            'education': \"Use exact degree names from Reference Examples, List educational background using \\\\cventry commands. Include relevant coursework or thesis topics if relevant to job.\",\n",
        "            'additional_info': \"Include publications, certifications, awards, or other relevant information using \\\\cvhonor or \\\\cventry commands, always based on Reference Examples.\"\n",
        "        }\n",
        "\n",
        "        instruction = category_instructions.get(category, \"Generate appropriate content for this section.\")\n",
        "\n",
        "        prompt = f\"\"\"You are an expert CV writer creating LaTeX files for the Awesome-CV template.\n",
        "\n",
        "**Task:** Generate a {category}.tex file customized for this job application.\n",
        "\n",
        "**Job Information:**\n",
        "- Title: {job_title}\n",
        "- Company: {company_name}\n",
        "\n",
        "**Job Description (extract key requirements):**\n",
        "{job_description[:1500]}\n",
        "\n",
        "**Reference Examples (from successful CVs):**\n",
        "{reference_text}\n",
        "\n",
        "**Instructions:**\n",
        "1. {instruction}\n",
        "2. Use ONLY Awesome-CV LaTeX commands:\n",
        "   - \\\\cventry{{date}}{{title}}{{company}}{{location}}{{description}}\n",
        "   - \\\\cvskill{{category}}{{skills list}}\n",
        "   - \\\\cvhonor{{award}}{{organization}}{{location}}{{date}}\n",
        "3. Use similar keywords to job description, and Tailor content of Reference Examples to match job requirements\n",
        "4. Keep formatting consistent with Reference Examples\n",
        "5. DO NOT include \"Here is the generated `key_skills.tex` file:\", \\\\documentclass, \\\\begin{{document}},  \\\\end{{document}}\n",
        "6. Return ONLY the LaTeX content, no explanations or markdown. Same format and structure as examples. Only select relevant experiences of the Reference Examples and change content based on job description.\n",
        "7. In resume tex file, use full path of files\n",
        "8. Prepare all required files\n",
        "\n",
        "\n",
        "Generate the {category}.tex content now:\n",
        "\"\"\"\n",
        "\n",
        "        print(f\"   ðŸ¤– Generating with {self.config.LLM_PROVIDER}...\")\n",
        "\n",
        "        prefix = \"prompt_output\"\n",
        "        filename = f\"{prefix}_{category}.txt\"\n",
        "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(prompt)\n",
        "        print(f\"âœ… Saved text to {filename}\")\n",
        "            \n",
        "        \n",
        "        \n",
        "        return self.llm.generate(prompt)\n",
        "\n",
        "    def _generate_resume_main(self, job_title: str, company_name: str) -> str:\n",
        "        \"\"\"Generate the main resume.tex file.\"\"\"\n",
        "\n",
        "        return r\"\"\"%!TEX TS-program = xelatex\n",
        "%!TEX encoding = UTF-8 Unicode\n",
        "\n",
        "\\documentclass[11pt, a4paper]{awesome-cv}\n",
        "\n",
        "\\geometry{left=1.4cm, top=1cm, right=1.4cm, bottom=1.2cm, footskip=.5cm}\n",
        "\n",
        "\\fontdir[fonts/]\n",
        "\n",
        "\\colorlet{awesome}{awesome-nephritis}\n",
        "\n",
        "\\renewcommand{\\acvHeaderSocialSep}{\\quad\\textbar\\quad}\n",
        "\n",
        "%-------------------------------------------------------------------------------\n",
        "%\tPERSONAL INFORMATION\n",
        "%-------------------------------------------------------------------------------\n",
        "\\photo[circle,noedge,right]{/Users/kbillis/bin/Awesome-CV/examples/photos/kbillis_2024_profphoto.jpg}\n",
        "\n",
        "\\name{Konstantinos}{Billis}\n",
        "\\position{PhD in Bioinformatics{\\enskip\\cdotp\\enskip}Data Solutions Developer in Healthcare \\& Life Sciences}\n",
        "\n",
        "\\address{Zurich, CH (Permit: B) - Citizenships: Greek (EU), British}\n",
        "\\dateofbirth{Aug 16, 1983}\n",
        "\\email{billis.konstantinos@gmail.com}\n",
        "\\homepage{kbillis.github.io/}\n",
        "\\linkedin{konstantinos-billis-phd-baa23a22}\n",
        "\\googlescholar{YsQgBgUAAAAJ&view_op=list_works&sortby=pubdate}{ }\n",
        "\n",
        "%-------------------------------------------------------------------------------\n",
        "\\begin{document}\n",
        "\\newpage\n",
        "\n",
        "\\makecvheader[C]\n",
        "\n",
        "\\makecvfooter\n",
        "  {\\today}\n",
        "  {Konstantinos Billis, PhD~~~Â·~~~RÃ©sumÃ©}\n",
        "  {\\thepage}\n",
        "\n",
        "%-------------------------------------------------------------------------------\n",
        "%\tCV/RESUME CONTENT\n",
        "%\tEach section is imported separately\n",
        "%-------------------------------------------------------------------------------\n",
        "\\input{summary.tex}\n",
        "\\input{key_skills.tex}\n",
        "\\input{experience.tex}\n",
        "\\input{education.tex}\n",
        "\\input{additional_info.tex}\n",
        "\n",
        "\\end{document}\n",
        "\"\"\"\n",
        "\n",
        "    def _generate_readme(self, job_title: str, company_name: str, job_description: str) -> str:\n",
        "        \"\"\"Generate README for the CV directory.\"\"\"\n",
        "        return f\"\"\"# CV for {job_title} at {company_name}\n",
        "\n",
        "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "Using: AI-Powered CV Customizer V2\n",
        "\n",
        "## Job Description\n",
        "\n",
        "{job_description[:500]}...\n",
        "\n",
        "## Files Generated\n",
        "\n",
        "- `resume_konstantinos_billis_*.tex` - Main resume file\n",
        "- `summary.tex` - Professional summary\n",
        "- `experience.tex` - Work experience\n",
        "- `key_skills.tex` - Key skills\n",
        "- `education.tex` - Education\n",
        "- `additional_info.tex` - Publications, awards, etc.\n",
        "\n",
        "## Compilation\n",
        "```bash\n",
        "cd \"{os.path.basename(self.config.OUTPUT_BASE_PATH)}\"\n",
        "xelatex resume_konstantinos_billis_*.tex\n",
        "```\n",
        "\n",
        "## Customization\n",
        "\n",
        "Generated using:\n",
        "- LLM Provider: {self.config.LLM_PROVIDER}\n",
        "- Reference files from organized CV database\n",
        "- Keyword matching and relevance scoring\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "1. Review all generated files\n",
        "2. Customize any sections as needed\n",
        "3. Compile to PDF\n",
        "4. Proofread final output\n",
        "\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e4bd87d",
      "metadata": {
        "id": "0e4bd87d"
      },
      "outputs": [],
      "source": [
        "read_adv = False\n",
        "if read_adv:\n",
        "  #parse the page with the job description\n",
        "  # langchain gets the public description of the job\n",
        "  # make sure we are not behind a proxy\n",
        "  from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "  jd_url=\"https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/4314922582\"\n",
        "  print(f\"Getting information for job {jd_url}\")\n",
        "  loader = WebBaseLoader(jd_url )\n",
        "  loader.default_parser=\"html.parser\"\n",
        "  docs = loader.load()\n",
        "  # docs[0]\n",
        "\n",
        "\n",
        "  # from langchain_unstructured import UnstructuredLoader\n",
        "  # loader = UnstructuredLoader(web_url=jd_url)\n",
        "  # docs=loader.load()\n",
        "\n",
        "  description = docs[0].page_content\n",
        "  # print(f\"Job Description\\n{description} \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49cf70f0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49cf70f0",
        "outputId": "c25eeff0-451f-4195-f528-4d67a79170e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "ðŸ“„ AI-POWERED CV CUSTOMIZER V2\n",
            "======================================================================\n",
            "\n",
            "âš™ï¸  Configuration:\n",
            "   LLM Provider: ollama\n",
            "   Organized Files: /Users/kbillis/bin/Awesome-CV/organized_filesdeduplicated\n",
            "   Output Path: /Users/kbillis/bin/Awesome-CV/examples/AI/\n",
            "\n",
            "ðŸ¤– Initializing LLM...\n",
            "âœ… Ollama initialized on http://127.0.0.1:11434 with model: llama3.1:8b\n",
            "\n",
            "ðŸ“š Loading organized CV files...\n",
            "\n",
            "ðŸ” Loading organized CV files...\n",
            "   âœ… experience          :  48 files\n",
            "   âœ… education           :   9 files\n",
            "   âœ… additional_info     :   7 files\n",
            "   âœ… key_skills          :  47 files\n",
            "   âœ… summary             :  39 files\n",
            "\n",
            "======================================================================\n",
            "ðŸš€ GENERATING CUSTOMIZED CV\n",
            "======================================================================\n",
            "   Job Title: Senior Bioinformatics Scientist\n",
            "   Company: Roche\n",
            "   Output: /Users/kbillis/bin/Awesome-CV/examples/AI/Senior_Bioinformatics_Scientist_Roche_2025_11\n",
            "\n",
            "======================================================================\n",
            "ðŸ“ Generating SUMMARY\n",
            "======================================================================\n",
            "   ðŸ“‹ Top 5 files for summary:\n",
            "      1. Plants_2025_03                 (score: 83.67181153533711)\n",
            "      2. Bioinformatics_2025_10_SIB     (score: 80.64516129032258)\n",
            "      3. DS_Healthcare_2025_04          (score: 79.8283261802575)\n",
            "      4. Bioinformatics_2025_06         (score: 79.4590025359256)\n",
            "      5. Bioinformatics_2025_05         (score: 78.99461400359066)\n",
            "   ðŸ¤– Generating with ollama...\n",
            "   âœ… Saved: summary.tex\n",
            "\n",
            "======================================================================\n",
            "ðŸ“ Generating EXPERIENCE\n",
            "======================================================================\n",
            "   ðŸ“‹ Top 5 files for experience:\n",
            "      1. client_Platform_Nvidia_Med_2025_10 (score: 96.48230988206588)\n",
            "      2. Bioinformatics_2025_10_SIB     (score: 93.50202126066776)\n",
            "      3. LEAD_data_engineer_JJ_2025_10  (score: 93.02087597216537)\n",
            "      4. client_Platform_Med_2025_10    (score: 90.89951548346325)\n",
            "      5. Bioinformatics_2025_09         (score: 90.34341524889729)\n",
            "   ðŸ¤– Generating with ollama...\n",
            "   âœ… Saved: experience.tex\n",
            "\n",
            "======================================================================\n",
            "ðŸ“ Generating KEY_SKILLS\n",
            "======================================================================\n",
            "   ðŸ“‹ Top 5 files for key_skills:\n",
            "      1. client_Platform_Med_2025_10    (score: 74.4944283945522)\n",
            "      2. LEAD_data_engineer_JJ_2025_10  (score: 74.47257383966245)\n",
            "      3. business_Data_Prec_Med_2025_10 (score: 74.2818671454219)\n",
            "      4. client_Platform_Nvidia_Med_2025_10 (score: 72.75350370981039)\n",
            "      5. Plants_2025_09                 (score: 67.68720029509406)\n",
            "   ðŸ¤– Generating with ollama...\n",
            "   âœ… Saved: key_skills.tex\n",
            "\n",
            "======================================================================\n",
            "ðŸ“ Generating EDUCATION\n",
            "======================================================================\n",
            "   ðŸ“‹ Top 5 files for education:\n",
            "      1. Bioinformatics_2025_10_SIB     (score: 59.03398926654741)\n",
            "      2. Plants_2025_09                 (score: 57.30866884294523)\n",
            "      3. Bioinformatics_2025_10_RNA     (score: 54.20054200542005)\n",
            "      4. Data_Science_Novartis_2025_08  (score: 51.97342711996873)\n",
            "      5. LEAD_data_engineer_JJ_2025_10  (score: 48.58142246404975)\n",
            "   ðŸ¤– Generating with ollama...\n",
            "   âœ… Saved: education.tex\n",
            "\n",
            "======================================================================\n",
            "ðŸ“ Generating ADDITIONAL_INFO\n",
            "======================================================================\n",
            "   ðŸ“‹ Top 5 files for additional_info:\n",
            "      1. Bioinformatics_2025_10_SIB     (score: 63.05922454196847)\n",
            "      2. Bioinformatics_2025_09_cancer  (score: 56.053811659192824)\n",
            "      3. Bioinformatics_2025_09_biomarkers (score: 54.92638731596829)\n",
            "      4. Data_Analytics_Med_2025_08     (score: 54.80225988700565)\n",
            "      5. Bioinformatics_2025_10_RNA     (score: 54.80225988700565)\n",
            "   ðŸ¤– Generating with ollama...\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 85\u001b[39m\n\u001b[32m     82\u001b[39m job_title = \u001b[33m\"\u001b[39m\u001b[33mSenior Bioinformatics Scientist\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     83\u001b[39m company_name = \u001b[33m\"\u001b[39m\u001b[33mRoche\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_description\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_title\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompany_name\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(job_description, job_title, company_name)\u001b[39m\n\u001b[32m     30\u001b[39m generator = CVGeneratorV2(llm, analyzer, config)\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# generator = CVGeneratorV3(llm, analyzer, config)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m generated_files = \u001b[43mgenerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_cv_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjob_description\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjob_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjob_title\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjob_title\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompany_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompany_name\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m70\u001b[39m)\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâœ… CV GENERATION COMPLETE!\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 60\u001b[39m, in \u001b[36mCVGeneratorV2.generate_cv_files\u001b[39m\u001b[34m(self, job_description, job_title, company_name, output_dir)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# Generate content\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[38;5;28mbreakpoint\u001b[39m()\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m content = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_file_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjob_description\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjob_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjob_title\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjob_title\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompany_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompany_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreference_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreference_files\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m content \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mError:\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m content:\n\u001b[32m     69\u001b[39m     filename = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.tex\u001b[39m\u001b[33m\"\u001b[39m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 179\u001b[39m, in \u001b[36mCVGeneratorV2._generate_file_content\u001b[39m\u001b[34m(self, category, job_description, job_title, company_name, reference_files)\u001b[39m\n\u001b[32m    147\u001b[39m         prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[33mYou are an expert CV writer creating LaTeX files for the Awesome-CV template.\u001b[39m\n\u001b[32m    148\u001b[39m \n\u001b[32m    149\u001b[39m \u001b[33m**Task:** Generate a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.tex file customized for this job application.\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    175\u001b[39m \u001b[33mGenerate the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.tex content now:\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m    178\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   ðŸ¤– Generating with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.config.LLM_PROVIDER\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 102\u001b[39m, in \u001b[36mOllamaProvider.generate\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m     99\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mError: Ollama not initialized\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    101\u001b[39m     \u001b[38;5;66;03m# Use the client instance's generate method\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mOLLAMA_MODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTEMPERATURE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnum_predict\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMAX_TOKENS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response[\u001b[33m'\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/everyday_repo/lib/python3.13/site-packages/ollama/_client.py:256\u001b[39m, in \u001b[36mClient.generate\u001b[39m\u001b[34m(self, model, prompt, suffix, system, template, context, stream, think, raw, format, images, options, keep_alive)\u001b[39m\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate\u001b[39m(\n\u001b[32m    230\u001b[39m   \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    231\u001b[39m   model: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    244\u001b[39m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    245\u001b[39m ) -> Union[GenerateResponse, Iterator[GenerateResponse]]:\n\u001b[32m    246\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03m  Create a response using the requested model.\u001b[39;00m\n\u001b[32m    248\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    253\u001b[39m \u001b[33;03m  Returns `GenerateResponse` if `stream` is `False`, otherwise returns a `GenerateResponse` generator.\u001b[39;00m\n\u001b[32m    254\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mGenerateResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/api/generate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mGenerateRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m      \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m      \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m      \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m      \u001b[49m\u001b[43mthink\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthink\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m      \u001b[49m\u001b[43mraw\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m      \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_copy_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/everyday_repo/lib/python3.13/site-packages/ollama/_client.py:189\u001b[39m, in \u001b[36mClient._request\u001b[39m\u001b[34m(self, cls, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    185\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**part)\n\u001b[32m    187\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.json())\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/everyday_repo/lib/python3.13/site-packages/ollama/_client.py:129\u001b[39m, in \u001b[36mClient._request_raw\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_request_raw\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    128\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     r = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m     r.raise_for_status()\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/everyday_repo/lib/python3.13/site-packages/httpx/_client.py:825\u001b[39m, in \u001b[36mClient.request\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m    810\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m    812\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m    813\u001b[39m     method=method,\n\u001b[32m    814\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    823\u001b[39m     extensions=extensions,\n\u001b[32m    824\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/everyday_repo/lib/python3.13/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/everyday_repo/lib/python3.13/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/everyday_repo/lib/python3.13/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/everyday_repo/lib/python3.13/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/everyday_repo/lib/python3.13/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/everyday_repo/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/everyday_repo/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/everyday_repo/lib/python3.13/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/everyday_repo/lib/python3.13/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/everyday_repo/lib/python3.13/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/everyday_repo/lib/python3.13/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/everyday_repo/lib/python3.13/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/everyday_repo/lib/python3.13/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# MAIN WORKFLOW\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def main(job_description: str, job_title: str, company_name: str):\n",
        "    \"\"\"Main execution workflow.\"\"\"\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"ðŸ“„ AI-POWERED CV CUSTOMIZER V2\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Initialize\n",
        "    config = Config()\n",
        "\n",
        "    print(f\"\\nâš™ï¸  Configuration:\")\n",
        "    print(f\"   LLM Provider: {config.LLM_PROVIDER}\")\n",
        "    print(f\"   Organized Files: {config.ORGANIZED_FILES_PATH}\")\n",
        "    print(f\"   Output Path: {config.OUTPUT_BASE_PATH}\")\n",
        "\n",
        "    # Initialize LLM\n",
        "    print(f\"\\nðŸ¤– Initializing LLM...\")\n",
        "    llm = get_llm_provider(config)\n",
        "\n",
        "    # Initialize Analyzer\n",
        "    print(f\"\\nðŸ“š Loading organized CV files...\")\n",
        "    analyzer = OrganizedCVAnalyzer(config.ORGANIZED_FILES_PATH)\n",
        "    analyzer.load_all_categories()\n",
        "\n",
        "    # Generate CV\n",
        "    generator = CVGeneratorV2(llm, analyzer, config)\n",
        "    # generator = CVGeneratorV3(llm, analyzer, config)\n",
        "\n",
        "\n",
        "    generated_files = generator.generate_cv_files(\n",
        "        job_description=job_description,\n",
        "        job_title=job_title,\n",
        "        company_name=company_name\n",
        "    )\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"âœ… CV GENERATION COMPLETE!\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nGenerated files:\")\n",
        "    for filename, filepath in generated_files.items():\n",
        "        print(f\"   ðŸ“„ {filename}\")\n",
        "\n",
        "    print(\"\\nðŸ’¡ Next steps:\")\n",
        "    print(\"   1. Review generated files\")\n",
        "    print(\"   2. Compile: xelatex resume_*.tex\")\n",
        "    print(\"   3. Customize if needed\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Example job application\n",
        "    job_description = \"\"\"\n",
        "    Senior Bioinformatics Scientist - Cancer Genomics\n",
        "\n",
        "    Join our Oncology Research team to lead bioinformatics analysis of\n",
        "    multi-omics cancer data. The ideal candidate will have extensive\n",
        "    experience in cancer genomics, RNA-seq analysis, and biomarker discovery.\n",
        "\n",
        "    Key Responsibilities:\n",
        "    - Lead bioinformatics analysis of multi-omics cancer data\n",
        "    - Develop computational pipelines for biomarker discovery\n",
        "    - Collaborate with wet-lab scientists and clinicians\n",
        "    - Mentor junior bioinformaticians\n",
        "\n",
        "    Required Skills:\n",
        "    - PhD in Bioinformatics, Computational Biology, or related field\n",
        "    - 5+ years experience in cancer genomics\n",
        "    - Expert in Python, R, and Unix/Linux\n",
        "    - Experience with RNA-seq, WGS, WES analysis\n",
        "    - Strong publication record\n",
        "    - Knowledge of AWS/cloud computing\n",
        "\n",
        "    Preferred:\n",
        "    - Experience with single-cell sequencing\n",
        "    - Knowledge of immunotherapy biomarkers\n",
        "    - Machine learning experience\n",
        "    \"\"\"\n",
        "\n",
        "    job_title = \"Senior Bioinformatics Scientist\"\n",
        "    company_name = \"Roche\"\n",
        "\n",
        "    main(job_description, job_title, company_name)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "everyday_repo",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
